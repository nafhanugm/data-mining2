{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97668933",
   "metadata": {},
   "source": [
    "# Training & Modelling\n",
    "\n",
    "This notebook trains and evaluates multiple models on the cleaned datasets prepared in `data-cleaning-preprocessing.ipynb`.\n",
    "\n",
    "Models included:\n",
    "- TF-IDF (char n-grams) + Logistic Regression\n",
    "- Hybrid: TF-IDF (char + word) + numeric features + Logistic Regression\n",
    "- Sentence-Transformers embeddings + Logistic Regression\n",
    "- IndoBERT fine-tuning (optional, lightweight config)\n",
    "- Google AI Studio Embeddings (API Key) + Logistic Regression\n",
    "- Vertex AI (Text Embeddings) + Logistic Regression\n",
    "\n",
    "You can choose between original or 50:50 balanced splits. Save artifacts to `models/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8803226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & configuration\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "MODELS_DIR = 'models'\n",
    "PROCESSED_DIR = 'processed_data'\n",
    "RANDOM_STATE = 42\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# External services configuration (leave empty to skip)\n",
    "GOOGLE_AI_STUDIO_API_KEY = os.environ.get('GOOGLE_AI_STUDIO_API_KEY', '#')  # or paste here\n",
    "VERTEX_PROJECT_ID = os.environ.get('VERTEX_PROJECT_ID', 'n8n-pmld')\n",
    "VERTEX_LOCATION = os.environ.get('VERTEX_LOCATION', 'us-central1')\n",
    "VERTEX_USE_GCLOUD_AUTH = bool(os.environ.get('VERTEX_USE_GCLOUD_AUTH', 'true'))  # if true, use ADC\n",
    "\n",
    "# HuggingFace/transformers models\n",
    "SENTENCE_TRANSFORMER_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "INDOBERT_MODEL = 'indobenchmark/indobert-base-p1'  # for optional finetuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optional dependencies (run once)\n",
    "# illustrative only\n",
    "# !pip install scikit-learn scikit-learn-intelex sentence-transformers transformers torch --quiet\n",
    "# For Google AI Studio (Gemini Embeddings):\n",
    "# !pip install google-generativeai --quiet\n",
    "# For Vertex AI embeddings:\n",
    "# !pip install google-cloud-aiplatform --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab632f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ((14908, 13), {1: 7454, 0: 7454}),\n",
       " 'test': ((4252, 13), {0: 2126, 1: 2126}),\n",
       " 'holdout': ((1884, 13), {1: 942, 0: 942})}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data loading\n",
    "\n",
    "def load_split(use_balanced: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "    if use_balanced:\n",
    "        train_path = os.path.join(PROCESSED_DIR, 'train_balanced_50_50.csv')\n",
    "        test_path = os.path.join(PROCESSED_DIR, 'test_balanced_50_50.csv')\n",
    "        holdout_path = os.path.join(PROCESSED_DIR, 'holdout_balanced_50_50.csv')\n",
    "    else:\n",
    "        train_path = os.path.join(PROCESSED_DIR, 'train_processed.csv')\n",
    "        test_path = os.path.join(PROCESSED_DIR, 'test_processed.csv')\n",
    "        holdout_path = os.path.join(PROCESSED_DIR, 'holdout_processed.csv')\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    holdout = pd.read_csv(holdout_path)\n",
    "    return {'train': train, 'test': test, 'holdout': holdout}\n",
    "\n",
    "splits = load_split(use_balanced=False)\n",
    "{ k: (v.shape, v['label'].value_counts().to_dict()) for k, v in splits.items() }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f37ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137c18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics and utilities\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def evaluate_and_report(y_true, y_pred, title: str):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    return {\n",
    "        'precision_1': precision_score(y_true, y_pred, pos_label=1),\n",
    "        'recall_1': recall_score(y_true, y_pred, pos_label=1),\n",
    "        'f1_1': f1_score(y_true, y_pred, pos_label=1),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_xy(df: pd.DataFrame, text_col: str = 'comment_clean'):\n",
    "    return df[text_col].astype(str).tolist(), df['label'].astype(int).values\n",
    "\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f7abd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Char TF-IDF + LogisticRegression (test) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9730    0.9986    0.9856      2126\n",
      "           1     0.9986    0.9722    0.9852      2126\n",
      "\n",
      "    accuracy                         0.9854      4252\n",
      "   macro avg     0.9858    0.9854    0.9854      4252\n",
      "weighted avg     0.9858    0.9854    0.9854      4252\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/lr_char.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1: TF-IDF (char) + Logistic Regression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train, y_train = get_xy(splits['train'])\n",
    "X_test, y_test = get_xy(splits['test'])\n",
    "\n",
    "char_tfidf_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer='char', ngram_range=(2,4), min_df=2)),\n",
    "    ('clf', LogisticRegression(max_iter=5000, class_weight='balanced', n_jobs=1, C=2.0, solver='liblinear'))\n",
    "])\n",
    "\n",
    "char_tfidf_lr.fit(X_train, y_train)\n",
    "y_pred = char_tfidf_lr.predict(X_test)\n",
    "metrics = evaluate_and_report(y_test, y_pred, 'Char TF-IDF + LogisticRegression (test)')\n",
    "results.append({'model':'char_tfidf_lr','metrics':metrics})\n",
    "\n",
    "joblib.dump(char_tfidf_lr, os.path.join(MODELS_DIR, 'lr_char.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b729b119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hybrid (char+word+numeric) + LogisticRegression (test) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9712    0.9986    0.9847      2126\n",
      "           1     0.9985    0.9704    0.9843      2126\n",
      "\n",
      "    accuracy                         0.9845      4252\n",
      "   macro avg     0.9849    0.9845    0.9845      4252\n",
      "weighted avg     0.9849    0.9845    0.9845      4252\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/hybrid_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2: Hybrid (char + word TF-IDF + numeric features)\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_features = ['char_count','word_count','avg_word_length','number_ratio','special_char_ratio','stylized_char_ratio','money_symbol_count','fire_symbol_count']\n",
    "\n",
    "# Prepare dataframes for ColumnTransformer\n",
    "train_df = splits['train'].copy()\n",
    "test_df = splits['test'].copy()\n",
    "\n",
    "# ColumnTransformer expects array-like; we will build union of text features and numeric\n",
    "char_vec = ('char', TfidfVectorizer(analyzer='char', ngram_range=(2,4), min_df=2), 'comment_clean')\n",
    "word_vec = ('word', TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=2), 'comment_clean')\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    char_vec,\n",
    "    word_vec,\n",
    "    ('num', StandardScaler(with_mean=False), num_features)\n",
    "], remainder='drop', sparse_threshold=0.3)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "hybrid_clf = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('clf', LogisticRegression(max_iter=5000, class_weight='balanced', C=2.0, solver='liblinear'))\n",
    "])\n",
    "\n",
    "hybrid_clf.fit(train_df, y_train)\n",
    "y_pred_h = hybrid_clf.predict(test_df)\n",
    "metrics_h = evaluate_and_report(y_test, y_pred_h, 'Hybrid (char+word+numeric) + LogisticRegression (test)')\n",
    "results.append({'model':'hybrid_lr','metrics':metrics_h})\n",
    "\n",
    "joblib.dump(hybrid_clf, os.path.join(MODELS_DIR, 'hybrid_model.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 233/233 [00:24<00:00,  9.44it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:06<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sentence-Transformer Embeddings + LogisticRegression (test) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9449    0.9280    0.9364      2126\n",
      "           1     0.9293    0.9459    0.9375      2126\n",
      "\n",
      "    accuracy                         0.9370      4252\n",
      "   macro avg     0.9371    0.9370    0.9370      4252\n",
      "weighted avg     0.9371    0.9370    0.9370      4252\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/st_embed_lr.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3: Sentence-Transformers embeddings + Logistic Regression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "st_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)\n",
    "\n",
    "def embed_texts(texts: List[str], batch_size: int = 64):\n",
    "    return st_model.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "Xtr_emb = embed_texts(splits['train']['comment_clean'].tolist())\n",
    "Xte_emb = embed_texts(splits['test']['comment_clean'].tolist())\n",
    "\n",
    "clf_emb = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "clf_emb.fit(Xtr_emb, y_train)\n",
    "y_pred_emb = clf_emb.predict(Xte_emb)\n",
    "metrics_emb = evaluate_and_report(y_test, y_pred_emb, 'Sentence-Transformer Embeddings + LogisticRegression (test)')\n",
    "results.append({'model':'st_embed_lr','metrics':metrics_emb})\n",
    "\n",
    "joblib.dump({'encoder': SENTENCE_TRANSFORMER_MODEL, 'clf': clf_emb}, os.path.join(MODELS_DIR, 'st_embed_lr.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd9101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:00<00:00, 12821.90 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 28255.79 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('models/indobert/tokenizer_config.json',\n",
       " 'models/indobert/special_tokens_map.json',\n",
       " 'models/indobert/vocab.txt',\n",
       " 'models/indobert/added_tokens.json',\n",
       " 'models/indobert/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 4: IndoBERT fine-tuning (optional, quick demo)\n",
    "# Note: This is a minimal example; for full training use Trainer API with epochs.\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "indobert_tokenizer = AutoTokenizer.from_pretrained(INDOBERT_MODEL)\n",
    "indobert_model = AutoModelForSequenceClassification.from_pretrained(INDOBERT_MODEL, num_labels=2)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return indobert_tokenizer(batch['comment_clean'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Small subset for quick demo to avoid long runtimes\n",
    "train_small = splits['train'].sample(n=min(4000, len(splits['train'])), random_state=RANDOM_STATE)\n",
    "test_small = splits['test'].sample(n=min(2000, len(splits['test'])), random_state=RANDOM_STATE)\n",
    "\n",
    "import datasets as hfds\n",
    "train_ds = hfds.Dataset.from_pandas(train_small[['comment_clean','label']].rename(columns={'label':'labels'}))\n",
    "test_ds = hfds.Dataset.from_pandas(test_small[['comment_clean','label']].rename(columns={'label':'labels'}))\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "test_ds = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "cols = ['input_ids','attention_mask','labels']\n",
    "train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in cols])\n",
    "test_ds = test_ds.remove_columns([c for c in test_ds.column_names if c not in cols])\n",
    "train_ds.set_format('torch')\n",
    "test_ds.set_format('torch')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=os.path.join(MODELS_DIR, 'indobert'),\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    m = evaluate_and_report(labels, preds, 'IndoBERT (eval)')\n",
    "    return {'precision_1': m['precision_1'], 'recall_1': m['recall_1'], 'f1_1': m['f1_1']}\n",
    "\n",
    "try:\n",
    "    trainer = Trainer(model=indobert_model, args=args, train_dataset=train_ds, eval_dataset=test_ds, tokenizer=indobert_tokenizer)\n",
    "    # Optional training (can be time-consuming). Commented by default.\n",
    "    # trainer.train()\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    results.append({'model':'indobert_finetune_1epoch','metrics':eval_metrics})\n",
    "except ImportError as e:\n",
    "    print('Accelerate/Trainer not available, running manual evaluation without Trainer...')\n",
    "    indobert_model.eval()\n",
    "    device = torch.device('cpu')\n",
    "    indobert_model.to(device)\n",
    "    from torch.utils.data import DataLoader\n",
    "    loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].numpy()\n",
    "            logits = indobert_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_labels)\n",
    "    m = evaluate_and_report(y_true, y_pred, 'IndoBERT (manual eval)')\n",
    "    results.append({'model':'indobert_manual_eval','metrics':m})\n",
    "\n",
    "indobert_model.save_pretrained(os.path.join(MODELS_DIR, 'indobert'))\n",
    "indobert_tokenizer.save_pretrained(os.path.join(MODELS_DIR, 'indobert'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae6f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759241581.461673 6163239 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1759241887.394719 6163239 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Google AI Studio (Gemini) Embeddings + LogisticRegression (test) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9900    0.9741    0.9820      2126\n",
      "           1     0.9745    0.9901    0.9823      2126\n",
      "\n",
      "    accuracy                         0.9821      4252\n",
      "   macro avg     0.9822    0.9821    0.9821      4252\n",
      "weighted avg     0.9822    0.9821    0.9821      4252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 5: Google AI Studio (Gemini) Embeddings + Logistic Regression\n",
    "# Requires: `pip install google-generativeai`\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    has_genai = True\n",
    "except Exception:\n",
    "    has_genai = False\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def embed_with_genai(texts: List[str], api_key: str, model: str = 'text-embedding-004', task_type: str = 'retrieval_document'):\n",
    "    assert has_genai, 'google-generativeai not installed. Run install cell.'\n",
    "    assert api_key, 'Provide GOOGLE_AI_STUDIO_API_KEY in env or set GOOGLE_AI_STUDIO_API_KEY variable.'\n",
    "    genai.configure(api_key=api_key)\n",
    "    # Batch to avoid payload limits\n",
    "    embeddings = []\n",
    "    batch_size = 128\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        resp = genai.embed_content(model=model, content=batch, task_type=task_type)\n",
    "        vecs = resp['embedding'] if 'embedding' in resp else resp['data'][0]['embedding']\n",
    "        # API returns list under 'embedding' for batch; normalize shape\n",
    "        if isinstance(vecs[0], (int, float)):\n",
    "            vecs = [vecs]\n",
    "        embeddings.extend(vecs)\n",
    "    return np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "if has_genai and GOOGLE_AI_STUDIO_API_KEY:\n",
    "    Xtr_g = embed_with_genai(splits['train']['comment_clean'].tolist(), GOOGLE_AI_STUDIO_API_KEY)\n",
    "    Xte_g = embed_with_genai(splits['test']['comment_clean'].tolist(), GOOGLE_AI_STUDIO_API_KEY)\n",
    "    y_train = splits['train']['label'].astype(int).values\n",
    "    y_test = splits['test']['label'].astype(int).values\n",
    "    clf_g = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "    clf_g.fit(Xtr_g, y_train)\n",
    "    y_pred_g = clf_g.predict(Xte_g)\n",
    "    metrics_g = evaluate_and_report(y_test, y_pred_g, 'Google AI Studio (Gemini) Embeddings + LogisticRegression (test)')\n",
    "    results.append({'model':'gemini_embed_lr','metrics':metrics_g})\n",
    "    joblib.dump({'provider':'google_ai_studio','model':'text-embedding-004','clf': clf_g}, os.path.join(MODELS_DIR, 'gemini_embed_lr.pkl'))\n",
    "else:\n",
    "    print('Skipping Google AI Studio embeddings (install google-generativeai and/or set GOOGLE_AI_STUDIO_API_KEY).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c190bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6: Vertex AI Text Embeddings + Logistic Regression\n",
    "# Requires: `pip install google-cloud-aiplatform`\n",
    "try:\n",
    "    from google.cloud import aiplatform\n",
    "    has_vertex = True\n",
    "except Exception:\n",
    "    has_vertex = False\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "VERTEX_EMBED_MODEL = \"text-embedding-004\"  # for TextEmbeddingModel\n",
    "\n",
    "def embed_with_vertex(texts: List[str], project_id: str, location: str) -> np.ndarray:\n",
    "    assert has_vertex, 'google-cloud-aiplatform not installed.'\n",
    "    assert project_id, 'Set VERTEX_PROJECT_ID.'\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    # Use TextEmbeddingModel endpoint\n",
    "    from vertexai.language_models import TextEmbeddingModel\n",
    "    model = TextEmbeddingModel.from_pretrained(VERTEX_EMBED_MODEL)\n",
    "    embeddings = []\n",
    "    batch_size = 128\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        resp = model.get_embeddings(batch)\n",
    "        for r in resp:\n",
    "            embeddings.append(r.values)\n",
    "    return np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "if has_vertex and VERTEX_PROJECT_ID:\n",
    "    Xtr_v = embed_with_vertex(splits['train']['comment_clean'].tolist(), VERTEX_PROJECT_ID, VERTEX_LOCATION)\n",
    "    Xte_v = embed_with_vertex(splits['test']['comment_clean'].tolist(), VERTEX_PROJECT_ID, VERTEX_LOCATION)\n",
    "    y_train = splits['train']['label'].astype(int).values\n",
    "    y_test = splits['test']['label'].astype(int).values\n",
    "    clf_v = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "    clf_v.fit(Xtr_v, y_train)\n",
    "    y_pred_v = clf_v.predict(Xte_v)\n",
    "    metrics_v = evaluate_and_report(y_test, y_pred_v, 'Vertex AI Embeddings + LogisticRegression (test)')\n",
    "    results.append({'model':'vertex_embed_lr','metrics':metrics_v})\n",
    "    joblib.dump({'provider':'vertex_ai','model':VERTEX_EMBED_MODEL,'clf': clf_v}, os.path.join(MODELS_DIR, 'vertex_embed_lr.pkl'))\n",
    "else:\n",
    "    print('Skipping Vertex AI embeddings (install google-cloud-aiplatform and/or set VERTEX_PROJECT_ID).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f50736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             model      f1_1  precision_1  recall_1\n",
      "1    char_tfidf_lr  0.985224     0.998551  0.972248\n",
      "2        hybrid_lr  0.984256     0.998548  0.970367\n",
      "0  gemini_embed_lr  0.982268     0.974537  0.990122\n",
      "3      st_embed_lr  0.937529     0.929298  0.945908\n",
      "Metadata saved to models/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Results Summary & Model Comparison\n",
    "summary = []\n",
    "for r in results:\n",
    "    name = r['model']\n",
    "    m = r['metrics']\n",
    "    if isinstance(m, dict) and 'f1_1' in m:\n",
    "        summary.append({'model': name, 'f1_1': float(m['f1_1']), 'precision_1': float(m['precision_1']), 'recall_1': float(m['recall_1'])})\n",
    "\n",
    "summary_df = pd.DataFrame(summary).sort_values('f1_1', ascending=False)\n",
    "print(summary_df)\n",
    "\n",
    "# Save summary to models/metadata.json\n",
    "meta_path = os.path.join(MODELS_DIR, 'metadata.json')\n",
    "with open(meta_path, 'w') as f:\n",
    "    json.dump({'results': summary}, f, indent=2)\n",
    "print(f'Metadata saved to {meta_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f2ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import os\n",
    "\n",
    "# Lazy loaders for saved models\n",
    "char_lr = joblib.load(os.path.join(MODELS_DIR, 'lr_char.pkl')) if os.path.exists(os.path.join(MODELS_DIR, 'lr_char.pkl')) else None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df8b9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_lr = joblib.load(os.path.join(MODELS_DIR, 'hybrid_model.pkl')) if os.path.exists(os.path.join(MODELS_DIR, 'hybrid_model.pkl')) else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6def761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini_bundle = joblib.load(os.path.join(MODELS_DIR, 'gemini_embed_lr.pkl')) if os.path.exists(os.path.join(MODELS_DIR, 'gemini_embed_lr.pkl')) else None\n",
    "# vertex_bundle = joblib.load(os.path.join(MODELS_DIR, 'vertex_embed_lr.pkl')) if os.path.exists(os.path.join(MODELS_DIR, 'vertex_embed_lr.pkl')) else None\n",
    "\n",
    "# indobert_path = os.path.join(MODELS_DIR, 'indobert')\n",
    "# indobert_model = None\n",
    "# indobert_tokenizer = None\n",
    "# if os.path.exists(indobert_path):\n",
    "#     from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#     indobert_tokenizer = AutoTokenizer.from_pretrained(indobert_path)\n",
    "#     indobert_model = AutoModelForSequenceClassification.from_pretrained(indobert_path)\n",
    "\n",
    "# print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cfb7d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char TF-IDF predictions: [0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Inference 1: Char TF-IDF + Logistic Regression\n",
    "def classify_char_tfidf(texts: Union[str, List[str]]):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    if char_lr is None:\n",
    "        print(\"Char TF-IDF model not found. Train Model 1 first.\")\n",
    "        return None\n",
    "    return char_lr.predict(texts).tolist()\n",
    "\n",
    "# Test\n",
    "test_texts = [\"ðƒ ðŽ ð™ ð˜ˆ ðŸŸ gacor banget!\", \"terima kasih dok\"]\n",
    "print(\"Char TF-IDF predictions:\", classify_char_tfidf(test_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5813d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid predictions: [0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Inference 2: Hybrid (char+word+numeric) + Logistic Regression\n",
    "def classify_hybrid(texts: Union[str, List[str]]):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    if hybrid_lr is None:\n",
    "        print(\"Hybrid model not found. Train Model 2 first.\")\n",
    "        return None\n",
    "    \n",
    "    # Create dummy dataframe with required features\n",
    "    dummy_df = pd.DataFrame({\n",
    "        'comment_clean': texts, \n",
    "        'char_count': [len(t) for t in texts], \n",
    "        'word_count': [len(t.split()) for t in texts],\n",
    "        'avg_word_length': [len(t)/max(1,len(t.split())) for t in texts], \n",
    "        'number_ratio': [sum(ch.isdigit() for ch in t)/max(1,len(t)) for t in texts],\n",
    "        'special_char_ratio': [sum((not ch.isalnum()) and not ch.isspace() for ch in t)/max(1,len(t)) for t in texts],\n",
    "        'stylized_char_ratio': [0.0 for _ in texts], \n",
    "        'money_symbol_count': [0 for _ in texts], \n",
    "        'fire_symbol_count': [0 for _ in texts]\n",
    "    })\n",
    "    return hybrid_lr.predict(dummy_df).tolist()\n",
    "\n",
    "# Test\n",
    "print(\"Hybrid predictions:\", classify_hybrid(test_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a14670",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st_bundle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSentence-Transformer predictions:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mclassify_sentence_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_texts\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mclassify_sentence_transformer\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(texts, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m      4\u001b[39m     texts = [texts]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mst_bundle\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSentence-Transformer model not found. Train Model 3 first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'st_bundle' is not defined"
     ]
    }
   ],
   "source": [
    "# Inference 3: Sentence-Transformers + Logistic Regression\n",
    "def classify_sentence_transformer(texts: Union[str, List[str]]):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    if st_bundle is None:\n",
    "        print(\"Sentence-Transformer model not found. Train Model 3 first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        enc = SentenceTransformer(st_bundle['encoder'])\n",
    "        X_emb = enc.encode(texts, batch_size=64, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        return st_bundle['clf'].predict(X_emb).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Sentence-Transformer prediction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "print(\"Sentence-Transformer predictions:\", classify_sentence_transformer(test_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference 4: IndoBERT\n",
    "def classify_indobert(texts: Union[str, List[str]]):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    if indobert_model is None or indobert_tokenizer is None:\n",
    "        print(\"IndoBERT model not found. Train Model 4 first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        indobert_model.eval()\n",
    "        device = torch.device('cpu')\n",
    "        indobert_model.to(device)\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = indobert_tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            logits = indobert_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
    "        return preds\n",
    "    except Exception as e:\n",
    "        print(f\"IndoBERT prediction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "print(\"IndoBERT predictions:\", classify_indobert(test_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d628fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference 5: Google AI Studio (Gemini) + Logistic Regression\n",
    "def classify_gemini(texts: Union[str, List[str]]):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    if gemini_bundle is None or not GOOGLE_AI_STUDIO_API_KEY:\n",
    "        print(\"Gemini model not found or API key missing. Train Model 5 first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=GOOGLE_AI_STUDIO_API_KEY)\n",
    "        embeddings = []\n",
    "        batch_size = 128\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            resp = genai.embed_content(model='text-embedding-004', content=batch, task_type='retrieval_document')\n",
    "            vecs = resp['embedding'] if 'embedding' in resp else resp['data'][0]['embedding']\n",
    "            if isinstance(vecs[0], (int, float)):\n",
    "                vecs = [vecs]\n",
    "            embeddings.extend(vecs)\n",
    "        X_emb = np.array(embeddings, dtype=np.float32)\n",
    "        return gemini_bundle['clf'].predict(X_emb).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini prediction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "print(\"Gemini predictions:\", classify_gemini(test_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference 6: Vertex AI + Logistic Regression\n",
    "def classify_vertex(texts: Union[str, List[str]]):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    if vertex_bundle is None or not VERTEX_PROJECT_ID:\n",
    "        print(\"Vertex AI model not found or project ID missing. Train Model 6 first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        from google.cloud import aiplatform\n",
    "        from vertexai.language_models import TextEmbeddingModel\n",
    "        aiplatform.init(project=VERTEX_PROJECT_ID, location=VERTEX_LOCATION)\n",
    "        model = TextEmbeddingModel.from_pretrained(vertex_bundle['model'])\n",
    "        embeddings = []\n",
    "        batch_size = 128\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            resp = model.get_embeddings(batch)\n",
    "            for r in resp:\n",
    "                embeddings.append(r.values)\n",
    "        X_emb = np.array(embeddings, dtype=np.float32)\n",
    "        return vertex_bundle['clf'].predict(X_emb).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Vertex AI prediction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "print(\"Vertex AI predictions:\", classify_vertex(test_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference 7: LSTM (Keras)\n",
    "def classify_lstm(texts: Union[str, List[str]]):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    if lstm_bundle is None or 'lstm_model' not in globals():\n",
    "        print(\"LSTM model not found. Train Model 7 first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "        tk = lstm_bundle['tokenizer']\n",
    "        max_len = lstm_bundle['max_len']\n",
    "        seq = pad_sequences(tk.texts_to_sequences(texts), maxlen=max_len)\n",
    "        preds = (lstm_model.predict(seq)[:,0] >= 0.5).astype(int).tolist()\n",
    "        return preds\n",
    "    except Exception as e:\n",
    "        print(f\"LSTM prediction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "print(\"LSTM predictions:\", classify_lstm(test_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89aa6453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All Model Predictions ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'st_bundle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Test all models\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== All Model Predictions ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m all_predictions = \u001b[43mclassify_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, predictions \u001b[38;5;129;01min\u001b[39;00m all_predictions.items():\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mclassify_all\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hybrid_preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     19\u001b[39m     outputs[\u001b[33m'\u001b[39m\u001b[33mhybrid_lr\u001b[39m\u001b[33m'\u001b[39m] = hybrid_preds\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m st_preds = \u001b[43mclassify_sentence_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m st_preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     23\u001b[39m     outputs[\u001b[33m'\u001b[39m\u001b[33mst_embed_lr\u001b[39m\u001b[33m'\u001b[39m] = st_preds\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mclassify_sentence_transformer\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(texts, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m      4\u001b[39m     texts = [texts]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mst_bundle\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSentence-Transformer model not found. Train Model 3 first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'st_bundle' is not defined"
     ]
    }
   ],
   "source": [
    "# Unified Inference: Run all available models\n",
    "def classify_all(texts: Union[str, List[str]]):\n",
    "    \"\"\"\n",
    "    Run all available models on the input text(s)\n",
    "    Returns a dictionary with model names as keys and predictions as values\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    # Try each model\n",
    "    char_preds = classify_char_tfidf(texts)\n",
    "    if char_preds is not None:\n",
    "        outputs['char_tfidf_lr'] = char_preds\n",
    "    \n",
    "    hybrid_preds = classify_hybrid(texts)\n",
    "    if hybrid_preds is not None:\n",
    "        outputs['hybrid_lr'] = hybrid_preds\n",
    "    \n",
    "    st_preds = classify_sentence_transformer(texts)\n",
    "    if st_preds is not None:\n",
    "        outputs['st_embed_lr'] = st_preds\n",
    "    \n",
    "    indobert_preds = classify_indobert(texts)\n",
    "    if indobert_preds is not None:\n",
    "        outputs['indobert'] = indobert_preds\n",
    "    \n",
    "    gemini_preds = classify_gemini(texts)\n",
    "    if gemini_preds is not None:\n",
    "        outputs['gemini_embed_lr'] = gemini_preds\n",
    "    \n",
    "    vertex_preds = classify_vertex(texts)\n",
    "    if vertex_preds is not None:\n",
    "        outputs['vertex_embed_lr'] = vertex_preds\n",
    "    \n",
    "    lstm_preds = classify_lstm(texts)\n",
    "    if lstm_preds is not None:\n",
    "        outputs['keras_lstm'] = lstm_preds\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Test all models\n",
    "print(\"=== All Model Predictions ===\")\n",
    "all_predictions = classify_all(test_texts)\n",
    "for model_name, predictions in all_predictions.items():\n",
    "    print(f\"{model_name}: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fe126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_char_tfidf(\"DORA7 gacor banget!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
