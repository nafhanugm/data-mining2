{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lAFAvJ9CTt_A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('https://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/train.csv')\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/test.csv')\n",
        "df_holdout = pd.read_csv('https://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/holdout.csv', delimiter=';')\n",
        "\n",
        "df_all = pd.concat([df_train, df_test, df_holdout], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.tail(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Qj5LUFWywgbI",
        "outputId": "14d285ff-d8a4-4f11-9742-988d22c6b4bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 comment  label\n",
              "10496                                     ok min weton88      1\n",
              "10497         bang review movie blood brothers bara naga      0\n",
              "10498                                    bang ni sombong      0\n",
              "10499  queensavha udah bakar duit bro buzzernya gak h...      0\n",
              "10500                            destinasi yg kuinginkan      0\n",
              "10501  sih tulisan gabung beranda chanel youtube lu s...      0\n",
              "10502  bagus jga sih mainan gojo y sampe yg termurah ...      0\n",
              "10503  anak2 indonesia pertumbuham badanya menurun ke...      0\n",
              "10504                         papua original asian nigga      0\n",
              "10505                                              ü§£ü§£ü§£ü§£ü§£      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-255590b6-368e-4a05-8dc2-692d1e9ec086\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10496</th>\n",
              "      <td>ok min weton88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10497</th>\n",
              "      <td>bang review movie blood brothers bara naga</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10498</th>\n",
              "      <td>bang ni sombong</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10499</th>\n",
              "      <td>queensavha udah bakar duit bro buzzernya gak h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10500</th>\n",
              "      <td>destinasi yg kuinginkan</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10501</th>\n",
              "      <td>sih tulisan gabung beranda chanel youtube lu s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10502</th>\n",
              "      <td>bagus jga sih mainan gojo y sampe yg termurah ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10503</th>\n",
              "      <td>anak2 indonesia pertumbuham badanya menurun ke...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10504</th>\n",
              "      <td>papua original asian nigga</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10505</th>\n",
              "      <td>ü§£ü§£ü§£ü§£ü§£</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-255590b6-368e-4a05-8dc2-692d1e9ec086')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-255590b6-368e-4a05-8dc2-692d1e9ec086 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-255590b6-368e-4a05-8dc2-692d1e9ec086');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b0c49bfc-2ff7-4ece-8f3e-b9fe0a0b7de5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b0c49bfc-2ff7-4ece-8f3e-b9fe0a0b7de5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b0c49bfc-2ff7-4ece-8f3e-b9fe0a0b7de5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_all\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"papua original asian nigga\",\n          \"bang review movie blood brothers bara naga\",\n          \"sih tulisan gabung beranda chanel youtube lu subscribe\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "class HomoglyphExtractor:\n",
        "    \"\"\"\n",
        "    Ekstraksi kar.akter homoglyph dari dataset\n",
        "    Mendeteksi karakter Unicode yang mirip huruf/angka normal tapi beda encoding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Range Unicode untuk karakter normal (baseline)\n",
        "        self.normal_ranges = [\n",
        "            (0x0020, 0x007E),  # Basic Latin (spasi sampai ~)\n",
        "            (0x00A0, 0x00FF),  # Latin-1 Supplement\n",
        "        ]\n",
        "\n",
        "        # Range Unicode untuk emoji (akan di-exclude)\n",
        "        self.emoji_ranges = [\n",
        "            (0x1F600, 0x1F64F),  # Emoticons\n",
        "            (0x1F300, 0x1F5FF),  # Symbols & Pictographs\n",
        "            (0x1F680, 0x1F6FF),  # Transport & Map\n",
        "            (0x1F1E0, 0x1F1FF),  # Flags\n",
        "            (0x2600, 0x26FF),    # Miscellaneous Symbols\n",
        "            (0x2700, 0x27BF),    # Dingbats\n",
        "            (0xFE00, 0xFE0F),    # Variation Selectors\n",
        "            (0x1F900, 0x1F9FF),  # Supplemental Symbols\n",
        "            (0x1FA70, 0x1FAFF),  # Symbols and Pictographs Extended-A\n",
        "        ]\n",
        "\n",
        "        # Kategori Unicode yang sering digunakan untuk homoglyph\n",
        "        self.suspicious_categories = [\n",
        "            'Lm',  # Letter, Modifier\n",
        "            'Sk',  # Symbol, Modifier\n",
        "            'So',  # Symbol, Other\n",
        "        ]\n",
        "\n",
        "    def is_emoji(self, char):\n",
        "        \"\"\"Cek apakah karakter adalah emoji\"\"\"\n",
        "        code_point = ord(char)\n",
        "        for start, end in self.emoji_ranges:\n",
        "            if start <= code_point <= end:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def is_normal_char(self, char):\n",
        "        \"\"\"Cek apakah karakter adalah ASCII/Latin normal\"\"\"\n",
        "        code_point = ord(char)\n",
        "        for start, end in self.normal_ranges:\n",
        "            if start <= code_point <= end:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def is_homoglyph(self, char):\n",
        "        \"\"\"\n",
        "        Deteksi homoglyph:\n",
        "        - Bukan karakter normal\n",
        "        - Bukan emoji\n",
        "        - Bukan whitespace biasa\n",
        "        - Adalah huruf/angka/simbol yang terlihat mirip normal\n",
        "        \"\"\"\n",
        "        # Skip whitespace biasa\n",
        "        if char in [' ', '\\t', '\\n', '\\r']:\n",
        "            return False\n",
        "\n",
        "        # Skip emoji\n",
        "        if self.is_emoji(char):\n",
        "            return False\n",
        "\n",
        "        # Skip karakter normal\n",
        "        if self.is_normal_char(char):\n",
        "            return False\n",
        "\n",
        "        # Dapatkan kategori Unicode\n",
        "        try:\n",
        "            category = unicodedata.category(char)\n",
        "            name = unicodedata.name(char, '')\n",
        "\n",
        "            # Karakter yang terlihat seperti huruf/angka\n",
        "            # Category: L* (Letter), N* (Number), atau simbol tertentu\n",
        "            is_letter_like = category.startswith('L')\n",
        "            is_number_like = category.startswith('N')\n",
        "            is_suspicious_symbol = category in self.suspicious_categories\n",
        "\n",
        "            # Kata kunci dalam nama Unicode yang mengindikasikan homoglyph\n",
        "            homoglyph_keywords = [\n",
        "                'MATHEMATICAL', 'BOLD', 'ITALIC', 'SCRIPT', 'FRAKTUR',\n",
        "                'DOUBLE-STRUCK', 'SANS-SERIF', 'MONOSPACE',\n",
        "                'FULLWIDTH', 'HALFWIDTH', 'CIRCLED', 'PARENTHESIZED',\n",
        "                'SQUARED', 'NEGATIVE', 'REGIONAL', 'TAG'\n",
        "            ]\n",
        "\n",
        "            has_homoglyph_keyword = any(keyword in name for keyword in homoglyph_keywords)\n",
        "\n",
        "            return (is_letter_like or is_number_like or is_suspicious_symbol) and has_homoglyph_keyword\n",
        "\n",
        "        except (ValueError, TypeError):\n",
        "            # Jika tidak bisa mendapat info Unicode, anggap bukan homoglyph\n",
        "            return False\n",
        "\n",
        "    def extract_homoglyphs_from_text(self, text):\n",
        "        \"\"\"Ekstraksi semua homoglyph dari satu teks\"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return []\n",
        "\n",
        "        homoglyphs = []\n",
        "        for char in text:\n",
        "            if self.is_homoglyph(char):\n",
        "                try:\n",
        "                    name = unicodedata.name(char, 'UNKNOWN')\n",
        "                    code_point = f\"U+{ord(char):04X}\"\n",
        "                    homoglyphs.append({\n",
        "                        'char': char,\n",
        "                        'unicode_name': name,\n",
        "                        'code_point': code_point,\n",
        "                        'category': unicodedata.category(char)\n",
        "                    })\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return homoglyphs\n",
        "\n",
        "    def analyze_dataset(self, df, text_column='comment'):\n",
        "        \"\"\"\n",
        "        Analisis dataset dan ekstraksi semua homoglyph yang ditemukan\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas DataFrame\n",
        "            Dataset dengan kolom teks\n",
        "        text_column : str\n",
        "            Nama kolom yang berisi teks (default: 'comment')\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Hasil analisis lengkap\n",
        "        \"\"\"\n",
        "        print(f\"Menganalisis kolom '{text_column}'...\")\n",
        "        print(f\"Total rows: {len(df)}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        all_homoglyphs = []\n",
        "        rows_with_homoglyphs = []\n",
        "\n",
        "        # Iterasi setiap baris\n",
        "        for idx, row in df.iterrows():\n",
        "            text = row[text_column]\n",
        "            homoglyphs = self.extract_homoglyphs_from_text(text)\n",
        "\n",
        "            if homoglyphs:\n",
        "                rows_with_homoglyphs.append({\n",
        "                    'index': idx,\n",
        "                    'text': text,\n",
        "                    'homoglyphs': homoglyphs,\n",
        "                    'homoglyph_count': len(homoglyphs)\n",
        "                })\n",
        "                all_homoglyphs.extend(homoglyphs)\n",
        "\n",
        "        # Statistik homoglyph\n",
        "        homoglyph_chars = [h['char'] for h in all_homoglyphs]\n",
        "        homoglyph_counter = Counter(homoglyph_chars)\n",
        "\n",
        "        # Buat mapping untuk normalisasi\n",
        "        homoglyph_mapping = self._create_normalization_mapping(all_homoglyphs)\n",
        "\n",
        "        results = {\n",
        "            'total_rows': len(df),\n",
        "            'rows_with_homoglyphs': len(rows_with_homoglyphs),\n",
        "            'total_homoglyph_chars': len(all_homoglyphs),\n",
        "            'unique_homoglyphs': len(homoglyph_counter),\n",
        "            'homoglyph_frequency': homoglyph_counter,\n",
        "            'detailed_rows': rows_with_homoglyphs,\n",
        "            'normalization_mapping': homoglyph_mapping\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _create_normalization_mapping(self, homoglyphs):\n",
        "        \"\"\"\n",
        "        Buat mapping otomatis dari homoglyph ke karakter normal\n",
        "        Berdasarkan nama Unicode\n",
        "        \"\"\"\n",
        "        mapping = {}\n",
        "\n",
        "        for h in homoglyphs:\n",
        "            char = h['char']\n",
        "            name = h['unicode_name']\n",
        "\n",
        "            if char in mapping:\n",
        "                continue\n",
        "\n",
        "            # Ekstraksi karakter normal dari nama Unicode\n",
        "            # Contoh: \"MATHEMATICAL BOLD CAPITAL A\" -> \"A\"\n",
        "            normal_char = self._extract_normal_char_from_name(name)\n",
        "            if normal_char:\n",
        "                mapping[char] = normal_char\n",
        "\n",
        "        return mapping\n",
        "\n",
        "    def _extract_normal_char_from_name(self, unicode_name):\n",
        "        \"\"\"Ekstraksi karakter normal dari nama Unicode\"\"\"\n",
        "        # Pattern untuk huruf kapital\n",
        "        if 'CAPITAL' in unicode_name or 'UPPER' in unicode_name:\n",
        "            # Cari huruf A-Z di akhir nama\n",
        "            match = re.search(r'\\b([A-Z])\\b', unicode_name[::-1])\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        # Pattern untuk huruf kecil\n",
        "        if 'SMALL' in unicode_name or 'LOWER' in unicode_name:\n",
        "            # Map ke huruf kecil\n",
        "            match = re.search(r'\\b([A-Z])\\b', unicode_name[::-1])\n",
        "            if match:\n",
        "                return match.group(1).lower()\n",
        "\n",
        "        # Pattern untuk angka\n",
        "        if 'DIGIT' in unicode_name:\n",
        "            match = re.search(r'DIGIT (\\w+)', unicode_name)\n",
        "            if match:\n",
        "                digit_name = match.group(1)\n",
        "                digit_map = {\n",
        "                    'ZERO': '0', 'ONE': '1', 'TWO': '2', 'THREE': '3',\n",
        "                    'FOUR': '4', 'FIVE': '5', 'SIX': '6', 'SEVEN': '7',\n",
        "                    'EIGHT': '8', 'NINE': '9'\n",
        "                }\n",
        "                return digit_map.get(digit_name)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def print_summary(self, results):\n",
        "        \"\"\"Cetak ringkasan hasil analisis\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RINGKASAN ANALISIS HOMOGLYPH\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\nüìä Statistik:\")\n",
        "        print(f\"  - Total baris dalam dataset: {results['total_rows']}\")\n",
        "        print(f\"  - Baris yang mengandung homoglyph: {results['rows_with_homoglyphs']}\")\n",
        "        print(f\"  - Persentase: {results['rows_with_homoglyphs']/results['total_rows']*100:.2f}%\")\n",
        "        print(f\"  - Total karakter homoglyph ditemukan: {results['total_homoglyph_chars']}\")\n",
        "        print(f\"  - Unique homoglyph characters: {results['unique_homoglyphs']}\")\n",
        "\n",
        "        print(f\"\\nüî§ Top 10 Homoglyph Paling Sering Muncul:\")\n",
        "        for char, count in results['homoglyph_frequency'].most_common(10):\n",
        "            try:\n",
        "                name = unicodedata.name(char, 'UNKNOWN')\n",
        "                code = f\"U+{ord(char):04X}\"\n",
        "                normal = results['normalization_mapping'].get(char, '?')\n",
        "                print(f\"  '{char}' ‚Üí '{normal}'  |  {code}  |  {count}x  |  {name}\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        print(f\"\\nüìù Contoh Komentar dengan Homoglyph (5 pertama):\")\n",
        "        for i, row in enumerate(results['detailed_rows'][:5], 1):\n",
        "            print(f\"\\n  [{i}] Index: {row['index']}\")\n",
        "            print(f\"      Text: {row['text'][:100]}{'...' if len(row['text']) > 100 else ''}\")\n",
        "            print(f\"      Homoglyphs found: {row['homoglyph_count']}\")\n",
        "            unique_chars = list(set([h['char'] for h in row['homoglyphs']]))\n",
        "            print(f\"      Characters: {', '.join(unique_chars)}\")\n",
        "\n",
        "    def export_mapping_code(self, results, output_file='homoglyph_mapping.py'):\n",
        "        \"\"\"\n",
        "        Export mapping ke file Python yang bisa langsung digunakan\n",
        "        \"\"\"\n",
        "        mapping = results['normalization_mapping']\n",
        "\n",
        "        code = \"# Auto-generated homoglyph mapping\\n\"\n",
        "        code += \"# Generated from dataset analysis\\n\\n\"\n",
        "        code += \"HOMOGLYPH_MAP = {\\n\"\n",
        "\n",
        "        for homo, normal in sorted(mapping.items()):\n",
        "            try:\n",
        "                name = unicodedata.name(homo, 'UNKNOWN')\n",
        "                code += f\"    '{homo}': '{normal}',  # {name}\\n\"\n",
        "            except:\n",
        "                code += f\"    '{homo}': '{normal}',\\n\"\n",
        "\n",
        "        code += \"}\\n\\n\"\n",
        "        code += \"def normalize_homoglyph(text):\\n\"\n",
        "        code += \"    \\\"\\\"\\\"Normalize homoglyph characters to normal ASCII\\\"\\\"\\\"\\n\"\n",
        "        code += \"    for homo, normal in HOMOGLYPH_MAP.items():\\n\"\n",
        "        code += \"        text = text.replace(homo, normal)\\n\"\n",
        "        code += \"    return text\\n\"\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(code)\n",
        "\n",
        "        print(f\"\\n‚úÖ Mapping code exported to: {output_file}\")\n",
        "        return code\n"
      ],
      "metadata": {
        "id": "GNHBM6fWwZz9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = HomoglyphExtractor()\n",
        "\n",
        "# Analisis dataset\n",
        "results = extractor.analyze_dataset(df_all, text_column='comment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Km87FgWwsEJ",
        "outputId": "1b3aa1e6-5930-4488-adbc-c7ee930e8f64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Menganalisis kolom 'comment'...\n",
            "Total rows: 10506\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extractor.print_summary(results)\n",
        "\n",
        "# Export mapping ke file Python\n",
        "mapping_code = extractor.export_mapping_code(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-OXTLf8wulB",
        "outputId": "d373408d-e6ee-4927-e96a-0ce61755b95a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RINGKASAN ANALISIS HOMOGLYPH\n",
            "================================================================================\n",
            "\n",
            "üìä Statistik:\n",
            "  - Total baris dalam dataset: 10506\n",
            "  - Baris yang mengandung homoglyph: 801\n",
            "  - Persentase: 7.62%\n",
            "  - Total karakter homoglyph ditemukan: 4889\n",
            "  - Unique homoglyph characters: 197\n",
            "\n",
            "üî§ Top 10 Homoglyph Paling Sering Muncul:\n",
            "  'ùü¥' ‚Üí '8'  |  U+1D7F4  |  307x  |  MATHEMATICAL SANS-SERIF BOLD DIGIT EIGHT\n",
            "  'ùóß' ‚Üí 'T'  |  U+1D5E7  |  194x  |  MATHEMATICAL SANS-SERIF BOLD CAPITAL T\n",
            "  'ùòº' ‚Üí 'A'  |  U+1D63C  |  174x  |  MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL A\n",
            "  'ùêÄ' ‚Üí 'A'  |  U+1D400  |  156x  |  MATHEMATICAL BOLD CAPITAL A\n",
            "  'ùó¢' ‚Üí 'O'  |  U+1D5E2  |  146x  |  MATHEMATICAL SANS-SERIF BOLD CAPITAL O\n",
            "  'ùó®' ‚Üí 'U'  |  U+1D5E8  |  140x  |  MATHEMATICAL SANS-SERIF BOLD CAPITAL U\n",
            "  'ùü©' ‚Üí '7'  |  U+1D7E9  |  125x  |  MATHEMATICAL SANS-SERIF DIGIT SEVEN\n",
            "  'ùòñ' ‚Üí 'O'  |  U+1D616  |  124x  |  MATHEMATICAL SANS-SERIF ITALIC CAPITAL O\n",
            "  'ùüñ' ‚Üí '8'  |  U+1D7D6  |  117x  |  MATHEMATICAL BOLD DIGIT EIGHT\n",
            "  'ùëÇ' ‚Üí 'O'  |  U+1D442  |  111x  |  MATHEMATICAL ITALIC CAPITAL O\n",
            "\n",
            "üìù Contoh Komentar dengan Homoglyph (5 pertama):\n",
            "\n",
            "  [1] Index: 27\n",
            "      Text: ùêÉ ùêé ùôç ùòà ùüΩ 7 emang gachor parah\n",
            "      Homoglyphs found: 5\n",
            "      Characters: ùüΩ, ùòà, ùôç, ùêÉ, ùêé\n",
            "\n",
            "  [2] Index: 33\n",
            "      Text: main d ùê∏ ùêñ ùêÄ d ùëÇ ùëÖ a bikin hariku menyenangkan terima kasih üçû\n",
            "      Homoglyphs found: 5\n",
            "      Characters: ùê∏, ùêÄ, ùëÇ, ùêñ, ùëÖ\n",
            "\n",
            "  [3] Index: 43\n",
            "      Text: 225 langsung ùêíùêÜùêàùüñùüñ bang üòπüòπ\n",
            "      Homoglyphs found: 5\n",
            "      Characters: ùêÜ, ùêí, ùêà, ùüñ\n",
            "\n",
            "  [4] Index: 58\n",
            "      Text: mencari platform mendorong pertumbuhan ùòø ùêÑ w a ùòã –æ ùëÖ –∞layak dicoba üç≠\n",
            "      Homoglyphs found: 4\n",
            "      Characters: ùòã, ùêÑ, ùòø, ùëÖ\n",
            "\n",
            "  [5] Index: 71\n",
            "      Text: makasih ùêäùó®ùó¶ùó®ùó†ùóîùóßùüéùóßùüéuda ngasih üîç ‚úÖ\n",
            "      Homoglyphs found: 10\n",
            "      Characters: ùóß, ùóî, ùêä, ùó®, ùüé, ùó¶, ùó†\n",
            "\n",
            "‚úÖ Mapping code exported to: homoglyph_mapping.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homoglyph Helper"
      ],
      "metadata": {
        "id": "R-Q3EZzPxaXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto-generated homoglyph mapping\n",
        "# Generated from dataset analysis\n",
        "\n",
        "HOMOGLYPH_MAP = {\n",
        "    '‚Ñç': 'H',  # DOUBLE-STRUCK CAPITAL H\n",
        "    'Ôºë': '1',  # FULLWIDTH DIGIT ONE\n",
        "    'Ôºì': '3',  # FULLWIDTH DIGIT THREE\n",
        "    'Ôºò': '8',  # FULLWIDTH DIGIT EIGHT\n",
        "    'ÔΩÅ': 'a',  # FULLWIDTH LATIN SMALL LETTER A\n",
        "    'ÔΩÑ': 'd',  # FULLWIDTH LATIN SMALL LETTER D\n",
        "    'ÔΩÖ': 'e',  # FULLWIDTH LATIN SMALL LETTER E\n",
        "    'ÔΩà': 'h',  # FULLWIDTH LATIN SMALL LETTER H\n",
        "    'ÔΩâ': 'i',  # FULLWIDTH LATIN SMALL LETTER I\n",
        "    'ÔΩä': 'j',  # FULLWIDTH LATIN SMALL LETTER J\n",
        "    'ÔΩå': 'l',  # FULLWIDTH LATIN SMALL LETTER L\n",
        "    'ÔΩé': 'n',  # FULLWIDTH LATIN SMALL LETTER N\n",
        "    'ÔΩè': 'o',  # FULLWIDTH LATIN SMALL LETTER O\n",
        "    'ÔΩê': 'p',  # FULLWIDTH LATIN SMALL LETTER P\n",
        "    'ÔΩì': 's',  # FULLWIDTH LATIN SMALL LETTER S\n",
        "    'ÔΩî': 't',  # FULLWIDTH LATIN SMALL LETTER T\n",
        "    'ÔΩï': 'u',  # FULLWIDTH LATIN SMALL LETTER U\n",
        "    'ÔΩó': 'w',  # FULLWIDTH LATIN SMALL LETTER W\n",
        "    'ùêÄ': 'A',  # MATHEMATICAL BOLD CAPITAL A\n",
        "    'ùêÅ': 'B',  # MATHEMATICAL BOLD CAPITAL B\n",
        "    'ùêÉ': 'D',  # MATHEMATICAL BOLD CAPITAL D\n",
        "    'ùêÑ': 'E',  # MATHEMATICAL BOLD CAPITAL E\n",
        "    'ùêÜ': 'G',  # MATHEMATICAL BOLD CAPITAL G\n",
        "    'ùêá': 'H',  # MATHEMATICAL BOLD CAPITAL H\n",
        "    'ùêà': 'I',  # MATHEMATICAL BOLD CAPITAL I\n",
        "    'ùêä': 'K',  # MATHEMATICAL BOLD CAPITAL K\n",
        "    'ùêã': 'L',  # MATHEMATICAL BOLD CAPITAL L\n",
        "    'ùêå': 'M',  # MATHEMATICAL BOLD CAPITAL M\n",
        "    'ùêç': 'N',  # MATHEMATICAL BOLD CAPITAL N\n",
        "    'ùêé': 'O',  # MATHEMATICAL BOLD CAPITAL O\n",
        "    'ùêë': 'R',  # MATHEMATICAL BOLD CAPITAL R\n",
        "    'ùêí': 'S',  # MATHEMATICAL BOLD CAPITAL S\n",
        "    'ùêì': 'T',  # MATHEMATICAL BOLD CAPITAL T\n",
        "    'ùêî': 'U',  # MATHEMATICAL BOLD CAPITAL U\n",
        "    'ùêï': 'V',  # MATHEMATICAL BOLD CAPITAL V\n",
        "    'ùêñ': 'W',  # MATHEMATICAL BOLD CAPITAL W\n",
        "    'ùêó': 'X',  # MATHEMATICAL BOLD CAPITAL X\n",
        "    'ùêò': 'Y',  # MATHEMATICAL BOLD CAPITAL Y\n",
        "    'ùêö': 'a',  # MATHEMATICAL BOLD SMALL A\n",
        "    'ùêõ': 'b',  # MATHEMATICAL BOLD SMALL B\n",
        "    'ùêù': 'd',  # MATHEMATICAL BOLD SMALL D\n",
        "    'ùêû': 'e',  # MATHEMATICAL BOLD SMALL E\n",
        "    'ùê†': 'g',  # MATHEMATICAL BOLD SMALL G\n",
        "    'ùê¢': 'i',  # MATHEMATICAL BOLD SMALL I\n",
        "    'ùê£': 'j',  # MATHEMATICAL BOLD SMALL J\n",
        "    'ùê•': 'l',  # MATHEMATICAL BOLD SMALL L\n",
        "    'ùêß': 'n',  # MATHEMATICAL BOLD SMALL N\n",
        "    'ùê®': 'o',  # MATHEMATICAL BOLD SMALL O\n",
        "    'ùê´': 'r',  # MATHEMATICAL BOLD SMALL R\n",
        "    'ùê¨': 's',  # MATHEMATICAL BOLD SMALL S\n",
        "    'ùê≠': 't',  # MATHEMATICAL BOLD SMALL T\n",
        "    'ùê≤': 'y',  # MATHEMATICAL BOLD SMALL Y\n",
        "    'ùê¥': 'A',  # MATHEMATICAL ITALIC CAPITAL A\n",
        "    'ùê∑': 'D',  # MATHEMATICAL ITALIC CAPITAL D\n",
        "    'ùê∏': 'E',  # MATHEMATICAL ITALIC CAPITAL E\n",
        "    'ùê∫': 'G',  # MATHEMATICAL ITALIC CAPITAL G\n",
        "    'ùêª': 'H',  # MATHEMATICAL ITALIC CAPITAL H\n",
        "    'ùêº': 'I',  # MATHEMATICAL ITALIC CAPITAL I\n",
        "    'ùêø': 'L',  # MATHEMATICAL ITALIC CAPITAL L\n",
        "    'ùëÄ': 'M',  # MATHEMATICAL ITALIC CAPITAL M\n",
        "    'ùëÇ': 'O',  # MATHEMATICAL ITALIC CAPITAL O\n",
        "    'ùëÖ': 'R',  # MATHEMATICAL ITALIC CAPITAL R\n",
        "    'ùëÜ': 'S',  # MATHEMATICAL ITALIC CAPITAL S\n",
        "    'ùëá': 'T',  # MATHEMATICAL ITALIC CAPITAL T\n",
        "    'ùëà': 'U',  # MATHEMATICAL ITALIC CAPITAL U\n",
        "    'ùëä': 'W',  # MATHEMATICAL ITALIC CAPITAL W\n",
        "    'ùëã': 'X',  # MATHEMATICAL ITALIC CAPITAL X\n",
        "    'ùë®': 'A',  # MATHEMATICAL BOLD ITALIC CAPITAL A\n",
        "    'ùë™': 'C',  # MATHEMATICAL BOLD ITALIC CAPITAL C\n",
        "    'ùë´': 'D',  # MATHEMATICAL BOLD ITALIC CAPITAL D\n",
        "    'ùëÆ': 'G',  # MATHEMATICAL BOLD ITALIC CAPITAL G\n",
        "    'ùë∞': 'I',  # MATHEMATICAL BOLD ITALIC CAPITAL I\n",
        "    'ùë≤': 'K',  # MATHEMATICAL BOLD ITALIC CAPITAL K\n",
        "    'ùë≥': 'L',  # MATHEMATICAL BOLD ITALIC CAPITAL L\n",
        "    'ùë¥': 'M',  # MATHEMATICAL BOLD ITALIC CAPITAL M\n",
        "    'ùëµ': 'N',  # MATHEMATICAL BOLD ITALIC CAPITAL N\n",
        "    'ùë∂': 'O',  # MATHEMATICAL BOLD ITALIC CAPITAL O\n",
        "    'ùë∫': 'S',  # MATHEMATICAL BOLD ITALIC CAPITAL S\n",
        "    'ùëª': 'T',  # MATHEMATICAL BOLD ITALIC CAPITAL T\n",
        "    'ùíÄ': 'Y',  # MATHEMATICAL BOLD ITALIC CAPITAL Y\n",
        "    'ùíú': 'A',  # MATHEMATICAL SCRIPT CAPITAL A\n",
        "    'ùí¢': 'G',  # MATHEMATICAL SCRIPT CAPITAL G\n",
        "    'ùí©': 'N',  # MATHEMATICAL SCRIPT CAPITAL N\n",
        "    'ùí´': 'P',  # MATHEMATICAL SCRIPT CAPITAL P\n",
        "    'ùíÆ': 'S',  # MATHEMATICAL SCRIPT CAPITAL S\n",
        "    'ùíØ': 'T',  # MATHEMATICAL SCRIPT CAPITAL T\n",
        "    'ùí∞': 'U',  # MATHEMATICAL SCRIPT CAPITAL U\n",
        "    'ùìê': 'A',  # MATHEMATICAL BOLD SCRIPT CAPITAL A\n",
        "    'ùìò': 'I',  # MATHEMATICAL BOLD SCRIPT CAPITAL I\n",
        "    'ùìõ': 'L',  # MATHEMATICAL BOLD SCRIPT CAPITAL L\n",
        "    'ùìù': 'N',  # MATHEMATICAL BOLD SCRIPT CAPITAL N\n",
        "    'ùìü': 'P',  # MATHEMATICAL BOLD SCRIPT CAPITAL P\n",
        "    'ùì§': 'U',  # MATHEMATICAL BOLD SCRIPT CAPITAL U\n",
        "    'ùì¶': 'W',  # MATHEMATICAL BOLD SCRIPT CAPITAL W\n",
        "    'ùïÇ': 'K',  # MATHEMATICAL DOUBLE-STRUCK CAPITAL K\n",
        "    'ùïí': 'a',  # MATHEMATICAL DOUBLE-STRUCK SMALL A\n",
        "    'ùïì': 'b',  # MATHEMATICAL DOUBLE-STRUCK SMALL B\n",
        "    'ùïï': 'd',  # MATHEMATICAL DOUBLE-STRUCK SMALL D\n",
        "    'ùïñ': 'e',  # MATHEMATICAL DOUBLE-STRUCK SMALL E\n",
        "    'ùïò': 'g',  # MATHEMATICAL DOUBLE-STRUCK SMALL G\n",
        "    'ùïô': 'h',  # MATHEMATICAL DOUBLE-STRUCK SMALL H\n",
        "    'ùïö': 'i',  # MATHEMATICAL DOUBLE-STRUCK SMALL I\n",
        "    'ùïü': 'n',  # MATHEMATICAL DOUBLE-STRUCK SMALL N\n",
        "    'ùï°': 'p',  # MATHEMATICAL DOUBLE-STRUCK SMALL P\n",
        "    'ùï£': 'r',  # MATHEMATICAL DOUBLE-STRUCK SMALL R\n",
        "    'ùï§': 's',  # MATHEMATICAL DOUBLE-STRUCK SMALL S\n",
        "    'ùï¶': 'u',  # MATHEMATICAL DOUBLE-STRUCK SMALL U\n",
        "    'ùñ™': 'K',  # MATHEMATICAL SANS-SERIF CAPITAL K\n",
        "    'ùñ∫': 'a',  # MATHEMATICAL SANS-SERIF SMALL A\n",
        "    'ùñæ': 'e',  # MATHEMATICAL SANS-SERIF SMALL E\n",
        "    'ùóÄ': 'g',  # MATHEMATICAL SANS-SERIF SMALL G\n",
        "    'ùóÅ': 'h',  # MATHEMATICAL SANS-SERIF SMALL H\n",
        "    'ùóÇ': 'i',  # MATHEMATICAL SANS-SERIF SMALL I\n",
        "    'ùóÉ': 'j',  # MATHEMATICAL SANS-SERIF SMALL J\n",
        "    'ùóÑ': 'k',  # MATHEMATICAL SANS-SERIF SMALL K\n",
        "    'ùóÖ': 'l',  # MATHEMATICAL SANS-SERIF SMALL L\n",
        "    'ùóÜ': 'm',  # MATHEMATICAL SANS-SERIF SMALL M\n",
        "    'ùóá': 'n',  # MATHEMATICAL SANS-SERIF SMALL N\n",
        "    'ùóà': 'o',  # MATHEMATICAL SANS-SERIF SMALL O\n",
        "    'ùóç': 't',  # MATHEMATICAL SANS-SERIF SMALL T\n",
        "    'ùóé': 'u',  # MATHEMATICAL SANS-SERIF SMALL U\n",
        "    'ùóí': 'y',  # MATHEMATICAL SANS-SERIF SMALL Y\n",
        "    'ùóî': 'A',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL A\n",
        "    'ùóï': 'B',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL B\n",
        "    'ùóñ': 'C',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL C\n",
        "    'ùóó': 'D',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL D\n",
        "    'ùóò': 'E',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL E\n",
        "    'ùóö': 'G',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL G\n",
        "    'ùóõ': 'H',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL H\n",
        "    'ùóú': 'I',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL I\n",
        "    'ùóù': 'J',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL J\n",
        "    'ùóû': 'K',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL K\n",
        "    'ùóü': 'L',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL L\n",
        "    'ùó†': 'M',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL M\n",
        "    'ùó°': 'N',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL N\n",
        "    'ùó¢': 'O',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL O\n",
        "    'ùó£': 'P',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL P\n",
        "    'ùó•': 'R',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL R\n",
        "    'ùó¶': 'S',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL S\n",
        "    'ùóß': 'T',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL T\n",
        "    'ùó®': 'U',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL U\n",
        "    'ùó™': 'W',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL W\n",
        "    'ùó¨': 'Y',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL Y\n",
        "    'ùóÆ': 'a',  # MATHEMATICAL SANS-SERIF BOLD SMALL A\n",
        "    'ùó≤': 'e',  # MATHEMATICAL SANS-SERIF BOLD SMALL E\n",
        "    'ùó¥': 'g',  # MATHEMATICAL SANS-SERIF BOLD SMALL G\n",
        "    'ùó∂': 'i',  # MATHEMATICAL SANS-SERIF BOLD SMALL I\n",
        "    'ùóπ': 'l',  # MATHEMATICAL SANS-SERIF BOLD SMALL L\n",
        "    'ùó∫': 'm',  # MATHEMATICAL SANS-SERIF BOLD SMALL M\n",
        "    'ùóª': 'n',  # MATHEMATICAL SANS-SERIF BOLD SMALL N\n",
        "    'ùóº': 'o',  # MATHEMATICAL SANS-SERIF BOLD SMALL O\n",
        "    'ùóø': 'r',  # MATHEMATICAL SANS-SERIF BOLD SMALL R\n",
        "    'ùòÄ': 's',  # MATHEMATICAL SANS-SERIF BOLD SMALL S\n",
        "    'ùòÇ': 'u',  # MATHEMATICAL SANS-SERIF BOLD SMALL U\n",
        "    'ùòÑ': 'w',  # MATHEMATICAL SANS-SERIF BOLD SMALL W\n",
        "    'ùòÜ': 'y',  # MATHEMATICAL SANS-SERIF BOLD SMALL Y\n",
        "    'ùòà': 'A',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL A\n",
        "    'ùòã': 'D',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL D\n",
        "    'ùòå': 'E',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL E\n",
        "    'ùòé': 'G',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL G\n",
        "    'ùòè': 'H',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL H\n",
        "    'ùòì': 'L',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL L\n",
        "    'ùòî': 'M',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL M\n",
        "    'ùòñ': 'O',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL O\n",
        "    'ùòô': 'R',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL R\n",
        "    'ùòö': 'S',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL S\n",
        "    'ùòõ': 'T',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL T\n",
        "    'ùòú': 'U',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL U\n",
        "    'ùòû': 'W',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL W\n",
        "    'ùòü': 'X',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL X\n",
        "    'ùò¶': 'e',  # MATHEMATICAL SANS-SERIF ITALIC SMALL E\n",
        "    'ùò¨': 'k',  # MATHEMATICAL SANS-SERIF ITALIC SMALL K\n",
        "    'ùò∂': 'u',  # MATHEMATICAL SANS-SERIF ITALIC SMALL U\n",
        "    'ùòº': 'A',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL A\n",
        "    'ùòΩ': 'B',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL B\n",
        "    'ùòø': 'D',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL D\n",
        "    'ùôÄ': 'E',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL E\n",
        "    'ùôÅ': 'F',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL F\n",
        "    'ùôÇ': 'G',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL G\n",
        "    'ùôÉ': 'H',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL H\n",
        "    'ùôÑ': 'I',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL I\n",
        "    'ùôÜ': 'K',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL K\n",
        "    'ùôá': 'L',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL L\n",
        "    'ùôà': 'M',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL M\n",
        "    'ùôâ': 'N',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL N\n",
        "    'ùôä': 'O',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL O\n",
        "    'ùôã': 'P',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL P\n",
        "    'ùôç': 'R',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL R\n",
        "    'ùôé': 'S',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL S\n",
        "    'ùôè': 'T',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL T\n",
        "    'ùôê': 'U',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL U\n",
        "    'ùôí': 'W',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL W\n",
        "    'ùôì': 'X',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL X\n",
        "    'ùôî': 'Y',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL Y\n",
        "    'ùô∂': 'G',  # MATHEMATICAL MONOSPACE CAPITAL G\n",
        "    'ùöä': 'a',  # MATHEMATICAL MONOSPACE SMALL A\n",
        "    'ùöã': 'b',  # MATHEMATICAL MONOSPACE SMALL B\n",
        "    'ùöù': 't',  # MATHEMATICAL MONOSPACE SMALL T\n",
        "    'ùöû': 'u',  # MATHEMATICAL MONOSPACE SMALL U\n",
        "    'ùüé': '0',  # MATHEMATICAL BOLD DIGIT ZERO\n",
        "    'ùüè': '1',  # MATHEMATICAL BOLD DIGIT ONE\n",
        "    'ùüê': '2',  # MATHEMATICAL BOLD DIGIT TWO\n",
        "    'ùüë': '3',  # MATHEMATICAL BOLD DIGIT THREE\n",
        "    'ùüí': '4',  # MATHEMATICAL BOLD DIGIT FOUR\n",
        "    'ùüï': '7',  # MATHEMATICAL BOLD DIGIT SEVEN\n",
        "    'ùüñ': '8',  # MATHEMATICAL BOLD DIGIT EIGHT\n",
        "    'ùü©': '7',  # MATHEMATICAL SANS-SERIF DIGIT SEVEN\n",
        "    'ùü™': '8',  # MATHEMATICAL SANS-SERIF DIGIT EIGHT\n",
        "    'ùüÆ': '2',  # MATHEMATICAL SANS-SERIF BOLD DIGIT TWO\n",
        "    'ùü≥': '7',  # MATHEMATICAL SANS-SERIF BOLD DIGIT SEVEN\n",
        "    'ùü¥': '8',  # MATHEMATICAL SANS-SERIF BOLD DIGIT EIGHT\n",
        "    'ùüµ': '9',  # MATHEMATICAL SANS-SERIF BOLD DIGIT NINE\n",
        "    'ùüΩ': '7',  # MATHEMATICAL MONOSPACE DIGIT SEVEN\n",
        "    'ùüæ': '8',  # MATHEMATICAL MONOSPACE DIGIT EIGHT\n",
        "}\n",
        "\n",
        "def normalize_homoglyph(text):\n",
        "    \"\"\"Normalize homoglyph characters to normal ASCII\"\"\"\n",
        "    for homo, normal in HOMOGLYPH_MAP.items():\n",
        "        text = text.replace(homo, normal)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "4lutN-t7xdfp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "BR--9Mc_1a-9",
        "outputId": "3da24c9a-ae1a-409d-9ff5-32f2f84cf989"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "0615ca777eb44d89892844845c52b943"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import re\n",
        "import unicodedata\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================\n",
        "# PREPROCESSING & FEATURE ENGINEERING\n",
        "# ============================================================\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Preprocessing untuk menangani homoglyph dan variasi Unicode\"\"\"\n",
        "    def normalize_homoglyph(self, text):\n",
        "        \"\"\"Konversi homoglyph Unicode ke karakter normal\"\"\"\n",
        "        for homo, normal in HOMOGLYPH_MAP.items():\n",
        "            text = text.replace(homo, normal)\n",
        "        return text\n",
        "\n",
        "    def normalize_unicode(self, text):\n",
        "        \"\"\"Normalisasi Unicode menggunakan NFKD\"\"\"\n",
        "        return unicodedata.normalize('NFKD', text)\n",
        "\n",
        "    def remove_extra_spaces(self, text):\n",
        "        \"\"\"Hapus spasi berlebih\"\"\"\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Pipeline preprocessing lengkap\"\"\"\n",
        "        text = str(text).lower()\n",
        "        text = self.normalize_homoglyph(text)\n",
        "        text = self.normalize_unicode(text)\n",
        "        text = self.remove_extra_spaces(text)\n",
        "        return text\n",
        "\n",
        "\n",
        "class AdditionalFeatures:\n",
        "    \"\"\"Ekstraksi fitur tambahan untuk deteksi spam\"\"\"\n",
        "\n",
        "    def count_emoji(self, text):\n",
        "        \"\"\"Hitung jumlah emoji\"\"\"\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "        return len(emoji_pattern.findall(text))\n",
        "\n",
        "    def capital_ratio(self, text):\n",
        "        \"\"\"Rasio huruf kapital\"\"\"\n",
        "        if len(text) == 0:\n",
        "            return 0\n",
        "        return sum(1 for c in text if c.isupper()) / len(text)\n",
        "\n",
        "    def has_numbers_in_word(self, text):\n",
        "        \"\"\"Deteksi angka dalam kata (SLOT88, PLUTO88)\"\"\"\n",
        "        pattern = r'[a-zA-Z]+\\d+|\\d+[a-zA-Z]+'\n",
        "        return len(re.findall(pattern, text))\n",
        "\n",
        "    def excessive_spacing(self, text):\n",
        "        \"\"\"Deteksi spasi berlebih antar karakter\"\"\"\n",
        "        pattern = r'(\\w\\s){3,}'\n",
        "        return len(re.findall(pattern, text))\n",
        "\n",
        "    def extract_features(self, texts):\n",
        "        \"\"\"Ekstraksi semua fitur\"\"\"\n",
        "        features = []\n",
        "        for text in texts:\n",
        "            features.append([\n",
        "                self.count_emoji(text),\n",
        "                self.capital_ratio(text),\n",
        "                self.has_numbers_in_word(text),\n",
        "                self.excessive_spacing(text)\n",
        "            ])\n",
        "        return np.array(features)\n",
        "\n",
        "\n",
        "class AdditionalFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk fitur tambahan\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_extractor = AdditionalFeatures()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.feature_extractor.extract_features(X)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# WORD2VEC & FASTTEXT TRANSFORMERS\n",
        "# ============================================================\n",
        "\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk Word2Vec embedding\"\"\"\n",
        "\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, sg=0):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.sg = sg  # 0=CBOW, 1=Skip-gram\n",
        "        self.model = None\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Train Word2Vec model\"\"\"\n",
        "        # Tokenize sentences\n",
        "        sentences = [self.preprocessor.preprocess(text).split() for text in X]\n",
        "\n",
        "        # Train Word2Vec\n",
        "        self.model = Word2Vec(\n",
        "            sentences=sentences,\n",
        "            vector_size=self.vector_size,\n",
        "            window=self.window,\n",
        "            min_count=self.min_count,\n",
        "            workers=self.workers,\n",
        "            sg=self.sg\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform texts to averaged word vectors\"\"\"\n",
        "        vectors = []\n",
        "        for text in X:\n",
        "            text = self.preprocessor.preprocess(text)\n",
        "            words = text.split()\n",
        "\n",
        "            # Get vectors for words in vocabulary\n",
        "            word_vectors = [\n",
        "                self.model.wv[word] for word in words\n",
        "                if word in self.model.wv\n",
        "            ]\n",
        "\n",
        "            # Average word vectors\n",
        "            if word_vectors:\n",
        "                vectors.append(np.mean(word_vectors, axis=0))\n",
        "            else:\n",
        "                vectors.append(np.zeros(self.vector_size))\n",
        "\n",
        "        return np.array(vectors)\n",
        "\n",
        "\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk FastText embedding\"\"\"\n",
        "\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, sg=0):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.sg = sg  # 0=CBOW, 1=Skip-gram\n",
        "        self.model = None\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Train FastText model\"\"\"\n",
        "        sentences = [self.preprocessor.preprocess(text).split() for text in X]\n",
        "\n",
        "        self.model = FastText(\n",
        "            sentences=sentences,\n",
        "            vector_size=self.vector_size,\n",
        "            window=self.window,\n",
        "            min_count=self.min_count,\n",
        "            workers=self.workers,\n",
        "            sg=self.sg\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform texts to averaged word vectors\"\"\"\n",
        "        vectors = []\n",
        "        for text in X:\n",
        "            text = self.preprocessor.preprocess(text)\n",
        "            words = text.split()\n",
        "\n",
        "            # FastText can handle OOV words\n",
        "            word_vectors = [self.model.wv[word] for word in words if words]\n",
        "\n",
        "            if word_vectors:\n",
        "                vectors.append(np.mean(word_vectors, axis=0))\n",
        "            else:\n",
        "                vectors.append(np.zeros(self.vector_size))\n",
        "\n",
        "        return np.array(vectors)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MODEL CONFIGURATIONS\n",
        "# ============================================================\n",
        "\n",
        "def get_classifiers():\n",
        "    \"\"\"\n",
        "    Konfigurasi semua classifier yang tersedia\n",
        "    Returns dict: {nama_model: instance_model}\n",
        "    \"\"\"\n",
        "    classifiers = {\n",
        "        # Linear Models\n",
        "        'logistic_regression': LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            solver='lbfgs'\n",
        "        ),\n",
        "\n",
        "        # Tree-based Models\n",
        "        'random_forest': RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=20,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "\n",
        "        'decision_tree': DecisionTreeClassifier(\n",
        "            max_depth=20,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        'gradient_boosting': GradientBoostingClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        'xgboost': XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss'\n",
        "        ),\n",
        "\n",
        "        'lightgbm': LGBMClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42,\n",
        "            verbose=-1\n",
        "        ),\n",
        "\n",
        "        # SVM\n",
        "        'svm_linear': SVC(\n",
        "            kernel='linear',\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            probability=True\n",
        "        ),\n",
        "\n",
        "        'svm_rbf': SVC(\n",
        "            kernel='rbf',\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            probability=True\n",
        "        ),\n",
        "\n",
        "        # Naive Bayes\n",
        "        'naive_bayes': MultinomialNB(alpha=1.0),\n",
        "\n",
        "        # KNN\n",
        "        'knn': KNeighborsClassifier(\n",
        "            n_neighbors=5,\n",
        "            weights='distance',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    }\n",
        "\n",
        "    return classifiers\n",
        "\n",
        "\n",
        "def get_vectorizers():\n",
        "    \"\"\"\n",
        "    Konfigurasi semua vectorizer yang tersedia\n",
        "    Returns dict: {nama_vectorizer: config}\n",
        "    \"\"\"\n",
        "    preprocessor = TextPreprocessor()\n",
        "\n",
        "    vectorizers = {\n",
        "        # TF-IDF variants\n",
        "        'tfidf_char': TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'tfidf_word': TfidfVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'tfidf_char_wb': TfidfVectorizer(\n",
        "            analyzer='char_wb',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Count Vectorizer\n",
        "        'count_char': CountVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'count_word': CountVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Hashing Vectorizer (memory efficient)\n",
        "        'hashing_char': HashingVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            n_features=2**16,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Word2Vec\n",
        "        'word2vec_cbow': Word2VecTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=0  # CBOW\n",
        "        ),\n",
        "\n",
        "        'word2vec_skipgram': Word2VecTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=1  # Skip-gram\n",
        "        ),\n",
        "\n",
        "        # FastText\n",
        "        'fasttext_cbow': FastTextTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=0  # CBOW\n",
        "        ),\n",
        "\n",
        "        'fasttext_skipgram': FastTextTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=1  # Skip-gram\n",
        "        ),\n",
        "\n",
        "        # Hybrid combinations\n",
        "        'hybrid_word_char': FeatureUnion([\n",
        "            ('word_tfidf', TfidfVectorizer(\n",
        "                analyzer='word',\n",
        "                ngram_range=(1, 2),\n",
        "                max_features=3000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('char_tfidf', TfidfVectorizer(\n",
        "                analyzer='char',\n",
        "                ngram_range=(2, 5),\n",
        "                max_features=3000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            ))\n",
        "        ]),\n",
        "\n",
        "        'hybrid_all_features': FeatureUnion([\n",
        "            ('word_tfidf', TfidfVectorizer(\n",
        "                analyzer='word',\n",
        "                ngram_range=(1, 2),\n",
        "                max_features=2000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('char_tfidf', TfidfVectorizer(\n",
        "                analyzer='char',\n",
        "                ngram_range=(2, 5),\n",
        "                max_features=2000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('additional', AdditionalFeaturesTransformer())\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    return vectorizers\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PIPELINE BUILDER\n",
        "# ============================================================\n",
        "\n",
        "def create_custom_pipeline(vectorizer_name, classifier_name):\n",
        "    \"\"\"\n",
        "    Buat pipeline custom dengan kombinasi vectorizer dan classifier\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    vectorizer_name : str\n",
        "        Nama vectorizer dari get_vectorizers()\n",
        "    classifier_name : str\n",
        "        Nama classifier dari get_classifiers()\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Pipeline object\n",
        "    \"\"\"\n",
        "    vectorizers = get_vectorizers()\n",
        "    classifiers = get_classifiers()\n",
        "\n",
        "    if vectorizer_name not in vectorizers:\n",
        "        raise ValueError(f\"Vectorizer '{vectorizer_name}' tidak tersedia. \"\n",
        "                        f\"Pilihan: {list(vectorizers.keys())}\")\n",
        "\n",
        "    if classifier_name not in classifiers:\n",
        "        raise ValueError(f\"Classifier '{classifier_name}' tidak tersedia. \"\n",
        "                        f\"Pilihan: {list(classifiers.keys())}\")\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('vectorizer', vectorizers[vectorizer_name]),\n",
        "        ('classifier', classifiers[classifier_name])\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING & EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def train_and_evaluate(X, y, pipeline, pipeline_name):\n",
        "    \"\"\"Training dan evaluasi model\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUASI: {pipeline_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"Training model...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Prediction\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred,\n",
        "                                target_names=['Non-Judi', 'Judi']))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Cross-validation\n",
        "    print(\"\\nPerforming cross-validation...\")\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')\n",
        "    print(f\"Cross-Validation F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "    return pipeline, {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'cv_f1_mean': cv_scores.mean(),\n",
        "        'cv_f1_std': cv_scores.std()\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_multiple_models(X, y, vectorizer_configs, classifier_configs):\n",
        "    \"\"\"\n",
        "    Bandingkan multiple kombinasi vectorizer dan classifier\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like\n",
        "        Text data\n",
        "    y : array-like\n",
        "        Labels\n",
        "    vectorizer_configs : list of str\n",
        "        List nama vectorizer yang ingin dicoba\n",
        "    classifier_configs : list of str\n",
        "        List nama classifier yang ingin dicoba\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame dengan hasil perbandingan\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    total_experiments = len(vectorizer_configs) * len(classifier_configs)\n",
        "    experiment_num = 0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MEMULAI PERBANDINGAN {total_experiments} KOMBINASI MODEL\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for vec_name in vectorizer_configs:\n",
        "        for clf_name in classifier_configs:\n",
        "            experiment_num += 1\n",
        "            print(f\"\\n[{experiment_num}/{total_experiments}] Testing: {vec_name} + {clf_name}\")\n",
        "\n",
        "            try:\n",
        "                # Create pipeline\n",
        "                pipeline = create_custom_pipeline(vec_name, clf_name)\n",
        "\n",
        "                # Train and evaluate\n",
        "                model, metrics = train_and_evaluate(\n",
        "                    X, y, pipeline,\n",
        "                    f\"{vec_name} + {clf_name}\"\n",
        "                )\n",
        "\n",
        "                # Store results\n",
        "                results.append({\n",
        "                    'vectorizer': vec_name,\n",
        "                    'classifier': clf_name,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'f1_score': metrics['f1_score'],\n",
        "                    'cv_f1_mean': metrics['cv_f1_mean'],\n",
        "                    'cv_f1_std': metrics['cv_f1_std']\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error: {str(e)}\")\n",
        "                results.append({\n",
        "                    'vectorizer': vec_name,\n",
        "                    'classifier': clf_name,\n",
        "                    'accuracy': 0,\n",
        "                    'f1_score': 0,\n",
        "                    'cv_f1_mean': 0,\n",
        "                    'cv_f1_std': 0,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values('f1_score', ascending=False)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# QUICK PRESETS\n",
        "# ============================================================\n",
        "\n",
        "def create_pipeline_char_tfidf():\n",
        "    \"\"\"Pipeline 1: Character-Level TF-IDF (REKOMENDASI UTAMA)\"\"\"\n",
        "    return create_custom_pipeline('tfidf_char', 'logistic_regression')\n",
        "\n",
        "\n",
        "def create_pipeline_hybrid():\n",
        "    \"\"\"Pipeline 2: Hybrid (Word + Char)\"\"\"\n",
        "    return create_custom_pipeline('hybrid_word_char', 'logistic_regression')\n",
        "\n",
        "\n",
        "def create_pipeline_advanced():\n",
        "    \"\"\"Pipeline 3: Advanced (All Features)\"\"\"\n",
        "    return create_custom_pipeline('hybrid_all_features', 'random_forest')\n",
        "\n"
      ],
      "metadata": {
        "id": "mh5Xbv24w6lS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_all['comment'].values\n",
        "y = df_all['label'].values\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Label distribution:\", np.bincount(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S9K2Khix4eq",
        "outputId": "689a2957-a29d-417c-b632-cd2d72234a3d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10506,)\n",
            "Label distribution: [9580  926]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# OPSI 1: Test Pipeline Quick Presets\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 1: QUICK PRESETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipeline1 = create_pipeline_char_tfidf()\n",
        "model1, metrics1 = train_and_evaluate(X, y, pipeline1, \"Character TF-IDF\")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 2: Test Single Custom Pipeline\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 2: CUSTOM PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Contoh: FastText + Random Forest\n",
        "custom_pipeline = create_custom_pipeline('fasttext_cbow', 'random_forest')\n",
        "model_custom, metrics_custom = train_and_evaluate(\n",
        "    X, y, custom_pipeline,\n",
        "    \"FastText CBOW + Random Forest\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 3: Compare Multiple Models\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 3: PERBANDINGAN MULTIPLE MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pilih vectorizer dan classifier yang ingin dibandingkan\n",
        "vectorizers_to_test = [\n",
        "    'tfidf_char',\n",
        "    'tfidf_word',\n",
        "    'hybrid_word_char',\n",
        "    'fasttext_cbow',\n",
        "    'word2vec_cbow'\n",
        "]\n",
        "\n",
        "classifiers_to_test = [\n",
        "    'logistic_regression',\n",
        "    'random_forest',\n",
        "    'xgboost',\n",
        "    'svm_linear',\n",
        "    'svm_rbf',\n",
        "    'naive_bayes',\n",
        "    'knn',\n",
        "    'lightgbm',\n",
        "    'gradient_boosting'\n",
        "]\n",
        "\n",
        "# Jalankan perbandingan\n",
        "comparison_results = compare_multiple_models(\n",
        "    X, y,\n",
        "    vectorizers_to_test,\n",
        "    classifiers_to_test\n",
        ")\n",
        "\n",
        "    # Tampilkan hasil\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.to_string(index=False))\n",
        "\n",
        "    # Top 5 models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 5 BEST MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.head().to_string(index=False))\n",
        "\n",
        "    # ============================================================\n",
        "    # OPSI 4: List Available Configurations\n",
        "    # ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AVAILABLE CONFIGURATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìä Available Vectorizers:\")\n",
        "for i, vec in enumerate(get_vectorizers().keys(), 1):\n",
        "    print(f\"  {i}. {vec}\")\n",
        "\n",
        "print(\"\\nü§ñ Available Classifiers:\")\n",
        "for i, clf in enumerate(get_classifiers().keys(), 1):\n",
        "    print(f\"  {i}. {clf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPo_VrncyTM2",
        "outputId": "b92d0f6d-e105-4f3b-d7a2-3f359fe2c5e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "OPSI 1: QUICK PRESETS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: Character TF-IDF\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "============================================================\n",
            "OPSI 2: CUSTOM PIPELINE\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: FastText CBOW + Random Forest\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.96      0.99      0.97      2105\n",
            "        Judi       0.85      0.62      0.71       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.90      0.80      0.84      2335\n",
            "weighted avg       0.95      0.95      0.95      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2079   26]\n",
            " [  88  142]]\n",
            "\n",
            "Accuracy: 0.9512\n",
            "F1-Score: 0.7136\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.6368 (+/- 0.1749)\n",
            "\n",
            "============================================================\n",
            "OPSI 3: PERBANDINGAN MULTIPLE MODELS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "MEMULAI PERBANDINGAN 20 KOMBINASI MODEL\n",
            "============================================================\n",
            "\n",
            "\n",
            "[1/20] Testing: tfidf_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "[2/20] Testing: tfidf_char + random_forest\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + random_forest\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      1.00      0.99      2105\n",
            "        Judi       0.97      0.84      0.90       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.98      0.92      0.94      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2099    6]\n",
            " [  37  193]]\n",
            "\n",
            "Accuracy: 0.9816\n",
            "F1-Score: 0.8998\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8055 (+/- 0.1961)\n",
            "\n",
            "[3/20] Testing: tfidf_char + xgboost\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + xgboost\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.89      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.95      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  25  205]]\n",
            "\n",
            "Accuracy: 0.9884\n",
            "F1-Score: 0.9382\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8178 (+/- 0.2613)\n",
            "\n",
            "[4/20] Testing: tfidf_char + svm_linear\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + svm_linear\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       1.00      0.99      0.99      2105\n",
            "        Judi       0.94      0.96      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.98      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2092   13]\n",
            " [  10  220]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "F1-Score: 0.9503\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8832 (+/- 0.1428)\n",
            "\n",
            "[5/20] Testing: tfidf_word + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_word + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.87      0.88      0.87       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.93      0.93      0.93      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2075   30]\n",
            " [  28  202]]\n",
            "\n",
            "Accuracy: 0.9752\n",
            "F1-Score: 0.8745\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8480 (+/- 0.0613)\n",
            "\n",
            "[6/20] Testing: tfidf_word + random_forest\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_word + random_forest\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      0.97      0.97      2105\n",
            "        Judi       0.77      0.77      0.77       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.87      0.87      0.87      2335\n",
            "weighted avg       0.95      0.95      0.95      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2051   54]\n",
            " [  52  178]]\n",
            "\n",
            "Accuracy: 0.9546\n",
            "F1-Score: 0.7706\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.7429 (+/- 0.0443)\n",
            "\n",
            "[7/20] Testing: tfidf_word + xgboost\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_word + xgboost\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.96      1.00      0.98      2105\n",
            "        Judi       0.96      0.58      0.72       230\n",
            "\n",
            "    accuracy                           0.96      2335\n",
            "   macro avg       0.96      0.79      0.85      2335\n",
            "weighted avg       0.96      0.96      0.95      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2100    5]\n",
            " [  97  133]]\n",
            "\n",
            "Accuracy: 0.9563\n",
            "F1-Score: 0.7228\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.5952 (+/- 0.1531)\n",
            "\n",
            "[8/20] Testing: tfidf_word + svm_linear\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_word + svm_linear\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.98      0.98      2105\n",
            "        Judi       0.85      0.87      0.86       230\n",
            "\n",
            "    accuracy                           0.97      2335\n",
            "   macro avg       0.92      0.93      0.92      2335\n",
            "weighted avg       0.97      0.97      0.97      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2070   35]\n",
            " [  29  201]]\n",
            "\n",
            "Accuracy: 0.9726\n",
            "F1-Score: 0.8627\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8352 (+/- 0.0624)\n",
            "\n",
            "[9/20] Testing: hybrid_word_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.96      0.93      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.97      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2097    8]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "F1-Score: 0.9492\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8913 (+/- 0.0983)\n",
            "\n",
            "[10/20] Testing: hybrid_word_char + random_forest\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + random_forest\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      1.00      0.99      2105\n",
            "        Judi       0.95      0.82      0.88       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.97      0.91      0.93      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2096    9]\n",
            " [  42  188]]\n",
            "\n",
            "Accuracy: 0.9782\n",
            "F1-Score: 0.8806\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.7806 (+/- 0.1929)\n",
            "\n",
            "[11/20] Testing: hybrid_word_char + xgboost\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + xgboost\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.87      0.92       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.93      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  31  199]]\n",
            "\n",
            "Accuracy: 0.9859\n",
            "F1-Score: 0.9234\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8025 (+/- 0.2380)\n",
            "\n",
            "[12/20] Testing: hybrid_word_char + svm_linear\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + svm_linear\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      1.00      2105\n",
            "        Judi       0.97      0.94      0.96       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.97      0.98      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2099    6]\n",
            " [  14  216]]\n",
            "\n",
            "Accuracy: 0.9914\n",
            "F1-Score: 0.9558\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8724 (+/- 0.1530)\n",
            "\n",
            "[13/20] Testing: fasttext_cbow + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: fasttext_cbow + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.97      0.87      0.92      2105\n",
            "        Judi       0.39      0.76      0.52       230\n",
            "\n",
            "    accuracy                           0.86      2335\n",
            "   macro avg       0.68      0.82      0.72      2335\n",
            "weighted avg       0.91      0.86      0.88      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1834  271]\n",
            " [  55  175]]\n",
            "\n",
            "Accuracy: 0.8604\n",
            "F1-Score: 0.5178\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.5340 (+/- 0.0506)\n",
            "\n",
            "[14/20] Testing: fasttext_cbow + random_forest\n",
            "\n",
            "============================================================\n",
            "EVALUASI: fasttext_cbow + random_forest\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.96      0.99      0.97      2105\n",
            "        Judi       0.85      0.60      0.70       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.90      0.79      0.84      2335\n",
            "weighted avg       0.95      0.95      0.95      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2080   25]\n",
            " [  93  137]]\n",
            "\n",
            "Accuracy: 0.9495\n",
            "F1-Score: 0.6990\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.6435 (+/- 0.1678)\n",
            "\n",
            "[15/20] Testing: fasttext_cbow + xgboost\n",
            "\n",
            "============================================================\n",
            "EVALUASI: fasttext_cbow + xgboost\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.95      0.99      0.97      2105\n",
            "        Judi       0.91      0.57      0.70       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.93      0.78      0.84      2335\n",
            "weighted avg       0.95      0.95      0.95      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2092   13]\n",
            " [  99  131]]\n",
            "\n",
            "Accuracy: 0.9520\n",
            "F1-Score: 0.7005\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.6334 (+/- 0.1870)\n",
            "\n",
            "[16/20] Testing: fasttext_cbow + svm_linear\n",
            "\n",
            "============================================================\n",
            "EVALUASI: fasttext_cbow + svm_linear\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.97      0.94      0.95      2105\n",
            "        Judi       0.56      0.72      0.63       230\n",
            "\n",
            "    accuracy                           0.92      2335\n",
            "   macro avg       0.76      0.83      0.79      2335\n",
            "weighted avg       0.93      0.92      0.92      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1974  131]\n",
            " [  65  165]]\n",
            "\n",
            "Accuracy: 0.9161\n",
            "F1-Score: 0.6274\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.5795 (+/- 0.0822)\n",
            "\n",
            "[17/20] Testing: word2vec_cbow + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: word2vec_cbow + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.97      0.93      0.95      2105\n",
            "        Judi       0.53      0.77      0.63       230\n",
            "\n",
            "    accuracy                           0.91      2335\n",
            "   macro avg       0.75      0.85      0.79      2335\n",
            "weighted avg       0.93      0.91      0.92      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1948  157]\n",
            " [  54  176]]\n",
            "\n",
            "Accuracy: 0.9096\n",
            "F1-Score: 0.6252\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.6253 (+/- 0.0367)\n",
            "\n",
            "[18/20] Testing: word2vec_cbow + random_forest\n",
            "\n",
            "============================================================\n",
            "EVALUASI: word2vec_cbow + random_forest\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.96      0.98      0.97      2105\n",
            "        Judi       0.81      0.63      0.71       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.89      0.81      0.84      2335\n",
            "weighted avg       0.95      0.95      0.95      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2072   33]\n",
            " [  86  144]]\n",
            "\n",
            "Accuracy: 0.9490\n",
            "F1-Score: 0.7076\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.6501 (+/- 0.1778)\n",
            "\n",
            "[19/20] Testing: word2vec_cbow + xgboost\n",
            "\n",
            "============================================================\n",
            "EVALUASI: word2vec_cbow + xgboost\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.95      0.99      0.97      2105\n",
            "        Judi       0.82      0.57      0.68       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.89      0.78      0.82      2335\n",
            "weighted avg       0.94      0.95      0.94      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2076   29]\n",
            " [  98  132]]\n",
            "\n",
            "Accuracy: 0.9456\n",
            "F1-Score: 0.6752\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.6528 (+/- 0.1562)\n",
            "\n",
            "[20/20] Testing: word2vec_cbow + svm_linear\n",
            "\n",
            "============================================================\n",
            "EVALUASI: word2vec_cbow + svm_linear\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.97      0.91      0.94      2105\n",
            "        Judi       0.49      0.74      0.59       230\n",
            "\n",
            "    accuracy                           0.90      2335\n",
            "   macro avg       0.73      0.83      0.76      2335\n",
            "weighted avg       0.92      0.90      0.91      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1924  181]\n",
            " [  59  171]]\n",
            "\n",
            "Accuracy: 0.8972\n",
            "F1-Score: 0.5876\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.5886 (+/- 0.0460)\n",
            "\n",
            "================================================================================\n",
            "HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\n",
            "================================================================================\n",
            "      vectorizer          classifier  accuracy  f1_score  cv_f1_mean  cv_f1_std\n",
            "hybrid_word_char          svm_linear  0.991435  0.955752    0.872398   0.153005\n",
            "      tfidf_char          svm_linear  0.990150  0.950324    0.883182   0.142825\n",
            "hybrid_word_char logistic_regression  0.990150  0.949227    0.891296   0.098345\n",
            "      tfidf_char             xgboost  0.988437  0.938215    0.817803   0.261348\n",
            "      tfidf_char logistic_regression  0.987580  0.936819    0.888768   0.111537\n",
            "hybrid_word_char             xgboost  0.985867  0.923434    0.802463   0.238047\n",
            "      tfidf_char       random_forest  0.981585  0.899767    0.805510   0.196060\n",
            "hybrid_word_char       random_forest  0.978158  0.880562    0.780639   0.192860\n",
            "      tfidf_word logistic_regression  0.975161  0.874459    0.848039   0.061318\n",
            "      tfidf_word          svm_linear  0.972591  0.862661    0.835186   0.062358\n",
            "      tfidf_word       random_forest  0.954604  0.770563    0.742913   0.044322\n",
            "      tfidf_word             xgboost  0.956317  0.722826    0.595198   0.153126\n",
            "   word2vec_cbow       random_forest  0.949036  0.707617    0.650134   0.177797\n",
            "   fasttext_cbow             xgboost  0.952034  0.700535    0.633379   0.186970\n",
            "   fasttext_cbow       random_forest  0.949465  0.698980    0.643473   0.167809\n",
            "   word2vec_cbow             xgboost  0.945610  0.675192    0.652844   0.156151\n",
            "   fasttext_cbow          svm_linear  0.916060  0.627376    0.579484   0.082221\n",
            "   word2vec_cbow logistic_regression  0.909636  0.625222    0.625328   0.036665\n",
            "   word2vec_cbow          svm_linear  0.897216  0.587629    0.588555   0.045986\n",
            "   fasttext_cbow logistic_regression  0.860385  0.517751    0.533990   0.050608\n",
            "\n",
            "================================================================================\n",
            "TOP 5 BEST MODELS\n",
            "================================================================================\n",
            "      vectorizer          classifier  accuracy  f1_score  cv_f1_mean  cv_f1_std\n",
            "hybrid_word_char          svm_linear  0.991435  0.955752    0.872398   0.153005\n",
            "      tfidf_char          svm_linear  0.990150  0.950324    0.883182   0.142825\n",
            "hybrid_word_char logistic_regression  0.990150  0.949227    0.891296   0.098345\n",
            "      tfidf_char             xgboost  0.988437  0.938215    0.817803   0.261348\n",
            "      tfidf_char logistic_regression  0.987580  0.936819    0.888768   0.111537\n",
            "\n",
            "============================================================\n",
            "AVAILABLE CONFIGURATIONS\n",
            "============================================================\n",
            "\n",
            "üìä Available Vectorizers:\n",
            "  1. tfidf_char\n",
            "  2. tfidf_word\n",
            "  3. tfidf_char_wb\n",
            "  4. count_char\n",
            "  5. count_word\n",
            "  6. hashing_char\n",
            "  7. word2vec_cbow\n",
            "  8. word2vec_skipgram\n",
            "  9. fasttext_cbow\n",
            "  10. fasttext_skipgram\n",
            "  11. hybrid_word_char\n",
            "  12. hybrid_all_features\n",
            "\n",
            "ü§ñ Available Classifiers:\n",
            "  1. logistic_regression\n",
            "  2. random_forest\n",
            "  3. decision_tree\n",
            "  4. gradient_boosting\n",
            "  5. xgboost\n",
            "  6. lightgbm\n",
            "  7. svm_linear\n",
            "  8. svm_rbf\n",
            "  9. naive_bayes\n",
            "  10. knn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import re\n",
        "import unicodedata\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================\n",
        "# PREPROCESSING & FEATURE ENGINEERING\n",
        "# ============================================================\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Preprocessing untuk menangani homoglyph dan variasi Unicode\"\"\"\n",
        "    def normalize_homoglyph(self, text):\n",
        "        \"\"\"Konversi homoglyph Unicode ke karakter normal\"\"\"\n",
        "        for homo, normal in HOMOGLYPH_MAP.items():\n",
        "            text = text.replace(homo, normal)\n",
        "        return text\n",
        "\n",
        "    def normalize_unicode(self, text):\n",
        "        \"\"\"Normalisasi Unicode menggunakan NFKD\"\"\"\n",
        "        return unicodedata.normalize('NFKD', text)\n",
        "\n",
        "    def remove_extra_spaces(self, text):\n",
        "        \"\"\"Hapus spasi berlebih\"\"\"\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Pipeline preprocessing lengkap\"\"\"\n",
        "        text = str(text).lower()\n",
        "        text = self.normalize_homoglyph(text)\n",
        "        text = self.normalize_unicode(text)\n",
        "        text = self.remove_extra_spaces(text)\n",
        "        return text\n",
        "\n",
        "\n",
        "class AdditionalFeatures:\n",
        "    \"\"\"Ekstraksi fitur tambahan untuk deteksi spam\"\"\"\n",
        "\n",
        "    def count_emoji(self, text):\n",
        "        \"\"\"Hitung jumlah emoji\"\"\"\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "        return len(emoji_pattern.findall(text))\n",
        "\n",
        "    def capital_ratio(self, text):\n",
        "        \"\"\"Rasio huruf kapital\"\"\"\n",
        "        if len(text) == 0:\n",
        "            return 0\n",
        "        return sum(1 for c in text if c.isupper()) / len(text)\n",
        "\n",
        "    def has_numbers_in_word(self, text):\n",
        "        \"\"\"Deteksi angka dalam kata (SLOT88, PLUTO88)\"\"\"\n",
        "        pattern = r'[a-zA-Z]+\\d+|\\d+[a-zA-Z]+'\n",
        "        return len(re.findall(pattern, text))\n",
        "\n",
        "    def excessive_spacing(self, text):\n",
        "        \"\"\"Deteksi spasi berlebih antar karakter\"\"\"\n",
        "        pattern = r'(\\w\\s){3,}'\n",
        "        return len(re.findall(pattern, text))\n",
        "\n",
        "    def extract_features(self, texts):\n",
        "        \"\"\"Ekstraksi semua fitur\"\"\"\n",
        "        features = []\n",
        "        for text in texts:\n",
        "            features.append([\n",
        "                self.count_emoji(text),\n",
        "                self.capital_ratio(text),\n",
        "                self.has_numbers_in_word(text),\n",
        "                self.excessive_spacing(text)\n",
        "            ])\n",
        "        return np.array(features)\n",
        "\n",
        "\n",
        "class AdditionalFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk fitur tambahan\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_extractor = AdditionalFeatures()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.feature_extractor.extract_features(X)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# WORD2VEC & FASTTEXT TRANSFORMERS\n",
        "# ============================================================\n",
        "\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk Word2Vec embedding\"\"\"\n",
        "\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, sg=0):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.sg = sg  # 0=CBOW, 1=Skip-gram\n",
        "        self.model = None\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Train Word2Vec model\"\"\"\n",
        "        # Tokenize sentences\n",
        "        sentences = [self.preprocessor.preprocess(text).split() for text in X]\n",
        "\n",
        "        # Train Word2Vec\n",
        "        self.model = Word2Vec(\n",
        "            sentences=sentences,\n",
        "            vector_size=self.vector_size,\n",
        "            window=self.window,\n",
        "            min_count=self.min_count,\n",
        "            workers=self.workers,\n",
        "            sg=self.sg\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform texts to averaged word vectors\"\"\"\n",
        "        vectors = []\n",
        "        for text in X:\n",
        "            text = self.preprocessor.preprocess(text)\n",
        "            words = text.split()\n",
        "\n",
        "            # Get vectors for words in vocabulary\n",
        "            word_vectors = [\n",
        "                self.model.wv[word] for word in words\n",
        "                if word in self.model.wv\n",
        "            ]\n",
        "\n",
        "            # Average word vectors\n",
        "            if word_vectors:\n",
        "                vectors.append(np.mean(word_vectors, axis=0))\n",
        "            else:\n",
        "                vectors.append(np.zeros(self.vector_size))\n",
        "\n",
        "        return np.array(vectors)\n",
        "\n",
        "\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk FastText embedding\"\"\"\n",
        "\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, sg=0):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.sg = sg  # 0=CBOW, 1=Skip-gram\n",
        "        self.model = None\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Train FastText model\"\"\"\n",
        "        sentences = [self.preprocessor.preprocess(text).split() for text in X]\n",
        "\n",
        "        self.model = FastText(\n",
        "            sentences=sentences,\n",
        "            vector_size=self.vector_size,\n",
        "            window=self.window,\n",
        "            min_count=self.min_count,\n",
        "            workers=self.workers,\n",
        "            sg=self.sg\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform texts to averaged word vectors\"\"\"\n",
        "        vectors = []\n",
        "        for text in X:\n",
        "            text = self.preprocessor.preprocess(text)\n",
        "            words = text.split()\n",
        "\n",
        "            # FastText can handle OOV words\n",
        "            word_vectors = [self.model.wv[word] for word in words if words]\n",
        "\n",
        "            if word_vectors:\n",
        "                vectors.append(np.mean(word_vectors, axis=0))\n",
        "            else:\n",
        "                vectors.append(np.zeros(self.vector_size))\n",
        "\n",
        "        return np.array(vectors)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MODEL CONFIGURATIONS\n",
        "# ============================================================\n",
        "\n",
        "def get_classifiers():\n",
        "    \"\"\"\n",
        "    Konfigurasi semua classifier yang tersedia\n",
        "    Returns dict: {nama_model: instance_model}\n",
        "    \"\"\"\n",
        "    classifiers = {\n",
        "        # Linear Models\n",
        "        'logistic_regression': LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            solver='lbfgs'\n",
        "        ),\n",
        "\n",
        "        # Tree-based Models\n",
        "        'random_forest': RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=20,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "\n",
        "        'decision_tree': DecisionTreeClassifier(\n",
        "            max_depth=20,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        'gradient_boosting': GradientBoostingClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        'xgboost': XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss'\n",
        "        ),\n",
        "\n",
        "        'lightgbm': LGBMClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42,\n",
        "            verbose=-1\n",
        "        ),\n",
        "\n",
        "        # SVM\n",
        "        'svm_linear': SVC(\n",
        "            kernel='linear',\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            probability=True\n",
        "        ),\n",
        "\n",
        "        'svm_rbf': SVC(\n",
        "            kernel='rbf',\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            probability=True\n",
        "        ),\n",
        "\n",
        "        # Naive Bayes\n",
        "        'naive_bayes': MultinomialNB(alpha=1.0),\n",
        "\n",
        "        # KNN\n",
        "        'knn': KNeighborsClassifier(\n",
        "            n_neighbors=5,\n",
        "            weights='distance',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    }\n",
        "\n",
        "    return classifiers\n",
        "\n",
        "\n",
        "def get_vectorizers():\n",
        "    \"\"\"\n",
        "    Konfigurasi semua vectorizer yang tersedia\n",
        "    Returns dict: {nama_vectorizer: config}\n",
        "    \"\"\"\n",
        "    preprocessor = TextPreprocessor()\n",
        "\n",
        "    vectorizers = {\n",
        "        # TF-IDF variants\n",
        "        'tfidf_char': TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'tfidf_word': TfidfVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'tfidf_char_wb': TfidfVectorizer(\n",
        "            analyzer='char_wb',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Count Vectorizer\n",
        "        'count_char': CountVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'count_word': CountVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Hashing Vectorizer (memory efficient)\n",
        "        'hashing_char': HashingVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            n_features=2**16,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Word2Vec\n",
        "        'word2vec_cbow': Word2VecTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=0  # CBOW\n",
        "        ),\n",
        "\n",
        "        'word2vec_skipgram': Word2VecTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=1  # Skip-gram\n",
        "        ),\n",
        "\n",
        "        # FastText\n",
        "        'fasttext_cbow': FastTextTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=0  # CBOW\n",
        "        ),\n",
        "\n",
        "        'fasttext_skipgram': FastTextTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=1  # Skip-gram\n",
        "        ),\n",
        "\n",
        "        # Hybrid combinations\n",
        "        'hybrid_word_char': FeatureUnion([\n",
        "            ('word_tfidf', TfidfVectorizer(\n",
        "                analyzer='word',\n",
        "                ngram_range=(1, 2),\n",
        "                max_features=3000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('char_tfidf', TfidfVectorizer(\n",
        "                analyzer='char',\n",
        "                ngram_range=(2, 5),\n",
        "                max_features=3000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            ))\n",
        "        ]),\n",
        "\n",
        "        'hybrid_all_features': FeatureUnion([\n",
        "            ('word_tfidf', TfidfVectorizer(\n",
        "                analyzer='word',\n",
        "                ngram_range=(1, 2),\n",
        "                max_features=2000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('char_tfidf', TfidfVectorizer(\n",
        "                analyzer='char',\n",
        "                ngram_range=(2, 5),\n",
        "                max_features=2000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('additional', AdditionalFeaturesTransformer())\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    return vectorizers\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PIPELINE BUILDER\n",
        "# ============================================================\n",
        "\n",
        "def create_custom_pipeline(vectorizer_name, classifier_name):\n",
        "    \"\"\"\n",
        "    Buat pipeline custom dengan kombinasi vectorizer dan classifier\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    vectorizer_name : str\n",
        "        Nama vectorizer dari get_vectorizers()\n",
        "    classifier_name : str\n",
        "        Nama classifier dari get_classifiers()\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Pipeline object\n",
        "    \"\"\"\n",
        "    vectorizers = get_vectorizers()\n",
        "    classifiers = get_classifiers()\n",
        "\n",
        "    if vectorizer_name not in vectorizers:\n",
        "        raise ValueError(f\"Vectorizer '{vectorizer_name}' tidak tersedia. \"\n",
        "                        f\"Pilihan: {list(vectorizers.keys())}\")\n",
        "\n",
        "    if classifier_name not in classifiers:\n",
        "        raise ValueError(f\"Classifier '{classifier_name}' tidak tersedia. \"\n",
        "                        f\"Pilihan: {list(classifiers.keys())}\")\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('vectorizer', vectorizers[vectorizer_name]),\n",
        "        ('classifier', classifiers[classifier_name])\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TIMING UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def measure_single_prediction_time(pipeline, sample_text, num_iterations=100):\n",
        "    \"\"\"\n",
        "    Ukur waktu rata-rata untuk memprediksi satu teks\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    pipeline : fitted Pipeline\n",
        "        Model yang sudah ditraining\n",
        "    sample_text : str\n",
        "        Contoh teks untuk prediksi\n",
        "    num_iterations : int\n",
        "        Jumlah iterasi untuk mengambil rata-rata\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : {\n",
        "        'avg_time_ms': waktu rata-rata dalam milidetik,\n",
        "        'avg_time_sec': waktu rata-rata dalam detik,\n",
        "        'total_time_sec': total waktu untuk semua iterasi\n",
        "    }\n",
        "    \"\"\"\n",
        "    times = []\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        start_time = time.time()\n",
        "        pipeline.predict([sample_text])\n",
        "        end_time = time.time()\n",
        "        times.append(end_time - start_time)\n",
        "\n",
        "    avg_time = np.mean(times)\n",
        "\n",
        "    return {\n",
        "        'avg_time_ms': avg_time * 1000,\n",
        "        'avg_time_sec': avg_time,\n",
        "        'total_time_sec': sum(times),\n",
        "        'min_time_ms': min(times) * 1000,\n",
        "        'max_time_ms': max(times) * 1000\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING & EVALUATION WITH TIMING\n",
        "# ============================================================\n",
        "\n",
        "def train_and_evaluate(X, y, pipeline, pipeline_name):\n",
        "    \"\"\"Training dan evaluasi model dengan pengukuran waktu\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUASI: {pipeline_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Training with timing\n",
        "    print(\"Training model...\")\n",
        "    train_start = time.time()\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    train_time = time.time() - train_start\n",
        "    print(f\"‚è±Ô∏è  Training Time: {train_time:.4f} seconds\")\n",
        "\n",
        "    # Prediction with timing\n",
        "    print(\"\\nTesting prediction speed...\")\n",
        "    pred_start = time.time()\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    pred_time = time.time() - pred_start\n",
        "    avg_pred_time_per_sample = (pred_time / len(X_test)) * 1000  # in milliseconds\n",
        "    print(f\"‚è±Ô∏è  Batch Prediction Time: {pred_time:.4f} seconds\")\n",
        "    print(f\"‚è±Ô∏è  Average Time per Sample: {avg_pred_time_per_sample:.4f} ms\")\n",
        "\n",
        "    # Single prediction timing (more accurate for real-time use)\n",
        "    print(\"\\nMeasuring single prediction latency (100 iterations)...\")\n",
        "    if len(X_test) > 0:\n",
        "        single_pred_timing = measure_single_prediction_time(\n",
        "            pipeline, X_test.iloc[0] if hasattr(X_test, 'iloc') else X_test[0],\n",
        "            num_iterations=100\n",
        "        )\n",
        "        print(f\"‚è±Ô∏è  Single Prediction Time (avg): {single_pred_timing['avg_time_ms']:.4f} ms\")\n",
        "        print(f\"‚è±Ô∏è  Single Prediction Time (min): {single_pred_timing['min_time_ms']:.4f} ms\")\n",
        "        print(f\"‚è±Ô∏è  Single Prediction Time (max): {single_pred_timing['max_time_ms']:.4f} ms\")\n",
        "    else:\n",
        "        single_pred_timing = {'avg_time_ms': 0}\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred,\n",
        "                                target_names=['Non-Judi', 'Judi']))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Cross-validation with timing\n",
        "    print(\"\\nPerforming cross-validation...\")\n",
        "    cv_start = time.time()\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')\n",
        "    cv_time = time.time() - cv_start\n",
        "    print(f\"‚è±Ô∏è  Cross-Validation Time: {cv_time:.4f} seconds ({cv_time/60:.2f} minutes)\")\n",
        "    print(f\"Cross-Validation F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "    return pipeline, {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'cv_f1_mean': cv_scores.mean(),\n",
        "        'cv_f1_std': cv_scores.std(),\n",
        "        'train_time_sec': train_time,\n",
        "        'batch_pred_time_sec': pred_time,\n",
        "        'avg_pred_time_ms': avg_pred_time_per_sample,\n",
        "        'single_pred_time_ms': single_pred_timing['avg_time_ms'],\n",
        "        'single_pred_min_ms': single_pred_timing.get('min_time_ms', 0),\n",
        "        'single_pred_max_ms': single_pred_timing.get('max_time_ms', 0),\n",
        "        'cv_time_sec': cv_time,\n",
        "        'cv_time_min': cv_time / 60\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_multiple_models(X, y, vectorizer_configs, classifier_configs):\n",
        "    \"\"\"\n",
        "    Bandingkan multiple kombinasi vectorizer dan classifier dengan timing\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like\n",
        "        Text data\n",
        "    y : array-like\n",
        "        Labels\n",
        "    vectorizer_configs : list of str\n",
        "        List nama vectorizer yang ingin dicoba\n",
        "    classifier_configs : list of str\n",
        "        List nama classifier yang ingin dicoba\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame dengan hasil perbandingan termasuk timing\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    total_experiments = len(vectorizer_configs) * len(classifier_configs)\n",
        "    experiment_num = 0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MEMULAI PERBANDINGAN {total_experiments} KOMBINASI MODEL\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for vec_name in vectorizer_configs:\n",
        "        for clf_name in classifier_configs:\n",
        "            experiment_num += 1\n",
        "            print(f\"\\n[{experiment_num}/{total_experiments}] Testing: {vec_name} + {clf_name}\")\n",
        "\n",
        "            try:\n",
        "                # Create pipeline\n",
        "                pipeline = create_custom_pipeline(vec_name, clf_name)\n",
        "\n",
        "                # Train and evaluate\n",
        "                model, metrics = train_and_evaluate(\n",
        "                    X, y, pipeline,\n",
        "                    f\"{vec_name} + {clf_name}\"\n",
        "                )\n",
        "\n",
        "                # Store results\n",
        "                results.append({\n",
        "                    'vectorizer': vec_name,\n",
        "                    'classifier': clf_name,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'precision': metrics['precision'],\n",
        "                    'recall': metrics['recall'],\n",
        "                    'f1_score': metrics['f1_score'],\n",
        "                    'cv_f1_mean': metrics['cv_f1_mean'],\n",
        "                    'cv_f1_std': metrics['cv_f1_std'],\n",
        "                    'train_time_sec': metrics['train_time_sec'],\n",
        "                    'single_pred_ms': metrics['single_pred_time_ms'],\n",
        "                    'single_pred_min_ms': metrics['single_pred_min_ms'],\n",
        "                    'single_pred_max_ms': metrics['single_pred_max_ms'],\n",
        "                    'cv_time_min': metrics['cv_time_min']\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error: {str(e)}\")\n",
        "                results.append({\n",
        "                    'vectorizer': vec_name,\n",
        "                    'classifier': clf_name,\n",
        "                    'accuracy': 0,\n",
        "                    'precision': 0,\n",
        "                    'recall': 0,\n",
        "                    'f1_score': 0,\n",
        "                    'cv_f1_mean': 0,\n",
        "                    'cv_f1_std': 0,\n",
        "                    'train_time_sec': 0,\n",
        "                    'single_pred_ms': 0,\n",
        "                    'single_pred_min_ms': 0,\n",
        "                    'single_pred_max_ms': 0,\n",
        "                    'cv_time_min': 0,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values('f1_score', ascending=False)\n",
        "\n",
        "    # Display summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"RANKING BERDASARKAN F1-SCORE\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(results_df[['vectorizer', 'classifier', 'precision', 'recall', 'f1_score', 'single_pred_ms']].head(10))\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"RANKING BERDASARKAN KECEPATAN PREDIKSI (FASTEST)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(results_df.sort_values('single_pred_ms')[['vectorizer', 'classifier', 'precision', 'recall', 'f1_score', 'single_pred_ms']].head(10))\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# QUICK PRESETS\n",
        "# ============================================================\n",
        "\n",
        "def create_pipeline_char_tfidf():\n",
        "    \"\"\"Pipeline 1: Character-Level TF-IDF (REKOMENDASI UTAMA)\"\"\"\n",
        "    return create_custom_pipeline('tfidf_char', 'logistic_regression')\n",
        "\n",
        "\n",
        "def create_pipeline_hybrid():\n",
        "    \"\"\"Pipeline 2: Hybrid (Word + Char)\"\"\"\n",
        "    return create_custom_pipeline('hybrid_word_char', 'logistic_regression')\n",
        "\n",
        "\n",
        "def create_pipeline_advanced():\n",
        "    \"\"\"Pipeline 3: Advanced (All Features)\"\"\"\n",
        "    return create_custom_pipeline('hybrid_all_features', 'random_forest')\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# VISUALIZATION HELPER\n",
        "# ============================================================\n",
        "\n",
        "def create_comparison_table(results_df):\n",
        "    \"\"\"\n",
        "    Buat tabel perbandingan yang lebih mudah dibaca\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    results_df : DataFrame\n",
        "        Hasil dari compare_multiple_models()\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame dengan format yang lebih readable\n",
        "    \"\"\"\n",
        "    comparison_df = results_df.copy()\n",
        "\n",
        "    # Format kolom\n",
        "    comparison_df['model_name'] = comparison_df['vectorizer'] + ' + ' + comparison_df['classifier']\n",
        "    comparison_df['f1_score_pct'] = (comparison_df['f1_score'] * 100).round(2)\n",
        "    comparison_df['accuracy_pct'] = (comparison_df['accuracy'] * 100).round(2)\n",
        "    comparison_df['precision_pct'] = (comparison_df['precision'] * 100).round(2)\n",
        "    comparison_df['recall_pct'] = (comparison_df['recall'] * 100).round(2)\n",
        "    comparison_df['cv_f1_pct'] = (comparison_df['cv_f1_mean'] * 100).round(2)\n",
        "    comparison_df['pred_speed'] = comparison_df['single_pred_ms'].round(4)\n",
        "    comparison_df['cv_time'] = comparison_df['cv_time_min'].round(2)\n",
        "\n",
        "    # Select dan rename kolom\n",
        "    final_df = comparison_df[[\n",
        "        'model_name',\n",
        "        'accuracy_pct',\n",
        "        'precision_pct',\n",
        "        'recall_pct',\n",
        "        'f1_score_pct',\n",
        "        'cv_f1_pct',\n",
        "        'pred_speed',\n",
        "        'train_time_sec',\n",
        "        'cv_time'\n",
        "    ]].copy()\n",
        "\n",
        "    final_df.columns = [\n",
        "        'Model',\n",
        "        'Accuracy (%)',\n",
        "        'Precision (%)',\n",
        "        'Recall (%)',\n",
        "        'F1-Score (%)',\n",
        "        'CV F1-Score (%)',\n",
        "        'Pred Time (ms)',\n",
        "        'Train Time (s)',\n",
        "        'CV Time (min)'\n",
        "    ]\n",
        "\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "SlErfS1lTwjp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# OPSI 1: Test Pipeline Quick Presets\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 1: QUICK PRESETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipeline1 = create_pipeline_char_tfidf()\n",
        "model1, metrics1 = train_and_evaluate(X, y, pipeline1, \"Character TF-IDF\")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 2: Test Single Custom Pipeline\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 2: CUSTOM PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Contoh: FastText + Random Forest\n",
        "custom_pipeline = create_custom_pipeline('fasttext_cbow', 'random_forest')\n",
        "model_custom, metrics_custom = train_and_evaluate(\n",
        "    X, y, custom_pipeline,\n",
        "    \"FastText CBOW + Random Forest\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 3: Compare Multiple Models\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 3: PERBANDINGAN MULTIPLE MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pilih vectorizer dan classifier yang ingin dibandingkan\n",
        "vectorizers_to_test = [\n",
        "    'tfidf_char',\n",
        "    'hybrid_word_char'\n",
        "]\n",
        "\n",
        "classifiers_to_test = [\n",
        "    'logistic_regression',\n",
        "    'random_forest',\n",
        "    'xgboost',\n",
        "    'svm_linear',\n",
        "    'svm_rbf',\n",
        "    'naive_bayes',\n",
        "    'knn',\n",
        "    'lightgbm',\n",
        "    'gradient_boosting'\n",
        "]\n",
        "\n",
        "# Jalankan perbandingan\n",
        "comparison_results = compare_multiple_models(\n",
        "    X, y,\n",
        "    vectorizers_to_test,\n",
        "    classifiers_to_test\n",
        ")\n",
        "\n",
        "    # Tampilkan hasil\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.to_string(index=False))\n",
        "\n",
        "    # Top 5 models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 5 BEST MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.head().to_string(index=False))\n",
        "\n",
        "    # ============================================================\n",
        "    # OPSI 4: List Available Configurations\n",
        "    # ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AVAILABLE CONFIGURATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìä Available Vectorizers:\")\n",
        "for i, vec in enumerate(get_vectorizers().keys(), 1):\n",
        "    print(f\"  {i}. {vec}\")\n",
        "\n",
        "print(\"\\nü§ñ Available Classifiers:\")\n",
        "for i, clf in enumerate(get_classifiers().keys(), 1):\n",
        "    print(f\"  {i}. {clf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQCEWmzhScev",
        "outputId": "0a2e5e76-fb98-4dd9-c870-401bba79ea13"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "OPSI 1: QUICK PRESETS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: Character TF-IDF\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 3.2607 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.6717 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2877 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 1.4892 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 1.2794 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 3.4258 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "Precision: 0.9389\n",
            "Recall: 0.9348\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 17.2031 seconds (0.29 minutes)\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "============================================================\n",
            "OPSI 2: CUSTOM PIPELINE\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: FastText CBOW + Random Forest\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 16.1586 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.3077 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.1318 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 33.9015 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 23.9086 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 36.7796 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.96      0.98      0.97      2105\n",
            "        Judi       0.79      0.60      0.68       230\n",
            "\n",
            "    accuracy                           0.94      2335\n",
            "   macro avg       0.87      0.79      0.82      2335\n",
            "weighted avg       0.94      0.94      0.94      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2069   36]\n",
            " [  93  137]]\n",
            "\n",
            "Accuracy: 0.9448\n",
            "Precision: 0.7919\n",
            "Recall: 0.5957\n",
            "F1-Score: 0.6799\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 82.1778 seconds (1.37 minutes)\n",
            "Cross-Validation F1-Score: 0.6432 (+/- 0.1564)\n",
            "\n",
            "============================================================\n",
            "OPSI 3: PERBANDINGAN MULTIPLE MODELS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "MEMULAI PERBANDINGAN 18 KOMBINASI MODEL\n",
            "============================================================\n",
            "\n",
            "\n",
            "[1/18] Testing: tfidf_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 3.8896 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.3806 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.1630 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 0.9771 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 0.8657 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 2.0859 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "Precision: 0.9389\n",
            "Recall: 0.9348\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 16.8520 seconds (0.28 minutes)\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "[2/18] Testing: tfidf_char + random_forest\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + random_forest\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 6.1702 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.8099 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.3468 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 34.4557 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 24.8430 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 51.8684 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      1.00      0.99      2105\n",
            "        Judi       0.97      0.84      0.90       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.98      0.92      0.94      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2099    6]\n",
            " [  37  193]]\n",
            "\n",
            "Accuracy: 0.9816\n",
            "Precision: 0.9698\n",
            "Recall: 0.8391\n",
            "F1-Score: 0.8998\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 34.6217 seconds (0.58 minutes)\n",
            "Cross-Validation F1-Score: 0.8055 (+/- 0.1961)\n",
            "\n",
            "[3/18] Testing: tfidf_char + xgboost\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + xgboost\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 31.9616 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.4127 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.1767 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 2.1019 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 1.4861 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 7.1149 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.89      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.95      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  25  205]]\n",
            "\n",
            "Accuracy: 0.9884\n",
            "Precision: 0.9903\n",
            "Recall: 0.8913\n",
            "F1-Score: 0.9382\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 144.3294 seconds (2.41 minutes)\n",
            "Cross-Validation F1-Score: 0.8178 (+/- 0.2613)\n",
            "\n",
            "[4/18] Testing: tfidf_char + svm_linear\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + svm_linear\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 68.7856 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 2.6458 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 1.1331 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 2.8346 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 2.4970 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 4.7123 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       1.00      0.99      0.99      2105\n",
            "        Judi       0.94      0.96      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.98      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2092   13]\n",
            " [  10  220]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "Precision: 0.9442\n",
            "Recall: 0.9565\n",
            "F1-Score: 0.9503\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 355.1844 seconds (5.92 minutes)\n",
            "Cross-Validation F1-Score: 0.8832 (+/- 0.1428)\n",
            "\n",
            "[5/18] Testing: tfidf_char + svm_rbf\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + svm_rbf\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 130.2934 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 4.8350 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 2.0707 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 4.9756 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 4.5521 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 5.9624 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.90      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.95      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  22  208]]\n",
            "\n",
            "Accuracy: 0.9897\n",
            "Precision: 0.9905\n",
            "Recall: 0.9043\n",
            "F1-Score: 0.9455\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 674.3768 seconds (11.24 minutes)\n",
            "Cross-Validation F1-Score: 0.8443 (+/- 0.2100)\n",
            "\n",
            "[6/18] Testing: tfidf_char + naive_bayes\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + naive_bayes\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 2.6327 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.3909 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.1674 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 0.9403 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 0.8836 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 1.7738 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      1.00      0.99      2105\n",
            "        Judi       0.97      0.79      0.87       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.98      0.89      0.93      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2100    5]\n",
            " [  49  181]]\n",
            "\n",
            "Accuracy: 0.9769\n",
            "Precision: 0.9731\n",
            "Recall: 0.7870\n",
            "F1-Score: 0.8702\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 16.1022 seconds (0.27 minutes)\n",
            "Cross-Validation F1-Score: 0.7916 (+/- 0.1420)\n",
            "\n",
            "[7/18] Testing: tfidf_char + knn\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + knn\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 2.5721 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 2.9751 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 1.2742 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 39.3729 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 33.7527 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 89.7937 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.95      1.00      0.97      2105\n",
            "        Judi       0.99      0.50      0.66       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.97      0.75      0.82      2335\n",
            "weighted avg       0.95      0.95      0.94      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2104    1]\n",
            " [ 115  115]]\n",
            "\n",
            "Accuracy: 0.9503\n",
            "Precision: 0.9914\n",
            "Recall: 0.5000\n",
            "F1-Score: 0.6647\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 26.9645 seconds (0.45 minutes)\n",
            "Cross-Validation F1-Score: 0.5566 (+/- 0.2031)\n",
            "\n",
            "[8/18] Testing: tfidf_char + lightgbm\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + lightgbm\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 10.0404 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.4165 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.1784 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 13.0914 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 8.0714 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 235.3642 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.91      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.95      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  21  209]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "Precision: 0.9905\n",
            "Recall: 0.9087\n",
            "F1-Score: 0.9478\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 51.8752 seconds (0.86 minutes)\n",
            "Cross-Validation F1-Score: 0.8209 (+/- 0.2623)\n",
            "\n",
            "[9/18] Testing: tfidf_char + gradient_boosting\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + gradient_boosting\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 109.5585 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.4145 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.1775 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 1.3106 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 1.1995 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 2.5303 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.98      0.89      0.93       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.94      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2101    4]\n",
            " [  26  204]]\n",
            "\n",
            "Accuracy: 0.9872\n",
            "Precision: 0.9808\n",
            "Recall: 0.8870\n",
            "F1-Score: 0.9315\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 552.7661 seconds (9.21 minutes)\n",
            "Cross-Validation F1-Score: 0.8167 (+/- 0.2579)\n",
            "\n",
            "[10/18] Testing: hybrid_word_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 4.3088 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.4792 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2052 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 2.1641 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 1.9088 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 3.4857 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.96      0.93      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.97      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2097    8]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "Precision: 0.9641\n",
            "Recall: 0.9348\n",
            "F1-Score: 0.9492\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 20.9685 seconds (0.35 minutes)\n",
            "Cross-Validation F1-Score: 0.8913 (+/- 0.0983)\n",
            "\n",
            "[11/18] Testing: hybrid_word_char + random_forest\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + random_forest\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 7.2855 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.5548 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2376 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 37.2894 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 36.0146 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 39.0472 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      1.00      0.99      2105\n",
            "        Judi       0.95      0.82      0.88       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.97      0.91      0.93      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2096    9]\n",
            " [  42  188]]\n",
            "\n",
            "Accuracy: 0.9782\n",
            "Precision: 0.9543\n",
            "Recall: 0.8174\n",
            "F1-Score: 0.8806\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 39.3130 seconds (0.66 minutes)\n",
            "Cross-Validation F1-Score: 0.7806 (+/- 0.1929)\n",
            "\n",
            "[12/18] Testing: hybrid_word_char + xgboost\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + xgboost\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 29.9234 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.7275 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.3116 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 3.2501 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 2.6290 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 10.8104 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.87      0.92       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.93      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  31  199]]\n",
            "\n",
            "Accuracy: 0.9859\n",
            "Precision: 0.9900\n",
            "Recall: 0.8652\n",
            "F1-Score: 0.9234\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 143.6058 seconds (2.39 minutes)\n",
            "Cross-Validation F1-Score: 0.8025 (+/- 0.2380)\n",
            "\n",
            "[13/18] Testing: hybrid_word_char + svm_linear\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + svm_linear\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 62.8879 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 3.1671 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 1.3563 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 3.8867 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 3.5362 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 5.8331 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      1.00      2105\n",
            "        Judi       0.97      0.94      0.96       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.97      0.98      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2099    6]\n",
            " [  14  216]]\n",
            "\n",
            "Accuracy: 0.9914\n",
            "Precision: 0.9730\n",
            "Recall: 0.9391\n",
            "F1-Score: 0.9558\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 324.7219 seconds (5.41 minutes)\n",
            "Cross-Validation F1-Score: 0.8724 (+/- 0.1530)\n",
            "\n",
            "[14/18] Testing: hybrid_word_char + svm_rbf\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + svm_rbf\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 150.2684 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 6.1679 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 2.6415 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 10.1778 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 6.7422 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 15.0630 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      1.00      2105\n",
            "        Judi       1.00      0.92      0.96       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       1.00      0.96      0.98      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2105    0]\n",
            " [  19  211]]\n",
            "\n",
            "Accuracy: 0.9919\n",
            "Precision: 1.0000\n",
            "Recall: 0.9174\n",
            "F1-Score: 0.9569\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 770.6903 seconds (12.84 minutes)\n",
            "Cross-Validation F1-Score: 0.8389 (+/- 0.2004)\n",
            "\n",
            "[15/18] Testing: hybrid_word_char + naive_bayes\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + naive_bayes\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 3.3583 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.4690 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2008 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 2.3275 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 1.9646 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 3.6981 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.97      0.88      0.92       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.94      0.96      2335\n",
            "weighted avg       0.98      0.99      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2098    7]\n",
            " [  28  202]]\n",
            "\n",
            "Accuracy: 0.9850\n",
            "Precision: 0.9665\n",
            "Recall: 0.8783\n",
            "F1-Score: 0.9203\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 21.3439 seconds (0.36 minutes)\n",
            "Cross-Validation F1-Score: 0.8610 (+/- 0.0916)\n",
            "\n",
            "[16/18] Testing: hybrid_word_char + knn\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + knn\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 3.2929 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 2.8646 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 1.2268 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 43.0265 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 34.0073 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 75.2990 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.94      1.00      0.97      2105\n",
            "        Judi       0.99      0.41      0.58       230\n",
            "\n",
            "    accuracy                           0.94      2335\n",
            "   macro avg       0.96      0.71      0.78      2335\n",
            "weighted avg       0.94      0.94      0.93      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2104    1]\n",
            " [ 135   95]]\n",
            "\n",
            "Accuracy: 0.9418\n",
            "Precision: 0.9896\n",
            "Recall: 0.4130\n",
            "F1-Score: 0.5828\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 31.2692 seconds (0.52 minutes)\n",
            "Cross-Validation F1-Score: 0.4707 (+/- 0.1393)\n",
            "\n",
            "[17/18] Testing: hybrid_word_char + lightgbm\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + lightgbm\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 9.5430 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.5238 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2243 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 23.1652 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 11.0023 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 994.4463 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      1.00      0.99      2105\n",
            "        Judi       0.98      0.84      0.91       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.98      0.92      0.95      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2102    3]\n",
            " [  36  194]]\n",
            "\n",
            "Accuracy: 0.9833\n",
            "Precision: 0.9848\n",
            "Recall: 0.8435\n",
            "F1-Score: 0.9087\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 50.2899 seconds (0.84 minutes)\n",
            "Cross-Validation F1-Score: 0.7927 (+/- 0.2259)\n",
            "\n",
            "[18/18] Testing: hybrid_word_char + gradient_boosting\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + gradient_boosting\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 102.7772 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.4868 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2085 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 2.5861 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 2.1935 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 4.2508 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.95      0.92      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2095   10]\n",
            " [  19  211]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "Precision: 0.9548\n",
            "Recall: 0.9174\n",
            "F1-Score: 0.9357\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 515.3421 seconds (8.59 minutes)\n",
            "Cross-Validation F1-Score: 0.8058 (+/- 0.2532)\n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN F1-SCORE\n",
            "================================================================================\n",
            "          vectorizer           classifier  precision    recall  f1_score  \\\n",
            "13  hybrid_word_char              svm_rbf   1.000000  0.917391  0.956916   \n",
            "12  hybrid_word_char           svm_linear   0.972973  0.939130  0.955752   \n",
            "3         tfidf_char           svm_linear   0.944206  0.956522  0.950324   \n",
            "9   hybrid_word_char  logistic_regression   0.964126  0.934783  0.949227   \n",
            "7         tfidf_char             lightgbm   0.990521  0.908696  0.947846   \n",
            "4         tfidf_char              svm_rbf   0.990476  0.904348  0.945455   \n",
            "2         tfidf_char              xgboost   0.990338  0.891304  0.938215   \n",
            "0         tfidf_char  logistic_regression   0.938865  0.934783  0.936819   \n",
            "17  hybrid_word_char    gradient_boosting   0.954751  0.917391  0.935698   \n",
            "8         tfidf_char    gradient_boosting   0.980769  0.886957  0.931507   \n",
            "\n",
            "    single_pred_ms  \n",
            "13       10.177784  \n",
            "12        3.886681  \n",
            "3         2.834647  \n",
            "9         2.164071  \n",
            "7        13.091395  \n",
            "4         4.975564  \n",
            "2         2.101884  \n",
            "0         0.977063  \n",
            "17        2.586095  \n",
            "8         1.310639  \n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN KECEPATAN PREDIKSI (FASTEST)\n",
            "================================================================================\n",
            "          vectorizer           classifier  precision    recall  f1_score  \\\n",
            "5         tfidf_char          naive_bayes   0.973118  0.786957  0.870192   \n",
            "0         tfidf_char  logistic_regression   0.938865  0.934783  0.936819   \n",
            "8         tfidf_char    gradient_boosting   0.980769  0.886957  0.931507   \n",
            "2         tfidf_char              xgboost   0.990338  0.891304  0.938215   \n",
            "9   hybrid_word_char  logistic_regression   0.964126  0.934783  0.949227   \n",
            "14  hybrid_word_char          naive_bayes   0.966507  0.878261  0.920273   \n",
            "17  hybrid_word_char    gradient_boosting   0.954751  0.917391  0.935698   \n",
            "3         tfidf_char           svm_linear   0.944206  0.956522  0.950324   \n",
            "11  hybrid_word_char              xgboost   0.990050  0.865217  0.923434   \n",
            "12  hybrid_word_char           svm_linear   0.972973  0.939130  0.955752   \n",
            "\n",
            "    single_pred_ms  \n",
            "5         0.940270  \n",
            "0         0.977063  \n",
            "8         1.310639  \n",
            "2         2.101884  \n",
            "9         2.164071  \n",
            "14        2.327461  \n",
            "17        2.586095  \n",
            "3         2.834647  \n",
            "11        3.250072  \n",
            "12        3.886681  \n",
            "\n",
            "================================================================================\n",
            "HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\n",
            "================================================================================\n",
            "      vectorizer          classifier  accuracy  precision   recall  f1_score  cv_f1_mean  cv_f1_std  train_time_sec  single_pred_ms  single_pred_min_ms  single_pred_max_ms  cv_time_min\n",
            "hybrid_word_char             svm_rbf  0.991863   1.000000 0.917391  0.956916    0.838919   0.200354      150.268370       10.177784            6.742239           15.063047    12.844838\n",
            "hybrid_word_char          svm_linear  0.991435   0.972973 0.939130  0.955752    0.872398   0.153005       62.887941        3.886681            3.536224            5.833149     5.412032\n",
            "      tfidf_char          svm_linear  0.990150   0.944206 0.956522  0.950324    0.883182   0.142825       68.785555        2.834647            2.496958            4.712343     5.919740\n",
            "hybrid_word_char logistic_regression  0.990150   0.964126 0.934783  0.949227    0.891296   0.098345        4.308781        2.164071            1.908779            3.485680     0.349475\n",
            "      tfidf_char            lightgbm  0.990150   0.990521 0.908696  0.947846    0.820867   0.262328       10.040376       13.091395            8.071423          235.364199     0.864587\n",
            "      tfidf_char             svm_rbf  0.989722   0.990476 0.904348  0.945455    0.844258   0.209963      130.293381        4.975564            4.552126            5.962372    11.239613\n",
            "      tfidf_char             xgboost  0.988437   0.990338 0.891304  0.938215    0.817803   0.261348       31.961616        2.101884            1.486063            7.114887     2.405491\n",
            "      tfidf_char logistic_regression  0.987580   0.938865 0.934783  0.936819    0.888768   0.111537        3.889606        0.977063            0.865698            2.085924     0.280866\n",
            "hybrid_word_char   gradient_boosting  0.987580   0.954751 0.917391  0.935698    0.805760   0.253164      102.777230        2.586095            2.193451            4.250765     8.589034\n",
            "      tfidf_char   gradient_boosting  0.987152   0.980769 0.886957  0.931507    0.816654   0.257931      109.558513        1.310639            1.199484            2.530336     9.212768\n",
            "hybrid_word_char             xgboost  0.985867   0.990050 0.865217  0.923434    0.802463   0.238047       29.923387        3.250072            2.629042           10.810375     2.393430\n",
            "hybrid_word_char         naive_bayes  0.985011   0.966507 0.878261  0.920273    0.860996   0.091627        3.358254        2.327461            1.964569            3.698111     0.355732\n",
            "hybrid_word_char            lightgbm  0.983298   0.984772 0.843478  0.908665    0.792685   0.225918        9.543047       23.165247           11.002302          994.446278     0.838165\n",
            "      tfidf_char       random_forest  0.981585   0.969849 0.839130  0.899767    0.805510   0.196060        6.170224       34.455714           24.842978           51.868439     0.577028\n",
            "hybrid_word_char       random_forest  0.978158   0.954315 0.817391  0.880562    0.780639   0.192860        7.285459       37.289393           36.014557           39.047241     0.655217\n",
            "      tfidf_char         naive_bayes  0.976874   0.973118 0.786957  0.870192    0.791611   0.142049        2.632657        0.940270            0.883579            1.773834     0.268369\n",
            "      tfidf_char                 knn  0.950321   0.991379 0.500000  0.664740    0.556557   0.203099        2.572119       39.372914           33.752680           89.793682     0.449408\n",
            "hybrid_word_char                 knn  0.941756   0.989583 0.413043  0.582822    0.470688   0.139283        3.292913       43.026533           34.007311           75.299025     0.521154\n",
            "\n",
            "================================================================================\n",
            "TOP 5 BEST MODELS\n",
            "================================================================================\n",
            "      vectorizer          classifier  accuracy  precision   recall  f1_score  cv_f1_mean  cv_f1_std  train_time_sec  single_pred_ms  single_pred_min_ms  single_pred_max_ms  cv_time_min\n",
            "hybrid_word_char             svm_rbf  0.991863   1.000000 0.917391  0.956916    0.838919   0.200354      150.268370       10.177784            6.742239           15.063047    12.844838\n",
            "hybrid_word_char          svm_linear  0.991435   0.972973 0.939130  0.955752    0.872398   0.153005       62.887941        3.886681            3.536224            5.833149     5.412032\n",
            "      tfidf_char          svm_linear  0.990150   0.944206 0.956522  0.950324    0.883182   0.142825       68.785555        2.834647            2.496958            4.712343     5.919740\n",
            "hybrid_word_char logistic_regression  0.990150   0.964126 0.934783  0.949227    0.891296   0.098345        4.308781        2.164071            1.908779            3.485680     0.349475\n",
            "      tfidf_char            lightgbm  0.990150   0.990521 0.908696  0.947846    0.820867   0.262328       10.040376       13.091395            8.071423          235.364199     0.864587\n",
            "\n",
            "============================================================\n",
            "AVAILABLE CONFIGURATIONS\n",
            "============================================================\n",
            "\n",
            "üìä Available Vectorizers:\n",
            "  1. tfidf_char\n",
            "  2. tfidf_word\n",
            "  3. tfidf_char_wb\n",
            "  4. count_char\n",
            "  5. count_word\n",
            "  6. hashing_char\n",
            "  7. word2vec_cbow\n",
            "  8. word2vec_skipgram\n",
            "  9. fasttext_cbow\n",
            "  10. fasttext_skipgram\n",
            "  11. hybrid_word_char\n",
            "  12. hybrid_all_features\n",
            "\n",
            "ü§ñ Available Classifiers:\n",
            "  1. logistic_regression\n",
            "  2. random_forest\n",
            "  3. decision_tree\n",
            "  4. gradient_boosting\n",
            "  5. xgboost\n",
            "  6. lightgbm\n",
            "  7. svm_linear\n",
            "  8. svm_rbf\n",
            "  9. naive_bayes\n",
            "  10. knn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# OPSI 1: Test Pipeline Quick Presets\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 1: QUICK PRESETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipeline1 = create_pipeline_char_tfidf()\n",
        "model1, metrics1 = train_and_evaluate(X, y, pipeline1, \"Character TF-IDF\")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 2: Test Single Custom Pipeline\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 2: CUSTOM PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Contoh: FastText + Random Forest\n",
        "custom_pipeline = create_custom_pipeline('fasttext_cbow', 'random_forest')\n",
        "model_custom, metrics_custom = train_and_evaluate(\n",
        "    X, y, custom_pipeline,\n",
        "    \"FastText CBOW + Random Forest\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 3: Compare Multiple Models\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 3: PERBANDINGAN MULTIPLE MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pilih vectorizer dan classifier yang ingin dibandingkan\n",
        "vectorizers_to_test = [\n",
        "    'tfidf_char',\n",
        "    'hybrid_word_char',\n",
        "    'hybrid_all_features'\n",
        "]\n",
        "\n",
        "classifiers_to_test = [\n",
        "    'logistic_regression',\n",
        "    'svm_rbf',\n",
        "]\n",
        "\n",
        "# Jalankan perbandingan\n",
        "comparison_results = compare_multiple_models(\n",
        "    X, y,\n",
        "    vectorizers_to_test,\n",
        "    classifiers_to_test\n",
        ")\n",
        "\n",
        "    # Tampilkan hasil\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.to_string(index=False))\n",
        "\n",
        "    # Top 5 models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 5 BEST MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.head().to_string(index=False))\n",
        "\n",
        "    # ============================================================\n",
        "    # OPSI 4: List Available Configurations\n",
        "    # ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AVAILABLE CONFIGURATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìä Available Vectorizers:\")\n",
        "for i, vec in enumerate(get_vectorizers().keys(), 1):\n",
        "    print(f\"  {i}. {vec}\")\n",
        "\n",
        "print(\"\\nü§ñ Available Classifiers:\")\n",
        "for i, clf in enumerate(get_classifiers().keys(), 1):\n",
        "    print(f\"  {i}. {clf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdDru7OSq6aD",
        "outputId": "f269df35-8570-4f02-8f50-19dab080d4af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "OPSI 1: QUICK PRESETS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: Character TF-IDF\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 2.8122 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.6696 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2867 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 1.3891 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 1.2758 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 4.1847 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "Precision: 0.9389\n",
            "Recall: 0.9348\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 17.4073 seconds (0.29 minutes)\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "============================================================\n",
            "OPSI 2: CUSTOM PIPELINE\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: FastText CBOW + Random Forest\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 16.4656 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.3198 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.1370 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 35.2478 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 24.3530 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 73.2725 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.96      0.99      0.97      2105\n",
            "        Judi       0.85      0.61      0.71       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.91      0.80      0.84      2335\n",
            "weighted avg       0.95      0.95      0.95      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2081   24]\n",
            " [  89  141]]\n",
            "\n",
            "Accuracy: 0.9516\n",
            "Precision: 0.8545\n",
            "Recall: 0.6130\n",
            "F1-Score: 0.7139\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 80.5652 seconds (1.34 minutes)\n",
            "Cross-Validation F1-Score: 0.6409 (+/- 0.1757)\n",
            "\n",
            "============================================================\n",
            "OPSI 3: PERBANDINGAN MULTIPLE MODELS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "MEMULAI PERBANDINGAN 6 KOMBINASI MODEL\n",
            "============================================================\n",
            "\n",
            "\n",
            "[1/6] Testing: tfidf_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 2.7830 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.6870 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2942 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 1.3291 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 1.2646 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 1.7855 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "Precision: 0.9389\n",
            "Recall: 0.9348\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 17.4141 seconds (0.29 minutes)\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "[2/6] Testing: tfidf_char + svm_rbf\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + svm_rbf\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 124.0332 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 5.5786 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 2.3891 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 4.6268 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 4.0050 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 5.4522 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.90      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.95      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  22  208]]\n",
            "\n",
            "Accuracy: 0.9897\n",
            "Precision: 0.9905\n",
            "Recall: 0.9043\n",
            "F1-Score: 0.9455\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 649.0175 seconds (10.82 minutes)\n",
            "Cross-Validation F1-Score: 0.8443 (+/- 0.2100)\n",
            "\n",
            "[3/6] Testing: hybrid_word_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 4.0288 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.4638 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.1986 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 2.1512 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 1.8535 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 3.1409 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.96      0.93      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.97      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2097    8]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "Precision: 0.9641\n",
            "Recall: 0.9348\n",
            "F1-Score: 0.9492\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 20.6851 seconds (0.34 minutes)\n",
            "Cross-Validation F1-Score: 0.8913 (+/- 0.0983)\n",
            "\n",
            "[4/6] Testing: hybrid_word_char + svm_rbf\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + svm_rbf\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 145.6773 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 5.5207 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 2.3643 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 6.6091 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 5.9130 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 9.4397 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      1.00      2105\n",
            "        Judi       1.00      0.92      0.96       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       1.00      0.96      0.98      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2105    0]\n",
            " [  19  211]]\n",
            "\n",
            "Accuracy: 0.9919\n",
            "Precision: 1.0000\n",
            "Recall: 0.9174\n",
            "F1-Score: 0.9569\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 754.4627 seconds (12.57 minutes)\n",
            "Cross-Validation F1-Score: 0.8389 (+/- 0.2004)\n",
            "\n",
            "[5/6] Testing: hybrid_all_features + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_all_features + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 3.6761 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.8632 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.3697 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 3.7075 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 3.2601 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 6.1803 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       1.00      1.00      1.00      2105\n",
            "        Judi       0.97      0.96      0.96       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.98      0.98      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2097    8]\n",
            " [   9  221]]\n",
            "\n",
            "Accuracy: 0.9927\n",
            "Precision: 0.9651\n",
            "Recall: 0.9609\n",
            "F1-Score: 0.9630\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 22.7832 seconds (0.38 minutes)\n",
            "Cross-Validation F1-Score: 0.9043 (+/- 0.1181)\n",
            "\n",
            "[6/6] Testing: hybrid_all_features + svm_rbf\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_all_features + svm_rbf\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 83.8516 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 3.2185 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 1.3784 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 5.2096 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 4.5962 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 7.8228 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       1.00      1.00      1.00      2105\n",
            "        Judi       0.96      0.96      0.96       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.98      0.98      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2096    9]\n",
            " [   9  221]]\n",
            "\n",
            "Accuracy: 0.9923\n",
            "Precision: 0.9609\n",
            "Recall: 0.9609\n",
            "F1-Score: 0.9609\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 414.4594 seconds (6.91 minutes)\n",
            "Cross-Validation F1-Score: 0.8967 (+/- 0.1260)\n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN F1-SCORE\n",
            "================================================================================\n",
            "            vectorizer           classifier  precision    recall  f1_score  \\\n",
            "4  hybrid_all_features  logistic_regression   0.965066  0.960870  0.962963   \n",
            "5  hybrid_all_features              svm_rbf   0.960870  0.960870  0.960870   \n",
            "3     hybrid_word_char              svm_rbf   1.000000  0.917391  0.956916   \n",
            "2     hybrid_word_char  logistic_regression   0.964126  0.934783  0.949227   \n",
            "1           tfidf_char              svm_rbf   0.990476  0.904348  0.945455   \n",
            "0           tfidf_char  logistic_regression   0.938865  0.934783  0.936819   \n",
            "\n",
            "   single_pred_ms  \n",
            "4        3.707533  \n",
            "5        5.209599  \n",
            "3        6.609132  \n",
            "2        2.151158  \n",
            "1        4.626832  \n",
            "0        1.329062  \n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN KECEPATAN PREDIKSI (FASTEST)\n",
            "================================================================================\n",
            "            vectorizer           classifier  precision    recall  f1_score  \\\n",
            "0           tfidf_char  logistic_regression   0.938865  0.934783  0.936819   \n",
            "2     hybrid_word_char  logistic_regression   0.964126  0.934783  0.949227   \n",
            "4  hybrid_all_features  logistic_regression   0.965066  0.960870  0.962963   \n",
            "1           tfidf_char              svm_rbf   0.990476  0.904348  0.945455   \n",
            "5  hybrid_all_features              svm_rbf   0.960870  0.960870  0.960870   \n",
            "3     hybrid_word_char              svm_rbf   1.000000  0.917391  0.956916   \n",
            "\n",
            "   single_pred_ms  \n",
            "0        1.329062  \n",
            "2        2.151158  \n",
            "4        3.707533  \n",
            "1        4.626832  \n",
            "5        5.209599  \n",
            "3        6.609132  \n",
            "\n",
            "================================================================================\n",
            "HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\n",
            "================================================================================\n",
            "         vectorizer          classifier  accuracy  precision   recall  f1_score  cv_f1_mean  cv_f1_std  train_time_sec  single_pred_ms  single_pred_min_ms  single_pred_max_ms  cv_time_min\n",
            "hybrid_all_features logistic_regression  0.992719   0.965066 0.960870  0.962963    0.904330   0.118138        3.676054        3.707533            3.260136            6.180286     0.379720\n",
            "hybrid_all_features             svm_rbf  0.992291   0.960870 0.960870  0.960870    0.896697   0.125979       83.851641        5.209599            4.596233            7.822752     6.907657\n",
            "   hybrid_word_char             svm_rbf  0.991863   1.000000 0.917391  0.956916    0.838919   0.200354      145.677299        6.609132            5.913019            9.439707    12.574378\n",
            "   hybrid_word_char logistic_regression  0.990150   0.964126 0.934783  0.949227    0.891296   0.098345        4.028850        2.151158            1.853466            3.140926     0.344752\n",
            "         tfidf_char             svm_rbf  0.989722   0.990476 0.904348  0.945455    0.844258   0.209963      124.033185        4.626832            4.004955            5.452156    10.816958\n",
            "         tfidf_char logistic_regression  0.987580   0.938865 0.934783  0.936819    0.888768   0.111537        2.782959        1.329062            1.264572            1.785517     0.290236\n",
            "\n",
            "================================================================================\n",
            "TOP 5 BEST MODELS\n",
            "================================================================================\n",
            "         vectorizer          classifier  accuracy  precision   recall  f1_score  cv_f1_mean  cv_f1_std  train_time_sec  single_pred_ms  single_pred_min_ms  single_pred_max_ms  cv_time_min\n",
            "hybrid_all_features logistic_regression  0.992719   0.965066 0.960870  0.962963    0.904330   0.118138        3.676054        3.707533            3.260136            6.180286     0.379720\n",
            "hybrid_all_features             svm_rbf  0.992291   0.960870 0.960870  0.960870    0.896697   0.125979       83.851641        5.209599            4.596233            7.822752     6.907657\n",
            "   hybrid_word_char             svm_rbf  0.991863   1.000000 0.917391  0.956916    0.838919   0.200354      145.677299        6.609132            5.913019            9.439707    12.574378\n",
            "   hybrid_word_char logistic_regression  0.990150   0.964126 0.934783  0.949227    0.891296   0.098345        4.028850        2.151158            1.853466            3.140926     0.344752\n",
            "         tfidf_char             svm_rbf  0.989722   0.990476 0.904348  0.945455    0.844258   0.209963      124.033185        4.626832            4.004955            5.452156    10.816958\n",
            "\n",
            "============================================================\n",
            "AVAILABLE CONFIGURATIONS\n",
            "============================================================\n",
            "\n",
            "üìä Available Vectorizers:\n",
            "  1. tfidf_char\n",
            "  2. tfidf_word\n",
            "  3. tfidf_char_wb\n",
            "  4. count_char\n",
            "  5. count_word\n",
            "  6. hashing_char\n",
            "  7. word2vec_cbow\n",
            "  8. word2vec_skipgram\n",
            "  9. fasttext_cbow\n",
            "  10. fasttext_skipgram\n",
            "  11. hybrid_word_char\n",
            "  12. hybrid_all_features\n",
            "\n",
            "ü§ñ Available Classifiers:\n",
            "  1. logistic_regression\n",
            "  2. random_forest\n",
            "  3. decision_tree\n",
            "  4. gradient_boosting\n",
            "  5. xgboost\n",
            "  6. lightgbm\n",
            "  7. svm_linear\n",
            "  8. svm_rbf\n",
            "  9. naive_bayes\n",
            "  10. knn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('https://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/train.csv')\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/test.csv')\n",
        "df_holdout = pd.read_csv('https://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/holdout.csv', delimiter=';')\n",
        "\n",
        "df_all = pd.concat([df_train, df_test], ignore_index=True)\n",
        "X = df_all['comment'].values\n",
        "y = df_all['label'].values\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Label distribution:\", np.bincount(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCk87C2sO9R4",
        "outputId": "3881ec03-853b-48f5-ff76-f2eabce702f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10506,)\n",
            "Label distribution: [9580  926]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ef452de",
        "outputId": "fb9bdbe0-e6ea-4af3-f0ae-8af4dbeea817"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grids for each pipeline\n",
        "param_grid_hybrid_all_lr = {\n",
        "    'vectorizer__word_tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'vectorizer__char_tfidf__ngram_range': [(2, 4), (2, 5)],\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "param_grid_hybrid_word_char_lr = {\n",
        "    'vectorizer__word_tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'vectorizer__char_tfidf__ngram_range': [(2, 4), (2, 5)],\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Create the pipelines\n",
        "pipeline_hybrid_all_lr = create_custom_pipeline('hybrid_all_features', 'logistic_regression')\n",
        "pipeline_hybrid_word_char_lr = create_custom_pipeline('hybrid_word_char', 'logistic_regression')\n",
        "\n",
        "# Perform GridSearchCV for hybrid_all_features + logistic_regression\n",
        "print(\"Performing GridSearchCV for hybrid_all_features + logistic_regression...\")\n",
        "grid_search_hybrid_all_lr = GridSearchCV(\n",
        "    pipeline_hybrid_all_lr,\n",
        "    param_grid_hybrid_all_lr,\n",
        "    cv=3,  # Using 3-fold cross-validation for faster tuning\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "grid_search_hybrid_all_lr.fit(X, y)\n",
        "\n",
        "print(\"\\nBest parameters for hybrid_all_features + logistic_regression:\")\n",
        "print(grid_search_hybrid_all_lr.best_params_)\n",
        "print(\"Best F1-Score:\", grid_search_hybrid_all_lr.best_score_)\n",
        "\n",
        "# Perform GridSearchCV for hybrid_word_char + logistic_regression\n",
        "print(\"\\nPerforming GridSearchCV for hybrid_word_char + logistic_regression...\")\n",
        "grid_search_hybrid_word_char_lr = GridSearchCV(\n",
        "    pipeline_hybrid_word_char_lr,\n",
        "    param_grid_hybrid_word_char_lr,\n",
        "    cv=3,  # Using 3-fold cross-validation for faster tuning\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "grid_search_hybrid_word_char_lr.fit(X, y)\n",
        "\n",
        "print(\"\\nBest parameters for hybrid_word_char + logistic_regression:\")\n",
        "print(grid_search_hybrid_word_char_lr.best_params_)\n",
        "print(\"Best F1-Score:\", grid_search_hybrid_word_char_lr.best_score_)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing GridSearchCV for hybrid_all_features + logistic_regression...\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "\n",
            "Best parameters for hybrid_all_features + logistic_regression:\n",
            "{'classifier__C': 10, 'classifier__solver': 'liblinear', 'vectorizer__char_tfidf__ngram_range': (2, 4), 'vectorizer__word_tfidf__ngram_range': (1, 1)}\n",
            "Best F1-Score: 0.9712223923669906\n",
            "\n",
            "Performing GridSearchCV for hybrid_word_char + logistic_regression...\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "\n",
            "Best parameters for hybrid_word_char + logistic_regression:\n",
            "{'classifier__C': 10, 'classifier__solver': 'liblinear', 'vectorizer__char_tfidf__ngram_range': (2, 4), 'vectorizer__word_tfidf__ngram_range': (1, 1)}\n",
            "Best F1-Score: 0.9615746948911467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCMDv_sINO39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_DnOzulDNV1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# OPSI 1: Test Pipeline Quick Presets\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 1: QUICK PRESETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipeline1 = create_pipeline_char_tfidf()\n",
        "model1, metrics1 = train_and_evaluate(X, y, pipeline1, \"Character TF-IDF\")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 2: Test Single Custom Pipeline\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 2: CUSTOM PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Contoh: FastText + Random Forest\n",
        "custom_pipeline = create_custom_pipeline('fasttext_cbow', 'random_forest')\n",
        "model_custom, metrics_custom = train_and_evaluate(\n",
        "    X, y, custom_pipeline,\n",
        "    \"FastText CBOW + Random Forest\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 3: Compare Multiple Models\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 3: PERBANDINGAN MULTIPLE MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pilih vectorizer dan classifier yang ingin dibandingkan\n",
        "vectorizers_to_test = [\n",
        "    'tfidf_char',\n",
        "    'hybrid_word_char',\n",
        "    'hybrid_all_features'\n",
        "]\n",
        "\n",
        "classifiers_to_test = [\n",
        "    'logistic_regression'\n",
        "]\n",
        "\n",
        "# Jalankan perbandingan\n",
        "comparison_results = compare_multiple_models(\n",
        "    X, y,\n",
        "    vectorizers_to_test,\n",
        "    classifiers_to_test\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 4: Add Tuned Models to Comparison\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 4: MENAMBAHKAN MODEL TUNED KE PERBANDINGAN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get the best estimators from GridSearchCV (assuming they were run in a previous cell)\n",
        "try:\n",
        "    tuned_hybrid_all_lr = grid_search_hybrid_all_lr.best_estimator_\n",
        "    tuned_hybrid_word_char_lr = grid_search_hybrid_word_char_lr.best_estimator_\n",
        "\n",
        "    # Evaluate tuned models\n",
        "    print(\"\\nEvaluating Tuned Hybrid All Features + Logistic Regression...\")\n",
        "    model_tuned_hybrid_all_lr, metrics_tuned_hybrid_all_lr = train_and_evaluate(\n",
        "        X, y, tuned_hybrid_all_lr,\n",
        "        \"TUNED Hybrid All Features + Logistic Regression\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nEvaluating Tuned Hybrid Word Char + Logistic Regression...\")\n",
        "    model_tuned_hybrid_word_char_lr, metrics_tuned_hybrid_word_char_lr = train_and_evaluate(\n",
        "        X, y, tuned_hybrid_word_char_lr,\n",
        "        \"TUNED Hybrid Word Char + Logistic Regression\"\n",
        "    )\n",
        "\n",
        "    # Add tuned model results to the comparison DataFrame\n",
        "    tuned_results = []\n",
        "    tuned_results.append({\n",
        "        'vectorizer': 'hybrid_all_features (TUNED)',\n",
        "        'classifier': 'logistic_regression (TUNED)',\n",
        "        'accuracy': metrics_tuned_hybrid_all_lr['accuracy'],\n",
        "        'precision': metrics_tuned_hybrid_all_lr['precision'],\n",
        "        'recall': metrics_tuned_hybrid_all_lr['recall'],\n",
        "        'f1_score': metrics_tuned_hybrid_all_lr['f1_score'],\n",
        "        'cv_f1_mean': metrics_tuned_hybrid_all_lr['cv_f1_mean'],\n",
        "        'cv_f1_std': metrics_tuned_hybrid_all_lr['cv_f1_std'],\n",
        "        'train_time_sec': metrics_tuned_hybrid_all_lr['train_time_sec'],\n",
        "        'single_pred_ms': metrics_tuned_hybrid_all_lr['single_pred_time_ms'],\n",
        "        'single_pred_min_ms': metrics_tuned_hybrid_all_lr['single_pred_min_ms'],\n",
        "        'single_pred_max_ms': metrics_tuned_hybrid_all_lr['single_pred_max_ms'],\n",
        "        'cv_time_min': metrics_tuned_hybrid_all_lr['cv_time_min']\n",
        "    })\n",
        "    tuned_results.append({\n",
        "        'vectorizer': 'hybrid_word_char (TUNED)',\n",
        "        'classifier': 'logistic_regression (TUNED)',\n",
        "        'accuracy': metrics_tuned_hybrid_word_char_lr['accuracy'],\n",
        "        'precision': metrics_tuned_hybrid_word_char_lr['precision'],\n",
        "        'recall': metrics_tuned_hybrid_word_char_lr['recall'],\n",
        "        'f1_score': metrics_tuned_hybrid_word_char_lr['f1_score'],\n",
        "        'cv_f1_mean': metrics_tuned_hybrid_word_char_lr['cv_f1_mean'],\n",
        "        'cv_f1_std': metrics_tuned_hybrid_word_char_lr['cv_f1_std'],\n",
        "        'train_time_sec': metrics_tuned_hybrid_word_char_lr['train_time_sec'],\n",
        "        'single_pred_ms': metrics_tuned_hybrid_word_char_lr['single_pred_time_ms'],\n",
        "        'single_pred_min_ms': metrics_tuned_hybrid_word_char_lr['single_pred_min_ms'],\n",
        "        'single_pred_max_ms': metrics_tuned_hybrid_word_char_lr['single_pred_max_ms'],\n",
        "        'cv_time_min': metrics_tuned_hybrid_word_char_lr['cv_time_min']\n",
        "    })\n",
        "\n",
        "    comparison_results = pd.concat([comparison_results, pd.DataFrame(tuned_results)], ignore_index=True)\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nSkipping tuned model comparison: GridSearchCV results not found. Please run the tuning cell first.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while adding tuned models: {str(e)}\")\n",
        "\n",
        "\n",
        "    # Tampilkan hasil\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.sort_values('f1_score', ascending=False).to_string(index=False))\n",
        "\n",
        "    # Top 5 models (including tuned if added)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 5 BEST MODELS (Sorted by F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.sort_values('f1_score', ascending=False).head().to_string(index=False))\n",
        "\n",
        "    # Display summary sorted by Cross-Validation F1-Score\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"RANKING BERDASARKAN CROSS-VALIDATION F1-SCORE\")\n",
        "print(f\"{'='*80}\")\n",
        "print(comparison_results.sort_values('cv_f1_mean', ascending=False)[['vectorizer', 'classifier', 'precision', 'recall', 'f1_score', 'cv_f1_mean', 'single_pred_ms']].head(10))\n",
        "\n",
        "    # Display summary sorted by prediction speed\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"RANKING BERDASARKAN KECEPATAN PREDIKSI (FASTEST)\")\n",
        "print(f\"{'='*80}\")\n",
        "print(comparison_results.sort_values('single_pred_ms')[['vectorizer', 'classifier', 'precision', 'recall', 'f1_score', 'single_pred_ms']].head(10))\n",
        "\n",
        "\n",
        "    # ============================================================\n",
        "    # OPSI 5: List Available Configurations (Moved to end for clarity)\n",
        "    # ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AVAILABLE CONFIGURATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìä Available Vectorizers:\")\n",
        "for i, vec in enumerate(get_vectorizers().keys(), 1):\n",
        "    print(f\"  {i}. {vec}\")\n",
        "\n",
        "print(\"\\nü§ñ Available Classifiers:\")\n",
        "for i, clf in enumerate(get_classifiers().keys(), 1):\n",
        "    print(f\"  {i}. {clf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRrkQEdEASBD",
        "outputId": "5334196f-6a4a-44d8-f834-9f8b4cd3abab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "OPSI 1: QUICK PRESETS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: Character TF-IDF\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 4.2647 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.4601 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2189 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 2.7903 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 2.0623 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 4.9741 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      1.00      1917\n",
            "        Judi       0.96      0.94      0.95       185\n",
            "\n",
            "    accuracy                           0.99      2102\n",
            "   macro avg       0.98      0.97      0.97      2102\n",
            "weighted avg       0.99      0.99      0.99      2102\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1909    8]\n",
            " [  11  174]]\n",
            "\n",
            "Accuracy: 0.9910\n",
            "Precision: 0.9560\n",
            "Recall: 0.9405\n",
            "F1-Score: 0.9482\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 20.2270 seconds (0.34 minutes)\n",
            "Cross-Validation F1-Score: 0.9448 (+/- 0.0104)\n",
            "\n",
            "============================================================\n",
            "OPSI 2: CUSTOM PIPELINE\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: FastText CBOW + Random Forest\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 19.6073 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.3287 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.1564 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 38.0794 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 35.7425 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 50.9551 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.96      1.00      0.98      1917\n",
            "        Judi       0.93      0.60      0.73       185\n",
            "\n",
            "    accuracy                           0.96      2102\n",
            "   macro avg       0.95      0.80      0.85      2102\n",
            "weighted avg       0.96      0.96      0.96      2102\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1909    8]\n",
            " [  74  111]]\n",
            "\n",
            "Accuracy: 0.9610\n",
            "Precision: 0.9328\n",
            "Recall: 0.6000\n",
            "F1-Score: 0.7303\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 88.2813 seconds (1.47 minutes)\n",
            "Cross-Validation F1-Score: 0.7073 (+/- 0.0214)\n",
            "\n",
            "============================================================\n",
            "OPSI 3: PERBANDINGAN MULTIPLE MODELS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "MEMULAI PERBANDINGAN 3 KOMBINASI MODEL\n",
            "============================================================\n",
            "\n",
            "\n",
            "[1/3] Testing: tfidf_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 3.7862 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.8116 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.3861 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 4.0269 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 2.6326 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 9.0737 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      1.00      1917\n",
            "        Judi       0.96      0.94      0.95       185\n",
            "\n",
            "    accuracy                           0.99      2102\n",
            "   macro avg       0.98      0.97      0.97      2102\n",
            "weighted avg       0.99      0.99      0.99      2102\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1909    8]\n",
            " [  11  174]]\n",
            "\n",
            "Accuracy: 0.9910\n",
            "Precision: 0.9560\n",
            "Recall: 0.9405\n",
            "F1-Score: 0.9482\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 20.3760 seconds (0.34 minutes)\n",
            "Cross-Validation F1-Score: 0.9448 (+/- 0.0104)\n",
            "\n",
            "[2/3] Testing: hybrid_word_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 4.9818 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.9670 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.4600 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 4.0094 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 3.3312 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 9.3727 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      1.00      1917\n",
            "        Judi       0.98      0.92      0.95       185\n",
            "\n",
            "    accuracy                           0.99      2102\n",
            "   macro avg       0.98      0.96      0.97      2102\n",
            "weighted avg       0.99      0.99      0.99      2102\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1913    4]\n",
            " [  15  170]]\n",
            "\n",
            "Accuracy: 0.9910\n",
            "Precision: 0.9770\n",
            "Recall: 0.9189\n",
            "F1-Score: 0.9471\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 26.6571 seconds (0.44 minutes)\n",
            "Cross-Validation F1-Score: 0.9448 (+/- 0.0112)\n",
            "\n",
            "[3/3] Testing: hybrid_all_features + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_all_features + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 4.5591 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.5459 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2597 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 4.6189 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 3.8178 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 6.3307 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       1.00      1.00      1.00      1917\n",
            "        Judi       0.96      0.96      0.96       185\n",
            "\n",
            "    accuracy                           0.99      2102\n",
            "   macro avg       0.98      0.98      0.98      2102\n",
            "weighted avg       0.99      0.99      0.99      2102\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1909    8]\n",
            " [   7  178]]\n",
            "\n",
            "Accuracy: 0.9929\n",
            "Precision: 0.9570\n",
            "Recall: 0.9622\n",
            "F1-Score: 0.9596\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 27.6919 seconds (0.46 minutes)\n",
            "Cross-Validation F1-Score: 0.9550 (+/- 0.0094)\n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN F1-SCORE\n",
            "================================================================================\n",
            "            vectorizer           classifier  precision    recall  f1_score  \\\n",
            "2  hybrid_all_features  logistic_regression   0.956989  0.962162  0.959569   \n",
            "0           tfidf_char  logistic_regression   0.956044  0.940541  0.948229   \n",
            "1     hybrid_word_char  logistic_regression   0.977011  0.918919  0.947075   \n",
            "\n",
            "   single_pred_ms  \n",
            "2        4.618917  \n",
            "0        4.026880  \n",
            "1        4.009423  \n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN KECEPATAN PREDIKSI (FASTEST)\n",
            "================================================================================\n",
            "            vectorizer           classifier  precision    recall  f1_score  \\\n",
            "1     hybrid_word_char  logistic_regression   0.977011  0.918919  0.947075   \n",
            "0           tfidf_char  logistic_regression   0.956044  0.940541  0.948229   \n",
            "2  hybrid_all_features  logistic_regression   0.956989  0.962162  0.959569   \n",
            "\n",
            "   single_pred_ms  \n",
            "1        4.009423  \n",
            "0        4.026880  \n",
            "2        4.618917  \n",
            "\n",
            "============================================================\n",
            "OPSI 4: MENAMBAHKAN MODEL TUNED KE PERBANDINGAN\n",
            "============================================================\n",
            "\n",
            "Evaluating Tuned Hybrid All Features + Logistic Regression...\n",
            "\n",
            "============================================================\n",
            "EVALUASI: TUNED Hybrid All Features + Logistic Regression\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 3.3606 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.6987 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.3324 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 3.6589 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 3.0596 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 5.5559 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       1.00      1.00      1.00      1917\n",
            "        Judi       0.99      0.95      0.97       185\n",
            "\n",
            "    accuracy                           1.00      2102\n",
            "   macro avg       0.99      0.98      0.98      2102\n",
            "weighted avg       1.00      1.00      1.00      2102\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1916    1]\n",
            " [   9  176]]\n",
            "\n",
            "Accuracy: 0.9952\n",
            "Precision: 0.9944\n",
            "Recall: 0.9514\n",
            "F1-Score: 0.9724\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 16.2200 seconds (0.27 minutes)\n",
            "Cross-Validation F1-Score: 0.9711 (+/- 0.0089)\n",
            "\n",
            "Evaluating Tuned Hybrid Word Char + Logistic Regression...\n",
            "\n",
            "============================================================\n",
            "EVALUASI: TUNED Hybrid Word Char + Logistic Regression\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 2.1494 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.3951 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.1880 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 2.8649 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 2.5010 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 4.3883 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      1.00      1917\n",
            "        Judi       0.99      0.94      0.96       185\n",
            "\n",
            "    accuracy                           0.99      2102\n",
            "   macro avg       0.99      0.97      0.98      2102\n",
            "weighted avg       0.99      0.99      0.99      2102\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1916    1]\n",
            " [  12  173]]\n",
            "\n",
            "Accuracy: 0.9938\n",
            "Precision: 0.9943\n",
            "Recall: 0.9351\n",
            "F1-Score: 0.9638\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 13.8898 seconds (0.23 minutes)\n",
            "Cross-Validation F1-Score: 0.9622 (+/- 0.0123)\n",
            "\n",
            "================================================================================\n",
            "HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\n",
            "================================================================================\n",
            "                 vectorizer                  classifier  accuracy  precision   recall  f1_score  cv_f1_mean  cv_f1_std  train_time_sec  single_pred_ms  single_pred_min_ms  single_pred_max_ms  cv_time_min\n",
            "hybrid_all_features (TUNED) logistic_regression (TUNED)  0.995243   0.994350 0.951351  0.972376    0.971140   0.008932        3.360562        3.658888            3.059626            5.555868     0.270334\n",
            "   hybrid_word_char (TUNED) logistic_regression (TUNED)  0.993815   0.994253 0.935135  0.963788    0.962163   0.012280        2.149384        2.864938            2.501011            4.388332     0.231497\n",
            "        hybrid_all_features         logistic_regression  0.992864   0.956989 0.962162  0.959569    0.955044   0.009381        4.559086        4.618917            3.817797            6.330729     0.461531\n",
            "                 tfidf_char         logistic_regression  0.990961   0.956044 0.940541  0.948229    0.944794   0.010361        3.786240        4.026880            2.632618            9.073734     0.339600\n",
            "           hybrid_word_char         logistic_regression  0.990961   0.977011 0.918919  0.947075    0.944830   0.011164        4.981821        4.009423            3.331184            9.372711     0.444285\n",
            "\n",
            "================================================================================\n",
            "TOP 5 BEST MODELS (Sorted by F1-Score)\n",
            "================================================================================\n",
            "                 vectorizer                  classifier  accuracy  precision   recall  f1_score  cv_f1_mean  cv_f1_std  train_time_sec  single_pred_ms  single_pred_min_ms  single_pred_max_ms  cv_time_min\n",
            "hybrid_all_features (TUNED) logistic_regression (TUNED)  0.995243   0.994350 0.951351  0.972376    0.971140   0.008932        3.360562        3.658888            3.059626            5.555868     0.270334\n",
            "   hybrid_word_char (TUNED) logistic_regression (TUNED)  0.993815   0.994253 0.935135  0.963788    0.962163   0.012280        2.149384        2.864938            2.501011            4.388332     0.231497\n",
            "        hybrid_all_features         logistic_regression  0.992864   0.956989 0.962162  0.959569    0.955044   0.009381        4.559086        4.618917            3.817797            6.330729     0.461531\n",
            "                 tfidf_char         logistic_regression  0.990961   0.956044 0.940541  0.948229    0.944794   0.010361        3.786240        4.026880            2.632618            9.073734     0.339600\n",
            "           hybrid_word_char         logistic_regression  0.990961   0.977011 0.918919  0.947075    0.944830   0.011164        4.981821        4.009423            3.331184            9.372711     0.444285\n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN CROSS-VALIDATION F1-SCORE\n",
            "================================================================================\n",
            "                    vectorizer                   classifier  precision  \\\n",
            "3  hybrid_all_features (TUNED)  logistic_regression (TUNED)   0.994350   \n",
            "4     hybrid_word_char (TUNED)  logistic_regression (TUNED)   0.994253   \n",
            "0          hybrid_all_features          logistic_regression   0.956989   \n",
            "2             hybrid_word_char          logistic_regression   0.977011   \n",
            "1                   tfidf_char          logistic_regression   0.956044   \n",
            "\n",
            "     recall  f1_score  cv_f1_mean  single_pred_ms  \n",
            "3  0.951351  0.972376    0.971140        3.658888  \n",
            "4  0.935135  0.963788    0.962163        2.864938  \n",
            "0  0.962162  0.959569    0.955044        4.618917  \n",
            "2  0.918919  0.947075    0.944830        4.009423  \n",
            "1  0.940541  0.948229    0.944794        4.026880  \n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN KECEPATAN PREDIKSI (FASTEST)\n",
            "================================================================================\n",
            "                    vectorizer                   classifier  precision  \\\n",
            "4     hybrid_word_char (TUNED)  logistic_regression (TUNED)   0.994253   \n",
            "3  hybrid_all_features (TUNED)  logistic_regression (TUNED)   0.994350   \n",
            "2             hybrid_word_char          logistic_regression   0.977011   \n",
            "1                   tfidf_char          logistic_regression   0.956044   \n",
            "0          hybrid_all_features          logistic_regression   0.956989   \n",
            "\n",
            "     recall  f1_score  single_pred_ms  \n",
            "4  0.935135  0.963788        2.864938  \n",
            "3  0.951351  0.972376        3.658888  \n",
            "2  0.918919  0.947075        4.009423  \n",
            "1  0.940541  0.948229        4.026880  \n",
            "0  0.962162  0.959569        4.618917  \n",
            "\n",
            "============================================================\n",
            "AVAILABLE CONFIGURATIONS\n",
            "============================================================\n",
            "\n",
            "üìä Available Vectorizers:\n",
            "  1. tfidf_char\n",
            "  2. tfidf_word\n",
            "  3. tfidf_char_wb\n",
            "  4. count_char\n",
            "  5. count_word\n",
            "  6. hashing_char\n",
            "  7. word2vec_cbow\n",
            "  8. word2vec_skipgram\n",
            "  9. fasttext_cbow\n",
            "  10. fasttext_skipgram\n",
            "  11. hybrid_word_char\n",
            "  12. hybrid_all_features\n",
            "\n",
            "ü§ñ Available Classifiers:\n",
            "  1. logistic_regression\n",
            "  2. random_forest\n",
            "  3. decision_tree\n",
            "  4. gradient_boosting\n",
            "  5. xgboost\n",
            "  6. lightgbm\n",
            "  7. svm_linear\n",
            "  8. svm_rbf\n",
            "  9. naive_bayes\n",
            "  10. knn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d4b4325"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = df_holdout['comment'].values\n",
        "y_val = df_holdout['label'].values"
      ],
      "metadata": {
        "id": "LqnsAJ9mS5LA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# OPSI 6: Evaluate Best Tuned Model on Holdout Data\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 6: EVALUASI MODEL TUNED PADA DATA HOLDOUT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    # Assuming grid_search_hybrid_all_lr and grid_search_hybrid_word_char_lr are available from previous tuning\n",
        "    tuned_hybrid_all_lr = grid_search_hybrid_all_lr.best_estimator_\n",
        "    tuned_hybrid_word_char_lr = grid_search_hybrid_word_char_lr.best_estimator_\n",
        "\n",
        "    print(\"\\nEvaluating TUNED Hybrid All Features + Logistic Regression on Holdout Data...\")\n",
        "    model_holdout_all, metrics_holdout_all = train_and_evaluate(\n",
        "        X_val, y_val, tuned_hybrid_all_lr,\n",
        "        \"TUNED Hybrid All Features + Logistic Regression (Holdout)\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nHasil Evaluasi TUNED Hybrid All Features + Logistic Regression pada Data Holdout:\")\n",
        "    print(f\"Accuracy: {metrics_holdout_all['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics_holdout_all['precision']:.4f}\")\n",
        "    print(f\"Recall: {metrics_holdout_all['recall']:.4f}\")\n",
        "    print(f\"F1-Score: {metrics_holdout_all['f1_score']:.4f}\")\n",
        "    print(f\"Single Prediction Time (avg): {metrics_holdout_all['single_pred_time_ms']:.4f} ms\")\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"Evaluating TUNED Hybrid Word Char + Logistic Regression on Holdout Data...\")\n",
        "    model_holdout_word_char, metrics_holdout_word_char = train_and_evaluate(\n",
        "        X_val, y_val, tuned_hybrid_word_char_lr,\n",
        "        \"TUNED Hybrid Word Char + Logistic Regression (Holdout)\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nHasil Evaluasi TUNED Hybrid Word Char + Logistic Regression pada Data Holdout:\")\n",
        "    print(f\"Accuracy: {metrics_holdout_word_char['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics_holdout_word_char['precision']:.4f}\")\n",
        "    print(f\"Recall: {metrics_holdout_word_char['recall']:.4f}\")\n",
        "    print(f\"F1-Score: {metrics_holdout_word_char['f1_score']:.4f}\")\n",
        "    print(f\"Single Prediction Time (avg): {metrics_holdout_word_char['single_pred_time_ms']:.4f} ms\")\n",
        "\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nSkipping holdout evaluation: Tuned models ('grid_search_hybrid_all_lr' or 'grid_search_hybrid_word_char_lr') not found. Please run the tuning cell first.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during holdout evaluation: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofIEK8-6TULr",
        "outputId": "b0a6e078-bcfb-42ad-9ba6-0a73f71e22e9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "OPSI 6: EVALUASI MODEL TUNED PADA DATA HOLDOUT\n",
            "============================================================\n",
            "\n",
            "Evaluating TUNED Hybrid All Features + Logistic Regression on Holdout Data...\n",
            "\n",
            "============================================================\n",
            "EVALUASI: TUNED Hybrid All Features + Logistic Regression (Holdout)\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 0.3134 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.0489 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.2089 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 2.8740 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 2.4090 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 8.1997 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      0.99      0.98       189\n",
            "        Judi       0.95      0.91      0.93        45\n",
            "\n",
            "    accuracy                           0.97       234\n",
            "   macro avg       0.97      0.95      0.96       234\n",
            "weighted avg       0.97      0.97      0.97       234\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[187   2]\n",
            " [  4  41]]\n",
            "\n",
            "Accuracy: 0.9744\n",
            "Precision: 0.9535\n",
            "Recall: 0.9111\n",
            "F1-Score: 0.9318\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 2.0250 seconds (0.03 minutes)\n",
            "Cross-Validation F1-Score: 0.9619 (+/- 0.0091)\n",
            "\n",
            "Hasil Evaluasi TUNED Hybrid All Features + Logistic Regression pada Data Holdout:\n",
            "Accuracy: 0.9744\n",
            "Precision: 0.9535\n",
            "Recall: 0.9111\n",
            "F1-Score: 0.9318\n",
            "Single Prediction Time (avg): 2.8740 ms\n",
            "\n",
            "------------------------------------------------------------\n",
            "Evaluating TUNED Hybrid Word Char + Logistic Regression on Holdout Data...\n",
            "\n",
            "============================================================\n",
            "EVALUASI: TUNED Hybrid Word Char + Logistic Regression (Holdout)\n",
            "============================================================\n",
            "Training model...\n",
            "‚è±Ô∏è  Training Time: 0.4090 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "‚è±Ô∏è  Batch Prediction Time: 0.0737 seconds\n",
            "‚è±Ô∏è  Average Time per Sample: 0.3148 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "‚è±Ô∏è  Single Prediction Time (avg): 3.1331 ms\n",
            "‚è±Ô∏è  Single Prediction Time (min): 2.6534 ms\n",
            "‚è±Ô∏è  Single Prediction Time (max): 5.9240 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      0.99      0.98       189\n",
            "        Judi       0.95      0.91      0.93        45\n",
            "\n",
            "    accuracy                           0.97       234\n",
            "   macro avg       0.97      0.95      0.96       234\n",
            "weighted avg       0.97      0.97      0.97       234\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[187   2]\n",
            " [  4  41]]\n",
            "\n",
            "Accuracy: 0.9744\n",
            "Precision: 0.9535\n",
            "Recall: 0.9111\n",
            "F1-Score: 0.9318\n",
            "\n",
            "Performing cross-validation...\n",
            "‚è±Ô∏è  Cross-Validation Time: 1.9003 seconds (0.03 minutes)\n",
            "Cross-Validation F1-Score: 0.9545 (+/- 0.0195)\n",
            "\n",
            "Hasil Evaluasi TUNED Hybrid Word Char + Logistic Regression pada Data Holdout:\n",
            "Accuracy: 0.9744\n",
            "Precision: 0.9535\n",
            "Recall: 0.9111\n",
            "F1-Score: 0.9318\n",
            "Single Prediction Time (avg): 3.1331 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "579e5496",
        "outputId": "5e3301e8-881a-4320-9000-0fb4112b4579"
      },
      "source": [
        "# Get predictions from the best tuned model on the holdout data\n",
        "# Assuming tuned_hybrid_all_lr is the best model from previous tuning\n",
        "try:\n",
        "    y_pred_holdout = tuned_hybrid_all_lr.predict(X_val)\n",
        "\n",
        "    # Find misclassified indices\n",
        "    misclassified_indices = np.where(y_val != y_pred_holdout)[0]\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MISCLASSIFIED SAMPLES ON HOLDOUT DATA ({len(misclassified_indices)} total)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Display misclassified samples\n",
        "    misclassified_data = df_holdout.iloc[misclassified_indices].copy()\n",
        "    misclassified_data['predicted_label'] = y_pred_holdout[misclassified_indices]\n",
        "\n",
        "    display(misclassified_data)\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nSkipping misspredict data display: Tuned model ('tuned_hybrid_all_lr') not found. Please run the tuning cell first.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while displaying misspredict data: {str(e)}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MISCLASSIFIED SAMPLES ON HOLDOUT DATA (6 total)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                comment  label  \\\n",
              "497               beneran gampang hasilnya nyata banget      1   \n",
              "604                                         nggak gacor      0   \n",
              "665           ubur ubur ikan lele ùôÅoùôçùôèùôêùôâùôÄùüÆzùü¥ mantap lee      1   \n",
              "668                      gicir murah jeppe lsg mendarat      1   \n",
              "930   banget ya muka lu macing¬≤ unboxing 3 btw nanam...      0   \n",
              "1137                                    ùë™ùëµùë´88 bosan ser      1   \n",
              "\n",
              "      predicted_label  \n",
              "497                 0  \n",
              "604                 1  \n",
              "665                 0  \n",
              "668                 0  \n",
              "930                 1  \n",
              "1137                0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc2f4d0a-33c7-4d2c-ba6e-6c492256a683\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>beneran gampang hasilnya nyata banget</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>604</th>\n",
              "      <td>nggak gacor</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665</th>\n",
              "      <td>ubur ubur ikan lele ùôÅoùôçùôèùôêùôâùôÄùüÆzùü¥ mantap lee</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>668</th>\n",
              "      <td>gicir murah jeppe lsg mendarat</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>930</th>\n",
              "      <td>banget ya muka lu macing¬≤ unboxing 3 btw nanam...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1137</th>\n",
              "      <td>ùë™ùëµùë´88 bosan ser</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc2f4d0a-33c7-4d2c-ba6e-6c492256a683')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cc2f4d0a-33c7-4d2c-ba6e-6c492256a683 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cc2f4d0a-33c7-4d2c-ba6e-6c492256a683');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2c2ba4d6-64b8-4319-83db-6176c165f2ac\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c2ba4d6-64b8-4319-83db-6176c165f2ac')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2c2ba4d6-64b8-4319-83db-6176c165f2ac button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_f26bb742-a22c-4c64-babd-6f8c93e12d54\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('misclassified_data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f26bb742-a22c-4c64-babd-6f8c93e12d54 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('misclassified_data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "misclassified_data",
              "summary": "{\n  \"name\": \"misclassified_data\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"beneran gampang hasilnya nyata banget\",\n          \"nggak gacor\",\n          \"\\ud835\\udc6a\\ud835\\udc75\\ud835\\udc6b88 bosan ser\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}