{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93420ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8888, 6) (2335, 6) (1167, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_feature_engineered = pd.read_csv(\"dataset/feature_enginering/train_feature_engineered.csv\")\n",
    "df_test_feature_engineered = pd.read_csv(\"dataset/feature_enginering/test_feature_engineered.csv\")\n",
    "df_holdout_feature_engineered = pd.read_csv(\"dataset/feature_enginering/holdout_feature_engineered.csv\")\n",
    "\n",
    "\n",
    "print(df_train_feature_engineered.shape, df_test_feature_engineered.shape, df_holdout_feature_engineered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28db2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def create_tfidf_vectorizer(\n",
    "    corpus: Union[List[str], pd.Series],\n",
    "    **kwargs,\n",
    ") -> TfidfVectorizer:\n",
    "    \"\"\"Create and fit a TfidfVectorizer on a text corpus, then return it.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    vectorizer.fit(list(corpus))\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "def transform_comment_to_tfidf(\n",
    "    comments: Union[str, List[str], pd.Series],\n",
    "    *,\n",
    "    vectorizer: TfidfVectorizer,\n",
    ") -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Transform a single comment or a list/Series of comments to TF-IDF vector(s)\n",
    "    using a fitted sklearn.feature_extraction.text.TfidfVectorizer.\n",
    "    \"\"\"\n",
    "    if isinstance(comments, str):\n",
    "        texts = [comments]\n",
    "    elif isinstance(comments, (list, pd.Series)):\n",
    "        texts = list(comments)\n",
    "    else:\n",
    "        raise TypeError(\"comments must be a str, list[str], or pandas.Series of str\")\n",
    "\n",
    "    return vectorizer.transform(texts)\n",
    "\n",
    "# Example (illustrative):\n",
    "# vec = create_tfidf_vectorizer(df_train_feature_engineered[\"comment\"],\n",
    "#                               lowercase=True, ngram_range=(1,2), max_features=50000)\n",
    "# X = transform_comment_to_tfidf([\"contoh komentar\"], vectorizer=vec)\n",
    "# X_single = transform_comment_to_tfidf(\"komentar tunggal\", vectorizer=vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266532af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before SMOTE:\n",
      "  Train: (8888, 5004)\n",
      "  Test: (2335, 5004)\n",
      "  Holdout: (1167, 5004)\n",
      "Label distribution before SMOTE: {0: 7454, 1: 1434}\n",
      "Train after SMOTE: (14908, 5004)\n",
      "Label distribution after SMOTE: {0: 7454, 1: 7454}\n",
      "Model trained.\n",
      "Saved model to: models/tfidf_num_logreg.pkl\n",
      "test accuracy: 0.979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/code/penambangan-data/.venv/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IBM PIPELINE: CONFIGURABLE TEXT CLASSIFICATION PIPELINE\n",
    "# =============================================================================\n",
    "# This pipeline supports:\n",
    "# - Configurable vectorizers (TF-IDF, Word2Vec, Sentence Transformers)\n",
    "# - Configurable models (Logistic Regression, Random Forest, SVM, etc.)\n",
    "# - Automatic feature engineering\n",
    "# - SMOTE balancing\n",
    "# - Model persistence and evaluation\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, Union, List, Tuple, Optional, Sequence\n",
    "from dataclasses import dataclass\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION CLASSES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Configuration for the entire pipeline\"\"\"\n",
    "    # Data paths\n",
    "    train_path: str = \"dataset/feature_enginering/train_feature_engineered.csv\"\n",
    "    test_path: str = \"dataset/feature_enginering/test_feature_engineered.csv\"\n",
    "    holdout_path: str = \"dataset/feature_enginering/holdout_feature_engineered.csv\"\n",
    "    \n",
    "    # Feature engineering\n",
    "    text_column: str = \"comment\"\n",
    "    label_column: str = \"label\"\n",
    "    numeric_features: List[str] = None\n",
    "    \n",
    "    # Vectorizer settings\n",
    "    vectorizer_type: str = \"tfidf\"  # tfidf, word2vec, sentence_transformer\n",
    "    vectorizer_params: Dict[str, Any] = None\n",
    "    \n",
    "    # Model settings\n",
    "    model_type: str = \"logistic_regression\"  # logistic_regression, random_forest, svm\n",
    "    model_params: Dict[str, Any] = None\n",
    "    \n",
    "    # SMOTE settings\n",
    "    use_smote: bool = True\n",
    "    smote_params: Dict[str, Any] = None\n",
    "    \n",
    "    # Output settings\n",
    "    output_dir: str = \"models\"\n",
    "    save_artifacts: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.numeric_features is None:\n",
    "            self.numeric_features = [\"text_length\", \"count_word\", \"num_emoji\", \"num_foreign_character\"]\n",
    "        \n",
    "        if self.vectorizer_params is None:\n",
    "            self.vectorizer_params = {}\n",
    "            \n",
    "        if self.model_params is None:\n",
    "            self.model_params = {}\n",
    "            \n",
    "        if self.smote_params is None:\n",
    "            self.smote_params = {\"sampling_strategy\": \"minority\", \"random_state\": 42}\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE ENGINEERING MODULE\n",
    "# =============================================================================\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Handles text feature engineering\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_emojis(text: str) -> int:\n",
    "        \"\"\"Count emojis in text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return 0\n",
    "        return len(emoji.emoji_list(text))\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_foreign_alpha_characters(text: str) -> int:\n",
    "        \"\"\"Count non-ASCII alphabetic characters\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return 0\n",
    "        count = 0\n",
    "        for ch in text:\n",
    "            if ch.isalpha() and ord(ch) > 127:\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_text_features(cls, text_col) -> Dict[str, Any]:\n",
    "        \"\"\"Compute text features for single text or pandas Series\"\"\"\n",
    "        if isinstance(text_col, str):\n",
    "            s = text_col\n",
    "            return {\n",
    "                \"text_length\": len(s),\n",
    "                \"count_word\": len(s.split()),\n",
    "                \"num_emoji\": cls.count_emojis(s),\n",
    "                \"num_foreign_character\": cls.count_foreign_alpha_characters(s),\n",
    "            }\n",
    "        elif hasattr(text_col, \"apply\"):\n",
    "            s = text_col.fillna(\"\")\n",
    "            return {\n",
    "                \"text_length\": s.str.len(),\n",
    "                \"count_word\": s.str.split().str.len(),\n",
    "                \"num_emoji\": s.apply(cls.count_emojis),\n",
    "                \"num_foreign_character\": s.apply(cls.count_foreign_alpha_characters),\n",
    "            }\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a string or a pandas Series.\")\n",
    "    \n",
    "    @classmethod\n",
    "    def add_text_features(cls, df: pd.DataFrame, text_col: str = \"comment\") -> pd.DataFrame:\n",
    "        \"\"\"Add text features to dataframe\"\"\"\n",
    "        df = df.copy()\n",
    "        features = cls.compute_text_features(df[text_col])\n",
    "        for feature_name, feature_values in features.items():\n",
    "            df[feature_name] = feature_values\n",
    "        return df\n",
    "\n",
    "# =============================================================================\n",
    "# VECTORIZER ABSTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "class BaseVectorizer(ABC):\n",
    "    \"\"\"Abstract base class for text vectorizers\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, texts: List[str]) -> 'BaseVectorizer':\n",
    "        \"\"\"Fit the vectorizer on texts\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Transform texts to vectors\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save vectorizer to disk\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def load(cls, path: str) -> 'BaseVectorizer':\n",
    "        \"\"\"Load vectorizer from disk\"\"\"\n",
    "        pass\n",
    "\n",
    "class TfidfVectorizerWrapper(BaseVectorizer):\n",
    "    \"\"\"TF-IDF vectorizer wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        self.vectorizer = TfidfVectorizer(**params)\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, texts: List[str]) -> 'TfidfVectorizerWrapper':\n",
    "        self.vectorizer.fit(texts)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Vectorizer must be fitted before transform\")\n",
    "        return self.vectorizer.transform(texts).toarray()\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'TfidfVectorizerWrapper':\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "class Word2VecVectorizer(BaseVectorizer):\n",
    "    \"\"\"Word2Vec vectorizer wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def _tokenize(self, texts: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Simple tokenization - can be enhanced\"\"\"\n",
    "        return [text.lower().split() for text in texts]\n",
    "    \n",
    "    def fit(self, texts: List[str]) -> 'Word2VecVectorizer':\n",
    "        tokenized_texts = self._tokenize(texts)\n",
    "        self.model = Word2Vec(sentences=tokenized_texts, **self.params)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Vectorizer must be fitted before transform\")\n",
    "        \n",
    "        tokenized_texts = self._tokenize(texts)\n",
    "        vectors = []\n",
    "        for tokens in tokenized_texts:\n",
    "            if tokens:\n",
    "                # Average word vectors for the sentence\n",
    "                word_vectors = [self.model.wv[word] for word in tokens if word in self.model.wv]\n",
    "                if word_vectors:\n",
    "                    vectors.append(np.mean(word_vectors, axis=0))\n",
    "                else:\n",
    "                    vectors.append(np.zeros(self.model.wv.vector_size))\n",
    "            else:\n",
    "                vectors.append(np.zeros(self.model.wv.vector_size))\n",
    "        \n",
    "        return np.array(vectors)\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'Word2VecVectorizer':\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "class SentenceTransformerVectorizer(BaseVectorizer):\n",
    "    \"\"\"Sentence Transformer vectorizer wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", **params):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, texts: List[str]) -> 'SentenceTransformerVectorizer':\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Vectorizer must be fitted before transform\")\n",
    "        return self.model.encode(texts)\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'SentenceTransformerVectorizer':\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL ABSTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\"Abstract base class for models\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'BaseModel':\n",
    "        \"\"\"Fit the model\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict probabilities\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save model to disk\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def load(cls, path: str) -> 'BaseModel':\n",
    "        \"\"\"Load model from disk\"\"\"\n",
    "        pass\n",
    "\n",
    "class SklearnModelWrapper(BaseModel):\n",
    "    \"\"\"Wrapper for sklearn models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, **params):\n",
    "        self.model_class = model_class\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'SklearnModelWrapper':\n",
    "        self.model = self.model_class(**self.params)\n",
    "        self.model.fit(X, y)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before predict\")\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before predict_proba\")\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            # For models without predict_proba, return hard predictions\n",
    "            predictions = self.predict(X)\n",
    "            proba = np.zeros((len(predictions), 2))\n",
    "            proba[np.arange(len(predictions)), predictions] = 1\n",
    "            return proba\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'SklearnModelWrapper':\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PIPELINE CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class TextClassificationPipeline:\n",
    "    \"\"\"Main pipeline class for text classification\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.vectorizer = None\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "        self.feature_engineer = FeatureEngineer()\n",
    "    \n",
    "    def _create_vectorizer(self) -> BaseVectorizer:\n",
    "        \"\"\"Create vectorizer based on config\"\"\"\n",
    "        vectorizer_type = self.config.vectorizer_type.lower()\n",
    "        params = self.config.vectorizer_params.copy()\n",
    "        \n",
    "        if vectorizer_type == \"tfidf\":\n",
    "            return TfidfVectorizerWrapper(**params)\n",
    "        elif vectorizer_type == \"word2vec\":\n",
    "            return Word2VecVectorizer(**params)\n",
    "        elif vectorizer_type == \"sentence_transformer\":\n",
    "            return SentenceTransformerVectorizer(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported vectorizer type: {vectorizer_type}\")\n",
    "    \n",
    "    def _create_model(self) -> BaseModel:\n",
    "        \"\"\"Create model based on config\"\"\"\n",
    "        model_type = self.config.model_type.lower()\n",
    "        params = self.config.model_params.copy()\n",
    "        \n",
    "        if model_type == \"logistic_regression\":\n",
    "            return SklearnModelWrapper(LogisticRegression, **params)\n",
    "        elif model_type == \"random_forest\":\n",
    "            return SklearnModelWrapper(RandomForestClassifier, **params)\n",
    "        elif model_type == \"svm\":\n",
    "            return SklearnModelWrapper(SVC, **params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    def _load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load train, test, and holdout data\"\"\"\n",
    "        train_df = pd.read_csv(self.config.train_path)\n",
    "        test_df = pd.read_csv(self.config.test_path)\n",
    "        holdout_df = pd.read_csv(self.config.holdout_path)\n",
    "        \n",
    "        # Add text features if not already present\n",
    "        for df_name, df in [(\"train\", train_df), (\"test\", test_df), (\"holdout\", holdout_df)]:\n",
    "            if not all(col in df.columns for col in self.config.numeric_features):\n",
    "                print(f\"Adding text features to {df_name} data...\")\n",
    "                if df_name == \"train\":\n",
    "                    train_df = self.feature_engineer.add_text_features(df, self.config.text_column)\n",
    "                elif df_name == \"test\":\n",
    "                    test_df = self.feature_engineer.add_text_features(df, self.config.text_column)\n",
    "                else:\n",
    "                    holdout_df = self.feature_engineer.add_text_features(df, self.config.text_column)\n",
    "        \n",
    "        return train_df, test_df, holdout_df\n",
    "    \n",
    "    def _prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepare features for training/prediction\"\"\"\n",
    "        # Text features\n",
    "        texts = df[self.config.text_column].astype(str).tolist()\n",
    "        X_text = self.vectorizer.transform(texts)\n",
    "        \n",
    "        # Numeric features\n",
    "        X_numeric = df[self.config.numeric_features].astype(float).fillna(0.0).values\n",
    "        \n",
    "        # Scale numeric features\n",
    "        if self.scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_numeric_scaled = self.scaler.fit_transform(X_numeric)\n",
    "        else:\n",
    "            X_numeric_scaled = self.scaler.transform(X_numeric)\n",
    "        \n",
    "        # Combine features\n",
    "        X_combined = np.hstack([X_text, X_numeric_scaled])\n",
    "        \n",
    "        # Labels\n",
    "        y = df[self.config.label_column].astype(int).values\n",
    "        \n",
    "        return X_combined, y\n",
    "    \n",
    "    def fit(self) -> 'TextClassificationPipeline':\n",
    "        \"\"\"Fit the entire pipeline\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        train_df, test_df, holdout_df = self._load_data()\n",
    "        \n",
    "        print(\"Creating vectorizer...\")\n",
    "        self.vectorizer = self._create_vectorizer()\n",
    "        \n",
    "        print(\"Fitting vectorizer...\")\n",
    "        self.vectorizer.fit(train_df[self.config.text_column].astype(str).tolist())\n",
    "        \n",
    "        print(\"Preparing training features...\")\n",
    "        X_train, y_train = self._prepare_features(train_df)\n",
    "        \n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Label distribution: {np.bincount(y_train)}\")\n",
    "        \n",
    "        # Apply SMOTE if enabled\n",
    "        if self.config.use_smote:\n",
    "            print(\"Applying SMOTE...\")\n",
    "            smote = SMOTE(**self.config.smote_params)\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "            print(f\"After SMOTE - Training data shape: {X_train.shape}\")\n",
    "            print(f\"After SMOTE - Label distribution: {np.bincount(y_train)}\")\n",
    "        \n",
    "        print(\"Creating and training model...\")\n",
    "        self.model = self._create_model()\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Pipeline fitted successfully!\")\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, test_df: pd.DataFrame = None) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate the pipeline\"\"\"\n",
    "        if test_df is None:\n",
    "            _, test_df, _ = self._load_data()\n",
    "        \n",
    "        X_test, y_test = self._prepare_features(test_df)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_proba = self.model.predict_proba(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        results = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"predictions\": y_pred,\n",
    "            \"probabilities\": y_proba,\n",
    "            \"classification_report\": classification_report(y_test, y_pred),\n",
    "            \"confusion_matrix\": confusion_matrix(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(results[\"classification_report\"])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, output_dir: str = None) -> None:\n",
    "        \"\"\"Save pipeline artifacts\"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = self.config.output_dir\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save vectorizer\n",
    "        vectorizer_path = os.path.join(output_dir, f\"{self.config.vectorizer_type}_vectorizer.pkl\")\n",
    "        self.vectorizer.save(vectorizer_path)\n",
    "        print(f\"Vectorizer saved to: {vectorizer_path}\")\n",
    "        \n",
    "        # Save scaler\n",
    "        scaler_path = os.path.join(output_dir, \"feature_scaler.pkl\")\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        print(f\"Scaler saved to: {scaler_path}\")\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(output_dir, f\"{self.config.model_type}_{self.config.vectorizer_type}_model.pkl\")\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to: {model_path}\")\n",
    "        \n",
    "        # Save config\n",
    "        config_path = os.path.join(output_dir, \"pipeline_config.pkl\")\n",
    "        with open(config_path, 'wb') as f:\n",
    "            pickle.dump(self.config, f)\n",
    "        print(f\"Config saved to: {config_path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, output_dir: str) -> 'TextClassificationPipeline':\n",
    "        \"\"\"Load pipeline from disk\"\"\"\n",
    "        # Load config\n",
    "        config_path = os.path.join(output_dir, \"pipeline_config.pkl\")\n",
    "        with open(config_path, 'rb') as f:\n",
    "            config = pickle.load(f)\n",
    "        \n",
    "        pipeline = cls(config)\n",
    "        \n",
    "        # Load vectorizer\n",
    "        vectorizer_path = os.path.join(output_dir, f\"{config.vectorizer_type}_vectorizer.pkl\")\n",
    "        if config.vectorizer_type == \"tfidf\":\n",
    "            pipeline.vectorizer = TfidfVectorizerWrapper.load(vectorizer_path)\n",
    "        elif config.vectorizer_type == \"word2vec\":\n",
    "            pipeline.vectorizer = Word2VecVectorizer.load(vectorizer_path)\n",
    "        elif config.vectorizer_type == \"sentence_transformer\":\n",
    "            pipeline.vectorizer = SentenceTransformerVectorizer.load(vectorizer_path)\n",
    "        \n",
    "        # Load scaler\n",
    "        scaler_path = os.path.join(output_dir, \"feature_scaler.pkl\")\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            pipeline.scaler = pickle.load(f)\n",
    "        \n",
    "        # Load model\n",
    "        model_path = os.path.join(output_dir, f\"{config.model_type}_{config.vectorizer_type}_model.pkl\")\n",
    "        pipeline.model = SklearnModelWrapper.load(model_path)\n",
    "        \n",
    "        return pipeline\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE AND CONFIGURATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_tfidf_logreg_config() -> PipelineConfig:\n",
    "    \"\"\"Create config for TF-IDF + Logistic Regression\"\"\"\n",
    "    return PipelineConfig(\n",
    "        vectorizer_type=\"tfidf\",\n",
    "        vectorizer_params={\n",
    "            \"max_features\": 50000,\n",
    "            \"ngram_range\": (1, 2),\n",
    "            \"lowercase\": True,\n",
    "            \"stop_words\": None\n",
    "        },\n",
    "        model_type=\"logistic_regression\",\n",
    "        model_params={\n",
    "            \"solver\": \"saga\",\n",
    "            \"penalty\": \"l2\",\n",
    "            \"C\": 1.0,\n",
    "            \"max_iter\": 2000,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    )\n",
    "\n",
    "def create_word2vec_rf_config() -> PipelineConfig:\n",
    "    \"\"\"Create config for Word2Vec + Random Forest\"\"\"\n",
    "    return PipelineConfig(\n",
    "        vectorizer_type=\"word2vec\",\n",
    "        vectorizer_params={\n",
    "            \"vector_size\": 100,\n",
    "            \"window\": 5,\n",
    "            \"min_count\": 1,\n",
    "            \"workers\": 4\n",
    "        },\n",
    "        model_type=\"random_forest\",\n",
    "        model_params={\n",
    "            \"n_estimators\": 100,\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1\n",
    "        }\n",
    "    )\n",
    "\n",
    "def create_sentence_transformer_svm_config() -> PipelineConfig:\n",
    "    \"\"\"Create config for Sentence Transformer + SVM\"\"\"\n",
    "    return PipelineConfig(\n",
    "        vectorizer_type=\"sentence_transformer\",\n",
    "        vectorizer_params={\n",
    "            \"model_name\": \"all-MiniLM-L6-v2\"\n",
    "        },\n",
    "        model_type=\"svm\",\n",
    "        model_params={\n",
    "            \"kernel\": \"rbf\",\n",
    "            \"C\": 1.0,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"IBM Pipeline: Configurable Text Classification Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Available configurations:\")\n",
    "print(\"1. TF-IDF + Logistic Regression\")\n",
    "print(\"2. Word2Vec + Random Forest\") \n",
    "print(\"3. Sentence Transformer + SVM\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 1: TF-IDF + Logistic Regression Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Example 1: TF-IDF + Logistic Regression\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create configuration\n",
    "config = create_tfidf_logreg_config()\n",
    "config.output_dir = \"models/tfidf_logreg_pipeline\"\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = TextClassificationPipeline(config)\n",
    "pipeline.fit()\n",
    "\n",
    "# Evaluate\n",
    "results = pipeline.evaluate()\n",
    "print(f\"Test Accuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "# Save pipeline\n",
    "pipeline.save()\n",
    "print(\"Pipeline saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 2: Word2Vec + Random Forest Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Example 2: Word2Vec + Random Forest\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create configuration\n",
    "config = create_word2vec_rf_config()\n",
    "config.output_dir = \"models/word2vec_rf_pipeline\"\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = TextClassificationPipeline(config)\n",
    "pipeline.fit()\n",
    "\n",
    "# Evaluate\n",
    "results = pipeline.evaluate()\n",
    "print(f\"Test Accuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "# Save pipeline\n",
    "pipeline.save()\n",
    "print(\"Pipeline saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 3: Sentence Transformer + SVM Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Example 3: Sentence Transformer + SVM\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create configuration\n",
    "config = create_sentence_transformer_svm_config()\n",
    "config.output_dir = \"models/sentence_transformer_svm_pipeline\"\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = TextClassificationPipeline(config)\n",
    "pipeline.fit()\n",
    "\n",
    "# Evaluate\n",
    "results = pipeline.evaluate()\n",
    "print(f\"Test Accuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "# Save pipeline\n",
    "pipeline.save()\n",
    "print(\"Pipeline saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 4: Custom Configuration\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Example 4: Custom Configuration\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create custom configuration\n",
    "custom_config = PipelineConfig(\n",
    "    # Data paths\n",
    "    train_path=\"dataset/feature_enginering/train_feature_engineered.csv\",\n",
    "    test_path=\"dataset/feature_enginering/test_feature_engineered.csv\",\n",
    "    holdout_path=\"dataset/feature_enginering/holdout_feature_engineered.csv\",\n",
    "    \n",
    "    # Vectorizer settings\n",
    "    vectorizer_type=\"tfidf\",\n",
    "    vectorizer_params={\n",
    "        \"max_features\": 30000,\n",
    "        \"ngram_range\": (1, 3),  # Use trigrams\n",
    "        \"lowercase\": True,\n",
    "        \"stop_words\": \"english\"  # Use English stop words\n",
    "    },\n",
    "    \n",
    "    # Model settings\n",
    "    model_type=\"random_forest\",\n",
    "    model_params={\n",
    "        \"n_estimators\": 200,\n",
    "        \"max_depth\": 10,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    },\n",
    "    \n",
    "    # SMOTE settings\n",
    "    use_smote=True,\n",
    "    smote_params={\n",
    "        \"sampling_strategy\": \"minority\",\n",
    "        \"random_state\": 42,\n",
    "        \"k_neighbors\": 5\n",
    "    },\n",
    "    \n",
    "    # Output settings\n",
    "    output_dir=\"models/custom_pipeline\",\n",
    "    save_artifacts=True\n",
    ")\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = TextClassificationPipeline(custom_config)\n",
    "pipeline.fit()\n",
    "\n",
    "# Evaluate\n",
    "results = pipeline.evaluate()\n",
    "print(f\"Test Accuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "# Save pipeline\n",
    "pipeline.save()\n",
    "print(\"Custom pipeline saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee816aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 5: Loading and Using Saved Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Example 5: Loading and Using Saved Pipeline\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load a saved pipeline\n",
    "try:\n",
    "    loaded_pipeline = TextClassificationPipeline.load(\"models/tfidf_logreg_pipeline\")\n",
    "    print(\"Pipeline loaded successfully!\")\n",
    "    \n",
    "    # Make predictions on new data\n",
    "    _, test_df, _ = loaded_pipeline._load_data()\n",
    "    \n",
    "    # Get predictions for first 5 test samples\n",
    "    X_test, y_test = loaded_pipeline._prepare_features(test_df.head(5))\n",
    "    predictions = loaded_pipeline.model.predict(X_test)\n",
    "    probabilities = loaded_pipeline.model.predict_proba(X_test)\n",
    "    \n",
    "    print(\"\\nSample predictions:\")\n",
    "    for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "        print(f\"Sample {i+1}: Prediction={pred}, Probability={prob}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading pipeline: {e}\")\n",
    "    print(\"Make sure to run Example 1 first to create the pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE COMPARISON AND BENCHMARKING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Pipeline Comparison and Benchmarking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "def benchmark_pipeline(config: PipelineConfig, name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Benchmark a pipeline configuration\"\"\"\n",
    "    print(f\"\\nBenchmarking {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create pipeline\n",
    "        pipeline = TextClassificationPipeline(config)\n",
    "        \n",
    "        # Fit pipeline\n",
    "        fit_start = time.time()\n",
    "        pipeline.fit()\n",
    "        fit_time = time.time() - fit_start\n",
    "        \n",
    "        # Evaluate pipeline\n",
    "        eval_start = time.time()\n",
    "        results = pipeline.evaluate()\n",
    "        eval_time = time.time() - eval_start\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"accuracy\": results[\"accuracy\"],\n",
    "            \"fit_time\": fit_time,\n",
    "            \"eval_time\": eval_time,\n",
    "            \"total_time\": total_time,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"fit_time\": 0.0,\n",
    "            \"eval_time\": 0.0,\n",
    "            \"total_time\": time.time() - start_time,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Define benchmark configurations\n",
    "benchmark_configs = [\n",
    "    (create_tfidf_logreg_config(), \"TF-IDF + Logistic Regression\"),\n",
    "    (create_word2vec_rf_config(), \"Word2Vec + Random Forest\"),\n",
    "    (create_sentence_transformer_svm_config(), \"Sentence Transformer + SVM\")\n",
    "]\n",
    "\n",
    "# Run benchmarks\n",
    "results = []\n",
    "for config, name in benchmark_configs:\n",
    "    config.output_dir = f\"models/benchmark_{name.lower().replace(' ', '_').replace('+', '_')}\"\n",
    "    result = benchmark_pipeline(config, name)\n",
    "    results.append(result)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Pipeline':<40} {'Accuracy':<10} {'Fit Time':<10} {'Eval Time':<10} {'Total Time':<10} {'Status':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for result in results:\n",
    "    if result[\"success\"]:\n",
    "        print(f\"{result['name']:<40} {result['accuracy']:<10.4f} {result['fit_time']:<10.2f} {result['eval_time']:<10.2f} {result['total_time']:<10.2f} {'SUCCESS':<10}\")\n",
    "    else:\n",
    "        print(f\"{result['name']:<40} {'N/A':<10} {'N/A':<10} {'N/A':<10} {'N/A':<10} {'FAILED':<10}\")\n",
    "        print(f\"  Error: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303286b",
   "metadata": {},
   "source": [
    "# IBM Pipeline: Configurable Text Classification Pipeline\n",
    "\n",
    "## Overview\n",
    "This pipeline provides a flexible, modular approach to text classification with the following features:\n",
    "\n",
    "### ✅ **Configurable Components**\n",
    "- **Vectorizers**: TF-IDF, Word2Vec, Sentence Transformers\n",
    "- **Models**: Logistic Regression, Random Forest, SVM (easily extensible)\n",
    "- **Feature Engineering**: Automatic text feature extraction\n",
    "- **Balancing**: SMOTE for handling class imbalance\n",
    "\n",
    "### ✅ **Key Features**\n",
    "1. **Modular Design**: Easy to swap vectorizers and models\n",
    "2. **Automatic Feature Engineering**: Computes text length, word count, emoji count, foreign characters\n",
    "3. **SMOTE Integration**: Handles class imbalance automatically\n",
    "4. **Model Persistence**: Save and load complete pipelines\n",
    "5. **Comprehensive Evaluation**: Accuracy, classification report, confusion matrix\n",
    "6. **Benchmarking**: Compare different configurations\n",
    "\n",
    "### ✅ **Usage Examples**\n",
    "\n",
    "#### Basic Usage\n",
    "```python\n",
    "# Create configuration\n",
    "config = create_tfidf_logreg_config()\n",
    "config.output_dir = \"models/my_pipeline\"\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = TextClassificationPipeline(config)\n",
    "pipeline.fit()\n",
    "results = pipeline.evaluate()\n",
    "pipeline.save()\n",
    "```\n",
    "\n",
    "#### Custom Configuration\n",
    "```python\n",
    "# Custom configuration\n",
    "config = PipelineConfig(\n",
    "    vectorizer_type=\"tfidf\",\n",
    "    vectorizer_params={\"max_features\": 30000, \"ngram_range\": (1, 3)},\n",
    "    model_type=\"random_forest\",\n",
    "    model_params={\"n_estimators\": 200, \"max_depth\": 10},\n",
    "    use_smote=True,\n",
    "    output_dir=\"models/custom_pipeline\"\n",
    ")\n",
    "\n",
    "pipeline = TextClassificationPipeline(config)\n",
    "pipeline.fit()\n",
    "```\n",
    "\n",
    "#### Loading Saved Pipeline\n",
    "```python\n",
    "# Load saved pipeline\n",
    "pipeline = TextClassificationPipeline.load(\"models/my_pipeline\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipeline.model.predict(X_new)\n",
    "probabilities = pipeline.model.predict_proba(X_new)\n",
    "```\n",
    "\n",
    "### ✅ **Supported Vectorizers**\n",
    "- **TF-IDF**: Traditional bag-of-words with n-grams\n",
    "- **Word2Vec**: Word embeddings with averaging\n",
    "- **Sentence Transformers**: Pre-trained sentence embeddings\n",
    "\n",
    "### ✅ **Supported Models**\n",
    "- **Logistic Regression**: Fast, interpretable\n",
    "- **Random Forest**: Robust, handles non-linearity\n",
    "- **SVM**: Good for high-dimensional data\n",
    "\n",
    "### ✅ **Pipeline Flow**\n",
    "1. **Data Loading** → Load train/test/holdout datasets\n",
    "2. **Feature Engineering** → Add text features (length, word count, emojis, etc.)\n",
    "3. **Vectorization** → Convert text to numerical features\n",
    "4. **Scaling** → Normalize numeric features\n",
    "5. **SMOTE** → Balance classes (optional)\n",
    "6. **Training** → Fit the model\n",
    "7. **Evaluation** → Test performance\n",
    "8. **Persistence** → Save all artifacts\n",
    "\n",
    "### ✅ **Output Files**\n",
    "Each pipeline saves:\n",
    "- `{vectorizer_type}_vectorizer.pkl` - Fitted vectorizer\n",
    "- `feature_scaler.pkl` - Feature scaler\n",
    "- `{model_type}_{vectorizer_type}_model.pkl` - Trained model\n",
    "- `pipeline_config.pkl` - Configuration used\n",
    "\n",
    "### ✅ **Extending the Pipeline**\n",
    "To add new vectorizers or models, simply:\n",
    "1. Inherit from `BaseVectorizer` or `BaseModel`\n",
    "2. Implement required methods\n",
    "3. Add to the factory methods in `TextClassificationPipeline`\n",
    "\n",
    "This pipeline is designed for production use with proper error handling, logging, and modularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130d6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb12e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a813990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338d46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109d03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
