{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e27751",
   "metadata": {},
   "source": [
    "# Data Cleaning & Preprocessing\n",
    "\n",
    "This notebook prepares the online gambling comment dataset for conventional (e.g., TF-IDF + Linear/Tree models) and modern pipelines (e.g., transformer embeddings). It follows the EDA guide's priorities, with a focus on stylized Unicode handling and class imbalance.\n",
    "\n",
    "## Plan\n",
    "- Load raw `train.csv`, `test.csv`, `holdout.csv` with delimiter auto-detection and consistent schema.\n",
    "- Normalize stylized Unicode and consolidate spaced stylized sequences (highest priority).\n",
    "- Clean text while preserving discriminative signals; add feature flags for downstream use.\n",
    "- Create balanced 50:50 splits for each dataset using stratified sampling/oversampling.\n",
    "- Export cleaned and balanced datasets to `processed_data/` as CSV and pickle.\n",
    "- Provide quick validation summaries.\n",
    "\n",
    "Outputs:\n",
    "- `processed_data/*_processed.csv|pkl` (original distribution, cleaned)\n",
    "- `processed_data/*_balanced_50_50.csv|pkl` (balanced by label)\n",
    "- `processed_data/all_datasets.pkl` (aggregated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d676282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import unicodedata\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_DIR = \"processed_data\"\n",
    "RAW_DIR = \"dataset\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cf310cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader with delimiter detection\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def read_csv_auto(path: str) -> pd.DataFrame:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        head = f.read(2048)\n",
    "    # Try csv.Sniffer on small sample\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(head, delimiters=[',',';','\\t'])\n",
    "        sep = dialect.delimiter\n",
    "    except Exception:\n",
    "        # Fallback: detect by header token count\n",
    "        first_line = head.splitlines()[0]\n",
    "        if ';' in first_line and first_line.count(';') >= first_line.count(','):\n",
    "            sep = ';'\n",
    "        else:\n",
    "            sep = ','\n",
    "    df = pd.read_csv(path, sep=sep, encoding='utf-8')\n",
    "    # Normalize column names\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    # Ensure expected schema\n",
    "    rename_map = {}\n",
    "    if 'comment' not in df.columns:\n",
    "        # Try common variants\n",
    "        for c in df.columns:\n",
    "            if 'comment' in c:\n",
    "                rename_map[c] = 'comment'\n",
    "                break\n",
    "    if 'label' not in df.columns:\n",
    "        for c in df.columns:\n",
    "            if 'label' in c or c in {'target','y'}:\n",
    "                rename_map[c] = 'label'\n",
    "                break\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "    assert {'comment','label'}.issubset(set(df.columns)), f\"Missing required columns in {path}: {df.columns}\"\n",
    "    # Coerce types\n",
    "    df['comment'] = df['comment'].astype(str)\n",
    "    df['label'] = pd.to_numeric(df['label'], errors='coerce').fillna(-1).astype(int)\n",
    "    # Drop rows with invalid labels\n",
    "    df = df[df['label'].isin([0,1])].reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17a6d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode blocks and stylized normalization utilities\n",
    "# Ranges for mathematical alphanumeric symbols\n",
    "MATH_ALNUM_RANGES = [\n",
    "    (0x1D400, 0x1D7FF),  # Mathematical Alphanumeric Symbols\n",
    "]\n",
    "\n",
    "FIRE_SYMBOLS = {\"üî•\", \"üí•\", \"‚ö°\"}\n",
    "MONEY_SYMBOLS = {\"üí∞\", \"üíµ\", \"üí∏\", \"ü™ô\", \"üí∑\", \"üí∂\", \"üí¥\", \"‚Çø\"}\n",
    "\n",
    "\n",
    "def is_math_alnum_char(ch: str) -> bool:\n",
    "    cp = ord(ch)\n",
    "    return any(start <= cp <= end for start, end in MATH_ALNUM_RANGES)\n",
    "\n",
    "\n",
    "def normalize_unicode_to_ascii(text: str) -> str:\n",
    "    # Convert to NFKD then strip combining marks\n",
    "    decomposed = unicodedata.normalize('NFKD', text)\n",
    "    filtered = ''.join(c for c in decomposed if not unicodedata.combining(c))\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def consolidate_spaced_stylized(text: str) -> str:\n",
    "    # Collapse sequences like \"ùêÉ ùêé ùôç ùòà\" -> \"ùêÉùêéùôçùòà\"\n",
    "    tokens = text.split()\n",
    "    if len(tokens) <= 1:\n",
    "        return text\n",
    "    # Heuristic: if most tokens are single char and many are math alnum, join them\n",
    "    single_char_tokens = [t for t in tokens if len(t) == 1]\n",
    "    if len(single_char_tokens) >= max(3, int(0.6 * len(tokens))):\n",
    "        # join without spaces\n",
    "        return ''.join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "def map_math_alnum_to_base(ch: str) -> str:\n",
    "    # Map mathematical alphanumeric variants to their base ASCII letter/number when possible\n",
    "    name = unicodedata.name(ch, '')\n",
    "    # Examples: MATHEMATICAL BOLD CAPITAL A -> A\n",
    "    match = re.search(r\"MATHEMATICAL (?:BOLD|ITALIC|BOLD ITALIC|SCRIPT|BOLD SCRIPT|FRAKTUR|DOUBLE-STRUCK|SANS-SERIF|SANS-SERIF BOLD|SANS-SERIF ITALIC|SANS-SERIF BOLD ITALIC|MONOSPACE) (CAPITAL|SMALL) ([A-Z])\", name)\n",
    "    if match:\n",
    "        case, letter = match.groups()\n",
    "        return letter if case == 'CAPITAL' else letter.lower()\n",
    "    # Digits\n",
    "    if name.startswith('MATHEMATICAL DOUBLE-STRUCK DIGIT') or name.startswith('MATHEMATICAL SANS-SERIF DIGIT'):\n",
    "        # Get last number in name\n",
    "        digits = re.findall(r\"\\d\", name)\n",
    "        if digits:\n",
    "            return digits[-1]\n",
    "    return ch\n",
    "\n",
    "\n",
    "def fold_math_alnum(text: str) -> str:\n",
    "    return ''.join(map_math_alnum_to_base(ch) if is_math_alnum_char(ch) else ch for ch in text)\n",
    "\n",
    "\n",
    "def normalize_stylized_text(text: str) -> Tuple[str, dict]:\n",
    "    # Consolidate spaced stylized sequences, then fold math alnum to base\n",
    "    consolidated = consolidate_spaced_stylized(text)\n",
    "    folded = fold_math_alnum(consolidated)\n",
    "    # Keep a plain ASCII fallback for models that need it\n",
    "    ascii_fallback = normalize_unicode_to_ascii(folded)\n",
    "    features = {\n",
    "        'has_math_alnum': any(is_math_alnum_char(c) for c in text),\n",
    "        'money_symbol_count': sum(c in MONEY_SYMBOLS for c in text),\n",
    "        'fire_symbol_count': sum(c in FIRE_SYMBOLS for c in text),\n",
    "    }\n",
    "    return ascii_fallback, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "065330e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and feature flags\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "MENTION_RE = re.compile(r\"@[\\w_]+\", re.UNICODE)\n",
    "HASHTAG_RE = re.compile(r\"#[\\w_]+\", re.UNICODE)\n",
    "REPEAT_CHAR_RE = re.compile(r\"(.)\\1{2,}\")\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "def clean_text_preserve_signals(text: str) -> Tuple[str, dict]:\n",
    "    original = text\n",
    "    # Remove urls, mentions, hashtags\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = MENTION_RE.sub(\" \", text)\n",
    "    text = HASHTAG_RE.sub(\" \", text)\n",
    "    # Consolidate stylized sequences and fold math alnum\n",
    "    norm_ascii, stylized_feats = normalize_stylized_text(text)\n",
    "    # Lowercase for normalization variants\n",
    "    lower = norm_ascii.lower()\n",
    "    # Collapse repeated characters to max 2\n",
    "    lower = REPEAT_CHAR_RE.sub(r\"\\1\\1\", lower)\n",
    "    # Normalize whitespace\n",
    "    lower = WHITESPACE_RE.sub(\" \", lower).strip()\n",
    "    # Feature flags\n",
    "    total_chars = max(len(original), 1)\n",
    "    num_digits = sum(ch.isdigit() for ch in original)\n",
    "    num_special = sum((not ch.isalnum()) and not ch.isspace() for ch in original)\n",
    "    stylized_count = sum(is_math_alnum_char(ch) for ch in original)\n",
    "    features = {\n",
    "        **stylized_feats,\n",
    "        'special_char_ratio': num_special / total_chars,\n",
    "        'number_ratio': num_digits / total_chars,\n",
    "        'stylized_char_ratio': stylized_count / total_chars,\n",
    "        'char_count': len(original),\n",
    "        'word_count': len(original.split()),\n",
    "    }\n",
    "    return lower, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cbf4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing functions\n",
    "\n",
    "def apply_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cleaned = []\n",
    "    feats = []\n",
    "    for txt in df['comment'].astype(str).tolist():\n",
    "        c, f = clean_text_preserve_signals(txt)\n",
    "        cleaned.append(c)\n",
    "        feats.append(f)\n",
    "    feat_df = pd.DataFrame(feats)\n",
    "    out = df.copy()\n",
    "    out['comment_clean'] = cleaned\n",
    "    for col in feat_df.columns:\n",
    "        out[col] = feat_df[col].values\n",
    "    # Additional length features\n",
    "    out['avg_word_length'] = out['char_count'] / out['word_count'].replace(0, np.nan)\n",
    "    out['is_very_short'] = out['char_count'] <= 3\n",
    "    return out\n",
    "\n",
    "\n",
    "def balance_50_50(df: pd.DataFrame, random_state: int = RANDOM_STATE) -> pd.DataFrame:\n",
    "    # Oversample minority to match majority; then downsample to exact 50:50 of total\n",
    "    labels = df['label']\n",
    "    count0 = int((labels == 0).sum())\n",
    "    count1 = int((labels == 1).sum())\n",
    "    if count0 == 0 or count1 == 0:\n",
    "        return df.copy()\n",
    "    # Oversample minority to majority\n",
    "    if count0 > count1:\n",
    "        maj_df, min_df = df[labels == 0], df[labels == 1]\n",
    "    else:\n",
    "        maj_df, min_df = df[labels == 1], df[labels == 0]\n",
    "    reps = math.ceil(len(maj_df) / len(min_df))\n",
    "    min_upsampled = pd.concat([min_df.sample(len(min_df), replace=True, random_state=random_state + i) for i in range(reps)], ignore_index=True).iloc[:len(maj_df)]\n",
    "    balanced = pd.concat([maj_df, min_upsampled], ignore_index=True)\n",
    "    # Shuffle\n",
    "    balanced = balanced.sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
    "    return balanced\n",
    "\n",
    "\n",
    "def save_outputs(df: pd.DataFrame, stem: str):\n",
    "    csv_path = os.path.join(PROCESSED_DIR, f\"{stem}.csv\")\n",
    "    pkl_path = os.path.join(PROCESSED_DIR, f\"{stem}.pkl\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    df.to_pickle(pkl_path)\n",
    "    print(f\"Saved: {csv_path} | {pkl_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09ac6f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (raw): (8171, 2) (2335, 2) (1167, 2)\n",
      "Saved: processed_data/train_processed.csv | processed_data/train_processed.pkl\n",
      "Saved: processed_data/test_processed.csv | processed_data/test_processed.pkl\n",
      "Saved: processed_data/holdout_processed.csv | processed_data/holdout_processed.pkl\n"
     ]
    }
   ],
   "source": [
    "# Run processing for train/test/holdout\n",
    "train_df = read_csv_auto(os.path.join(RAW_DIR, 'train.csv'))\n",
    "test_df = read_csv_auto(os.path.join(RAW_DIR, 'test.csv'))\n",
    "holdout_df = read_csv_auto(os.path.join(RAW_DIR, 'holdout.csv'))\n",
    "\n",
    "print('Shapes (raw):', train_df.shape, test_df.shape, holdout_df.shape)\n",
    "\n",
    "train_proc = apply_cleaning(train_df)\n",
    "test_proc = apply_cleaning(test_df)\n",
    "holdout_proc = apply_cleaning(holdout_df)\n",
    "\n",
    "save_outputs(train_proc, 'train_processed')\n",
    "save_outputs(test_proc, 'test_processed')\n",
    "save_outputs(holdout_proc, 'holdout_processed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b132200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts (train before/after): {0: 7454, 1: 717} {1: 7454, 0: 7454}\n",
      "Label counts (test before/after): {0: 2126, 1: 209} {0: 2126, 1: 2126}\n",
      "Label counts (holdout before/after): {0: 942, 1: 225} {1: 942, 0: 942}\n",
      "Saved: processed_data/train_balanced_50_50.csv | processed_data/train_balanced_50_50.pkl\n",
      "Saved: processed_data/test_balanced_50_50.csv | processed_data/test_balanced_50_50.pkl\n",
      "Saved: processed_data/holdout_balanced_50_50.csv | processed_data/holdout_balanced_50_50.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create 50:50 balanced splits and save\n",
    "train_bal = balance_50_50(train_proc, RANDOM_STATE)\n",
    "test_bal = balance_50_50(test_proc, RANDOM_STATE)\n",
    "holdout_bal = balance_50_50(holdout_proc, RANDOM_STATE)\n",
    "\n",
    "print('Label counts (train before/after):', train_proc['label'].value_counts().to_dict(), train_bal['label'].value_counts().to_dict())\n",
    "print('Label counts (test before/after):', test_proc['label'].value_counts().to_dict(), test_bal['label'].value_counts().to_dict())\n",
    "print('Label counts (holdout before/after):', holdout_proc['label'].value_counts().to_dict(), holdout_bal['label'].value_counts().to_dict())\n",
    "\n",
    "save_outputs(train_bal, 'train_balanced_50_50')\n",
    "save_outputs(test_bal, 'test_balanced_50_50')\n",
    "save_outputs(holdout_bal, 'holdout_balanced_50_50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7baed106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aggregated datasets to processed_data/all_datasets.pkl\n"
     ]
    }
   ],
   "source": [
    "# Aggregate all processed datasets\n",
    "all_df = pd.concat([\n",
    "    train_proc.assign(split='train'),\n",
    "    test_proc.assign(split='test'),\n",
    "    holdout_proc.assign(split='holdout')\n",
    "], ignore_index=True)\n",
    "\n",
    "all_bal_df = pd.concat([\n",
    "    train_bal.assign(split='train_balanced'),\n",
    "    test_bal.assign(split='test_balanced'),\n",
    "    holdout_bal.assign(split='holdout_balanced')\n",
    "], ignore_index=True)\n",
    "\n",
    "all_payload = {\n",
    "    'processed': all_df,\n",
    "    'balanced': all_bal_df\n",
    "}\n",
    "\n",
    "pd.to_pickle(all_payload, os.path.join(PROCESSED_DIR, 'all_datasets.pkl'))\n",
    "print('Saved aggregated datasets to processed_data/all_datasets.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80c42862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_processed: n=8171 labels={0: 7454, 1: 717}\n",
      "Avg chars: 64.41 Avg words: 10.51 Stylized ratio mean: 0.0107 Num ratio mean: 0.0236 Special ratio mean: 0.0269\n",
      "test_processed: n=2335 labels={0: 2126, 1: 209}\n",
      "Avg chars: 62.29 Avg words: 10.16 Stylized ratio mean: 0.0117 Num ratio mean: 0.0234 Special ratio mean: 0.0275\n",
      "holdout_processed: n=1167 labels={0: 942, 1: 225}\n",
      "Avg chars: 62.44 Avg words: 10.14 Stylized ratio mean: 0.0196 Num ratio mean: 0.0212 Special ratio mean: 0.0301\n",
      "train_balanced_50_50: n=14908 labels={1: 7454, 0: 7454}\n",
      "Avg chars: 56.53 Avg words: 10.24 Stylized ratio mean: 0.0591 Num ratio mean: 0.035 Special ratio mean: 0.0262\n",
      "test_balanced_50_50: n=4252 labels={0: 2126, 1: 2126}\n",
      "Avg chars: 55.39 Avg words: 9.92 Stylized ratio mean: 0.0615 Num ratio mean: 0.0362 Special ratio mean: 0.0254\n",
      "holdout_balanced_50_50: n=1884 labels={1: 942, 0: 942}\n",
      "Avg chars: 52.41 Avg words: 8.76 Stylized ratio mean: 0.0478 Num ratio mean: 0.0246 Special ratio mean: 0.0257\n"
     ]
    }
   ],
   "source": [
    "# Quick validation summaries\n",
    "\n",
    "def summarize(df: pd.DataFrame, name: str):\n",
    "    vc = df['label'].value_counts().to_dict()\n",
    "    print(f\"{name}: n={len(df)} labels={vc}\")\n",
    "    print('Avg chars:', df['char_count'].mean().round(2),\n",
    "          'Avg words:', df['word_count'].mean().round(2),\n",
    "          'Stylized ratio mean:', df['stylized_char_ratio'].mean().round(4),\n",
    "          'Num ratio mean:', df['number_ratio'].mean().round(4),\n",
    "          'Special ratio mean:', df['special_char_ratio'].mean().round(4))\n",
    "\n",
    "summarize(train_proc, 'train_processed')\n",
    "summarize(test_proc, 'test_processed')\n",
    "summarize(holdout_proc, 'holdout_processed')\n",
    "\n",
    "summarize(train_bal, 'train_balanced_50_50')\n",
    "summarize(test_bal, 'test_balanced_50_50')\n",
    "summarize(holdout_bal, 'holdout_balanced_50_50')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
